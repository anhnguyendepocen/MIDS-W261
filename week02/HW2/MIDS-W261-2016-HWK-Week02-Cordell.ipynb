{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Spring 2016 Homework Week 2\n",
    "\n",
    "Ron Cordell<br />\n",
    "W261-4<br />\n",
    "ron.cordell@ischool.berkeley.edu<br />\n",
    "January 26, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a race condition in the context of parallel computation? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A race condition in parallel computing can happen when more than one instance of a procedure attempts to modify a shared object. For example, suppose there are two instances of a process that number with its square, and that both instances operate on the same number. There is nothing to control the actual timing of execution of these instances. If one instance of the process performs its work after the other instance then the answer will be the number raised to the 4th power and not the square. If both instances get the number, compute the square, one instance will write the number before the other, and the second one will overwrite the first. In this case, however, we get the correct answer. The problem is that we don't know deterministically which case will happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is MapReduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is a two-stage functional programming recipe for processing large data sets.\n",
    "\n",
    "- The first stage performs a computation over all inputs, called a 'map' from Lisp terminology\n",
    "- The second stage aggregates the intermediate output from the first stage, called a 'reduce'\n",
    "\n",
    "While seemingly simple, map reduce can be applied to a huge number of problems and can be used in multiple map-reduce stages for more complex scenarios. When a map function can be applied very simply using commutative and associative types of operations it lends to supporting embarrasingly parallel scenarios. While MapReduce has been around in functional programming languages like Lisp since the 1960's, it's modern use has come to the forefront as a result of a combination of commidity priced hardware compute and storage nodes and the [Google paper](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it differ from Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop provides a framework in which to execute map-reduce and relieves the programmer of certain tasks while providing additional functionality. For example, Hadoop gathers and sorts the `(key,value)` pairs output by the mapper stage and routes them to the appropriate reducer, ensuring that each reducer has a complete set values for the keys given. This intermediate sort and route is part of the Hadoop Shuffle, which is the backbone of Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is based upon the map-reduce programming paradigm which is designed to allow parallel distributed processing of large sets of data by mapping/transforming them to sets of tuples then combining and reducing/aggregating them to smaller sets of tuples. The map and reduce steps are written by the user. \n",
    "\n",
    "What follows is a simple example of a map reduce application. The map and reduce steps are written in Python while the execution of the steps is executed by bash shell commands. This mapper and reducer count the number of occurances of a specified word in a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Simple Example\n",
    "\n",
    "What follows is a simple example of a map reduce application. The map and reduce steps are written in Python while the execution of the steps is executed by bash shell commands. This mapper and reducer count the number of occurances of a specified word in a text file.\n",
    "\n",
    "#### MapReduce Example - Map\n",
    "\n",
    "This map step reads a file from the local disk line by line, then breaks each line down to individual words.\n",
    "It then compares each word, ignoring case, to the specified search word and accumulates a count for that line. When all the lines have been examined the mapper writes the count to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "# the word to find is the first argument\n",
    "findword = sys.argv[1].lower()\n",
    "\n",
    "# the file to scan is the second argument\n",
    "filename = sys.argv[2]\n",
    "\n",
    "# open the file, read it line by line\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # process each word in the line by filtering out non-words and if it matches\n",
    "        # the search word, incrementing the word count\n",
    "        for word in WORD_RE.findall(line):\n",
    "            # make this a case-insensitve search\n",
    "            if word.lower() == findword:\n",
    "                count += 1\n",
    "            \n",
    "# now that we've counted this line, write out the count\n",
    "with open ('{0}.intermediateCount'.format(filename),'w') as outfile:\n",
    "    outfile.write('{0}\\n'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the appropriate execution mode on the file so we can execute it\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Example - Reduce\n",
    "\n",
    "The reduce step takes as input series of counts and accumulates them into a single count. The reducer recieves its input on STDIN and writes its output to STDOUT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        sum += int(line)\n",
    "    except:\n",
    "        pass\n",
    "sys.stdout.write('{0}'.format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the appropriate execution mode on the file so we can execute it\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Example - Putting it together\n",
    "\n",
    "The following bash shell script takes the file to scan and breaks it into separate sub-files, each containing a chunk of the original file. It then invokes several instances of the mapper.py program, one for each chunk, and supplies one of the chunked files. The bash script waits for all mappers to finish and then It then takes the output files from each of the mapper.py programs and steams them one after the other to the reducer.py program.\n",
    "\n",
    "The output of the MapReduce is the number of times a word has occurred in the file:\n",
    "\n",
    "     found [59] [COPYRIGHT] in the file [LICENSE.txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pGrepCount.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pGrepCount.sh\n",
    "ORIGINAL_FILE=$1\n",
    "FIND_WORD=$2\n",
    "BLOCK_SIZE=$3\n",
    "CHUNK_FILE_PREFIX=$ORIGINAL_FILE.split\n",
    "SORTED_CHUNK_FILES=$CHUNK_FILE_PREFIX*.sorted\n",
    "usage()\n",
    "{\n",
    "    echo Parallel grep\n",
    "    echo usage: pGrepCount filename word chuncksize\n",
    "    echo greps file file1 in $ORIGINAL_FILE and counts the number of lines\n",
    "    echo Note: file1 will be split in chunks up to $ BLOCK_SIZE chunks each\n",
    "    echo $FIND_WORD each chunk will be grepCounted in parallel\n",
    "}\n",
    "#Splitting $ORIGINAL_FILE INTO CHUNKS\n",
    "split -b $BLOCK_SIZE $ORIGINAL_FILE $CHUNK_FILE_PREFIX\n",
    "#DISTRIBUTE\n",
    "for file in $CHUNK_FILE_PREFIX*\n",
    "do\n",
    "    #grep -i $FIND_WORD $file|wc -l >$file.intermediateCount &\n",
    "    ./mapper.py $FIND_WORD $file >$file.intermediateCount &\n",
    "done\n",
    "wait\n",
    "#MERGEING INTERMEDIATE COUNT CAN TAKE THE FIRST COLUMN AND TOTAL...\n",
    "#numOfInstances=$(cat *.intermediateCount | cut -f 1 | paste -sd+ - |bc)\n",
    "numOfInstances=$(cat *.intermediateCount | ./reducer.py)\n",
    "echo \"found [$numOfInstances] [$FIND_WORD] in the file [$ORIGINAL_FILE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [59] [COPYRIGHT] in the file [LICENSE.txt]\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions on the bash script so it can execute\n",
    "!chmod a+x pGrepCount.sh\n",
    "\n",
    "# execute the MapReduce example and output the result. We will look for the word 'assistance'\n",
    "# the '4K' tells the bash script to break the original file down into chunks of no more than\n",
    "# 4K bytes in size\n",
    "!./pGrepCount.sh LICENSE.txt COPYRIGHT 4k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1. Sort in Hadoop MapReduce\n",
    "\n",
    "#### Given as input: Records of the form `<integer, “NA”>`, where integer is any integer, and “NA” is just the empty string.  Output: sorted key value pairs of the form `<integer, “NA”>` in decreasing order.  What happens if you have multiple reducers? Do you need additional steps? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple reducers each one will have a locally sorted set of records but won't be able to coordinate with the other reducers to produce a complete sorted list. However, it is possible to customize a partitioner to partition the keys across reducers such that reducer 1 has the lowest key values, reducer 2 has the next lowest, and so on. Then the output of the reducers can be easily merged based on the number of the reducer. Technically this is an additional step. Another way to deal with multiple reducers is to have an additional step of taking the output of each of the reducers and sorting that into a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000. Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Record Generator\n",
    "\n",
    "The `recordgneratory.py` accepts an argument to indicate the number of records to generate. Records are generated by selecting a random integer in the range of 1 to the maximum integer value allowed. That integer is appended with the string \",NA\" and written to the output file as a record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting recordgenerator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile recordgenerator.py\n",
    "#!/usr/bin/python\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# get number of records to generate\n",
    "num_records = int(sys.argv[1])\n",
    "\n",
    "# generator function to create random integers in the range of 1 to sys.maxint\n",
    "def gen(n):\n",
    "    count = 0\n",
    "    while count < n:\n",
    "        yield (random.randint(1, sys.maxint))\n",
    "        count += 1\n",
    "        \n",
    "# Generate a set of num_record key,value pairs with integer keys generated by the generator\n",
    "# and write them to an output file in the form of \"integer,NA\"\n",
    "for i in gen(num_records):\n",
    "    sys.stdout.write('{0},{1}\\n'.format(i,\"NA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x recordgenerator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Map\n",
    "\n",
    "For each line that is read from STDIN we expect a string in the form of:\n",
    "\n",
    "     key,value\n",
    "\n",
    "Write each `key, value` pair to STDOUT as `key<tab>value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# each record comes via STDIN one record at a time\n",
    "for record in sys.stdin:\n",
    "    # clean whitespace and split at the comma\n",
    "    k,v = record.strip().split(',')\n",
    "    # write to STDOUT as key <tab> value\n",
    "    print '{0}\\t{1}'.format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Reduce\n",
    "\n",
    "For each `key<tab>value` read from STDIN write back out to STDOUT. The output is sorted because the input is already sorted for us by the record key, thanks to Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# read each key,value pair from STDIN and write back out to STDOUT\n",
    "# we can do this because the keys are sorted for us by Hadoop\n",
    "for pair in sys.stdin:\n",
    "    k,v = pair.strip().split('\\t')\n",
    "    print '{0}\\t{1}'.format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Test Code\n",
    "\n",
    "Test the mapper and reducer by piping a small test set to the mapper, sort the output of the mapper based on the integer value (key), pipe the result to the reducer, then filter for the top 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8470506191635231078\tNA\r\n",
      "8026439827727903483\tNA\r\n",
      "7914562750245791116\tNA\r\n",
      "7866281371022919675\tNA\r\n",
      "7640348753853655778\tNA\r\n",
      "6499377995366904625\tNA\r\n",
      "5277692479661556743\tNA\r\n",
      "4708012296169908311\tNA\r\n",
      "4590245591349809339\tNA\r\n",
      "4346827061492181849\tNA\r\n"
     ]
    }
   ],
   "source": [
    "!./recordgenerator.py 20 | ./mapper.py | sort -nr -k1,1 | ./reducer.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Running in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Yarn and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-rcordell-resourcemanager-Rons-iMac-Retina.local.out\n",
      "localhost: no such identity: /Users/rcordell/.ssh/id_dsa: No such file or directory\n",
      "localhost: Saving password to keychain failed\n",
      "localhost: Identity added: /Users/rcordell/.ssh/id_rsa ((null))\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-rcordell-nodemanager-Rons-iMac-Retina.local.out\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-namenode-Rons-iMac-Retina.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-datanode-Rons-iMac-Retina.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-secondarynamenode-Rons-iMac-Retina.local.out\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an HDFS folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and upload record file containing 10000 records to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./recordgenerator.py 10000 > records.txt\n",
    "!hdfs dfs -put records.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop Streaming Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     hadoop jar hadoopstreamingjarfile \\\n",
    "          -D stream.num.map.output.key.fields=n \\\n",
    "          -mapper mapperfile \\\n",
    "          -reducer reducerfile \\\n",
    "          -input inputfile \\\n",
    "          -output outputfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit our MapReduce to Hadoop using Hadoop Streaming. Besides specifying the mapper, reducer, input file and output location, we also specify to use the KeyFieldBasedComparator and set the keycomparator options. This allows us to specify that the sorting will be done on the key field and to treat the key as a numeric value and to reverse sort (descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=-nr \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input records.txt \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First 10 Items\n",
    "\n",
    "The sorted records output of the MapReduce is on the HDFS file system in a file called `part-00000`. Let's look at the top 10 lines in the file, which should be the highest numbered records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9223149662631926773\tNA\r\n",
      "9222462994771806812\tNA\r\n",
      "9221376548595256568\tNA\r\n",
      "9221200099560410483\tNA\r\n",
      "9216651204641081164\tNA\r\n",
      "9216610805780812765\tNA\r\n",
      "9215182420885073012\tNA\r\n",
      "9214763879526561081\tNA\r\n",
      "9214470163579875161\tNA\r\n",
      "9212397989479861727\tNA\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last 10 Items\n",
    "\n",
    "Now let's look at the last 10 items in the file, which should be the lowest record ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9242791484899512\tNA\r\n",
      "8658217614831616\tNA\r\n",
      "7790490907682806\tNA\r\n",
      "7435515921714919\tNA\r\n",
      "4424418503692755\tNA\r\n",
      "4258466038084507\tNA\r\n",
      "3781763814522545\tNA\r\n",
      "3693207216031204\tNA\r\n",
      "2915017580505790\tNA\r\n",
      "1594448856056738\tNA\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000 | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up HDFS output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/recordsOutput\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/records.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/rcordell/records.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Yarn and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2.  WORDCOUNT\n",
    "\n",
    "#### Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    " \n",
    "CROSSCHECK: \n",
    "    >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "     8    \n",
    "\n",
    "#### NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2 - Map\n",
    "\n",
    "Read from STDIN where each line read consists of tab-delimeted fields:\n",
    "      \n",
    "      id <tab> label <tab> subject <tab> body\n",
    "\n",
    "For each word in the subject and body fields that matches the wordlist, emit a key,value pair of\n",
    "      \n",
    "      word, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.2\n",
    "## Read lines from STDIN, separate into fields\n",
    "## Output a key, value => (word,count) for each word in the subject and body text\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# extract words to count from first positional argument, making them all lower case\n",
    "wordlist = []\n",
    "if len(sys.argv) > 1:\n",
    "    for word in sys.argv[1].strip().split():\n",
    "        wordlist.append(word.lower())\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # capture the id, label, subject and body\n",
    "        email_id, label, subject, body = line.split('\\t')\n",
    "    except ValueError:\n",
    "        # if there were only 3 fields in the input, assume field 3 is body\n",
    "        email_id, label, body = line.split('\\t')\n",
    "        subject = ''\n",
    " \n",
    "    # extract only words from the combined subject and body text\n",
    "    for word in WORD_RE.findall(subject + ' ' + body):\n",
    "        if len(wordlist) > 0:\n",
    "            if word.lower() in wordlist:\n",
    "                print('{0}\\t{1}'.format(word.lower(), 1))\n",
    "        else:\n",
    "            # otherwise count all words\n",
    "            print('{0}\\t{1}'.format(word.lower(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.2 - Reduce\n",
    "\n",
    "Read lines from STDIN of\n",
    "\n",
    "      word, count\n",
    "      \n",
    "Accumulate the counts for each word and output the resulting word counts to STDOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW2.2\n",
    "## given a list of key,value pairs for word, count, aggregate and output the list\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2 - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n",
      "master\t3\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./mapper.py \"assistance master\" | sort -k1,1 | ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2 - Running in Hadoop\n",
    "\n",
    "#### Start YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create HDFS Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Enron email file to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/rcordell/enronemail_1h.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Hadoop Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=-nr \\\n",
    "    -mapper \"mapper.py 'assistance'\" \\\n",
    "    -reducer reducer.py \\\n",
    "    -input enronemail_1h.txt \\\n",
    "    -output countsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine output for 'assistance'\n",
    "\n",
    "The output indicates the 10 occurances of the word assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/countsOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/countsOutput\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/countsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2.1  \n",
    "\n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "There is no need to make a change to the mapper, so we'll use the same mapper from HW2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "The reducer is modified so that it sorts the resulting word accumulation and outputs the top 10 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW2.2\n",
    "## given a list of key,value pairs for word, count, aggregate and output the list\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "all_words = []\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # append result to list\n",
    "            all_words.append((current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    all_words.append((current_word, current_count))\n",
    "    \n",
    "# sort the list by value\n",
    "sorted_words = sorted(all_words, key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "# write out the top 10\n",
    "for i,word in enumerate(sorted_words):\n",
    "    print '{0}\\t{1}'.format(word[0],word[1])\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1247\r\n",
      "to\t963\r\n",
      "and\t668\r\n",
      "of\t566\r\n",
      "a\t542\r\n",
      "you\t432\r\n",
      "in\t417\r\n",
      "your\t394\r\n",
      "ect\t382\r\n",
      "for\t373\r\n",
      "on\t271\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./mapper.py | sort -k1,1 | ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.2.1 - Running in Hadoop\n",
    "\n",
    "#### Start YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create HDFS Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Enron email file to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/rcordell/enronemail_1h.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Hadoop Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output countsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Word Counts\n",
    "\n",
    "The output agrees with our tests but it is apparent that it is mostly what one would consider \"stop words\". I also see 'ect' in the list which is interesting because it ranks so high. Looking at the source text it appears that 'ect' refers to a person who is referenced quite often in the Enron email samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1247\r\n",
      "to\t963\r\n",
      "and\t668\r\n",
      "of\t566\r\n",
      "a\t542\r\n",
      "you\t432\r\n",
      "in\t417\r\n",
      "your\t394\r\n",
      "ect\t382\r\n",
      "for\t373\r\n",
      "on\t271\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/countsOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/countsOutput\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/countsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the log posterior probabilities (i.e., log(Pr(Class|Doc))) for each class over the training set. Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.3 - Map Stage 1 - Term Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting term_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile term_mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.3\n",
    "## Given input on STDIN read lines and count occurrences of words\n",
    "## Output a key, value => (token, email id, class, term_flag) = count\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "all_words = False\n",
    "## Words in the word list are space delimited\n",
    "## If no words specified, use all tokens as vocabulary terms\n",
    "if len(sys.argv) > 1:\n",
    "    wordlist = sys.argv[1].lower().split(' ')\n",
    "else:\n",
    "    all_words = True\n",
    "\n",
    "# read each email as a line from stdin\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Split the line into the 4 fields\n",
    "        email_id, label, subject, body = line.split('\\t')\n",
    "    except ValueError:\n",
    "        # If there are 3 fields the assume the 3rd field is the body\n",
    "        email_id, label, body = line.split('\\t')\n",
    "        subject = ''\n",
    "    \n",
    "    # extract only words from the combined subject and body text\n",
    "    for token in WORD_RE.findall(subject + ' ' + body):\n",
    "        # term indicates that this is a vocabulary word \n",
    "        # when not all words are considered this lets downstream processors know which are which\n",
    "        term = '0'\n",
    "        if all_words:\n",
    "            term = '1'\n",
    "        elif token.lower() in wordlist:\n",
    "            term = '1'\n",
    "            \n",
    "        # emit on each word a key, value pair of [(word, id, label, term),1]\n",
    "        # so that the sort function operates on the words as keys\n",
    "        print('{0}\\t{1}\\t{2}\\t{3}\\t{4}'.format(token, email_id, label, term, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x term_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3 - Reduce Stage 1 - Term Probabilities and Calculating the Naive Bayes model\n",
    "\n",
    "The reducer calculates the naive bayes model from the training data mapped by the mapper. \n",
    "Each unique term is added to the model and the statistics updated when subsequent instances of the term are encountered. The model is a dictionary with the term as the key and the class counts and probabilities. When all input is processed, the model is persisted for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting probability_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile probability_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "\n",
    "term_counts = {}\n",
    "spam_doc_ids = []\n",
    "ham_doc_ids = []\n",
    "spam_doc_word_count = 0.0\n",
    "ham_doc_word_count = 0.0\n",
    "spam_term_count = 0.0\n",
    "ham_term_count = 0.0\n",
    "terms = 0.0\n",
    "current_key = None\n",
    "current_count = 0\n",
    "\n",
    "# STDIN consists of single lines of: token <tab> id <tab> class <tab> term_flag <tab> count\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "# perform the accumulation functions for each key, value received\n",
    "def accumulate_key(key , _count, spam_doc_word_count, ham_doc_word_count, term_counts):\n",
    "    # accumulate the key, values in a dictionary\n",
    "    _token = key[0]\n",
    "    _id = key[1]\n",
    "    _label = key[2]\n",
    "    _term = key[3]\n",
    "\n",
    "    # accumulate into ham and spam dictionaries also\n",
    "    if _label == '1':\n",
    "        spam_doc_word_count += _count\n",
    "        if _id not in spam_doc_ids:\n",
    "            spam_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['spam_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0} \n",
    "    else:                \n",
    "        ham_doc_word_count += _count\n",
    "        if _id not in ham_doc_ids:\n",
    "            ham_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['ham_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'ham_count' : _count,\n",
    "                                      'spam_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0}\n",
    "    return spam_doc_word_count, ham_doc_word_count\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    # split the line into the key components and the value\n",
    "    token, email_id, label, term, token_count = line.split('\\t')\n",
    "\n",
    "    # this makes reading the code a little easier\n",
    "    if term == '0':\n",
    "        vocab_word = False\n",
    "    else:\n",
    "        vocab_word = True\n",
    "\n",
    "    # if we've been accumulating for this key, keep accumulating\n",
    "    # this works because the input is sorted on the key\n",
    "    if current_key == (token, email_id, label, term):\n",
    "        current_count += int(token_count)\n",
    "    else:\n",
    "        # we've just received a different key from what we've been accumulating\n",
    "        # wrap up with the current key\n",
    "        if current_key:\n",
    "            spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                                     float(current_count), \n",
    "                                                                     spam_doc_word_count, \n",
    "                                                                     ham_doc_word_count, \n",
    "                                                                     term_counts)\n",
    "        # start a new accumulation    \n",
    "        current_count = int(token_count)\n",
    "        current_key = (token, email_id, label, term)\n",
    "\n",
    "# add the last key we've been accumulating\n",
    "if current_key == (token, email_id, label, term):\n",
    "    spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                             float(current_count), \n",
    "                                                             spam_doc_word_count, \n",
    "                                                             ham_doc_word_count, \n",
    "                                                             term_counts)      \n",
    "                       \n",
    "# now we should have consolidated the intermediate counts and we can compute the rest\n",
    "\n",
    "# count the number of terms\n",
    "term_count = len(term_counts.keys()) * 1.0\n",
    "# compute the prior\n",
    "prior = (len(spam_doc_ids)*1.0)/(1.0*(len(spam_doc_ids) + len(ham_doc_ids)))\n",
    "\n",
    "# calculate the P(term|class) for each term\n",
    "# do not use smoothing here\n",
    "for term in term_counts:\n",
    "    term_counts[term]['prob_ham'] = (term_counts[term]['ham_count'])/(ham_doc_word_count + term_count)\n",
    "    term_counts[term]['prob_spam'] = (term_counts[term]['spam_count'])/(spam_doc_word_count + term_count)\n",
    "    \n",
    "    # output term <tab> probability_ham <tab> ham_count <tab> probability_spam <tab> spam_count <tab> prior\n",
    "    print '{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(term, term_counts[term]['prob_ham'], \n",
    "                                           term_counts[term]['ham_count'],\n",
    "                                           term_counts[term]['prob_spam'], \n",
    "                                           term_counts[term]['spam_count'], \n",
    "                                           prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x probability_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3 - Test Stage 1 Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat enronemail_1h.txt | ./term_mapper.py | sort -r -k1,1 | ./probability_reducer.py > term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3 - Map Stage 2 - Email Classifier\n",
    "\n",
    "Load in the Naive Bayes model which consists of each term, their relative counts and probabilities for each class.\n",
    "Read the stdin stream where each line is an email with email_id, label, subject and body\n",
    "Classify each email based on the log probabilities of each term\n",
    "Omit terms from the calculation that don't have both - note: this can be made easier than I implemented it.\n",
    "For each email, emit the id, label, class and log probabilities\n",
    "At the end, emit the tallied zero probability counts for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting email_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile email_mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.3\n",
    "## Given input on STDIN read lines and count occurrences of words\n",
    "## Output a key, value => (token, email id, class, term_flag) = count\n",
    "\n",
    "from math import log, exp\n",
    "import sys\n",
    "import re\n",
    "\n",
    "prior = 0.0\n",
    "zero_prob_ham = 0\n",
    "zero_prob_spam = 0\n",
    "terms = {}\n",
    "# open the file with the term probabilities (model) and load into the terms dictionary\n",
    "# each line in the file is \n",
    "#   term <tab> ham_probability <tab> ham_count <tab> spam_prob <tab> spam_count <tab> prior\n",
    "\n",
    "# all values are floats\n",
    "\n",
    "with open('term_probabilities.txt','r') as termfile:\n",
    "    for line in termfile.readlines():\n",
    "        term, ham_prob, ham_count, spam_prob, spam_count, _prior = line.strip().split('\\t')\n",
    "        terms[term.strip()] = {'ham_prob'  : float(ham_prob),  'ham_count'  : float(ham_count),\n",
    "                       'spam_prob' : float(spam_prob), 'spam_count' : float(spam_count)}\n",
    "        prior = float(_prior)\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# read each line from stdin, one email per line\n",
    "for line in sys.stdin:\n",
    "    log_prob_ham = log(1.0 - prior)\n",
    "    log_prob_spam = log(prior)\n",
    "\n",
    "    try:\n",
    "        email_id, label, subject, body = line.split('\\t')\n",
    "    except ValueError:\n",
    "        email_id, label, body = line.split('\\t')\n",
    "        subject = ''\n",
    "\n",
    "    pred_label = None\n",
    "        \n",
    "    # extract only words from the combined subject and body text\n",
    "    text = WORD_RE.findall(subject + ' ' + body)\n",
    "    for token in text:\n",
    "        t = token.strip().lower()\n",
    "        if t in terms:\n",
    "            # if we have a zero probability for either class conditional then add a minute value\n",
    "            # to offset the effects\n",
    "            if terms[t]['spam_prob'] > 0.0:\n",
    "                log_prob_spam += log(terms[t]['spam_prob'])\n",
    "                \n",
    "                # if the term only occurs in spam, then mark this as spam and move on.\n",
    "                if terms[t]['ham_prob'] <= 0.0:\n",
    "                    pred_label = '1'\n",
    "                    zero_prob_ham += 1\n",
    "            if terms[t]['ham_prob'] > 0.0: \n",
    "                log_prob_ham += log(terms[t]['ham_prob'])\n",
    "                \n",
    "                # if we've only ever seen this term in ham, then mark as such and move on.\n",
    "                if terms[t]['spam_prob'] <= 0.0:\n",
    "                    pred_label = '0'\n",
    "                    zero_prob_spam += 1\n",
    "\n",
    "                    \n",
    "    # if we didn't encounter a term that's not in one class then calculate the class\n",
    "    if not pred_label:\n",
    "        # We have what we need to classify the email\n",
    "        # emit the classification\n",
    "        if log_prob_spam > log_prob_ham:\n",
    "            pred_label = '1'\n",
    "        else:\n",
    "            pred_label = '0'\n",
    "            \n",
    "    # for each email emit the id <tab> label <tab> prediction <tag> log_prob_ham <tab> log_prob_spam\n",
    "    print '{0}\\t{1}\\t{2}\\t{3}\\t{4}'.format(email_id, label, pred_label, log_prob_ham, log_prob_spam)\n",
    "    \n",
    "# after all emails have been processed emit :<tab> zero ham probability count <tab> zero spam probability count\n",
    "# the ':' lets the reducer know to treat it differently from the email classification\n",
    "# pad out the end to keep the number of fields consistent\n",
    "print ':\\t{0}\\t{1}\\t{2}\\t{3}'.format(zero_prob_ham, zero_prob_spam, ' ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x email_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3 - Reduce Stage 2 - Email Classifier\n",
    "\n",
    "Read each line from stdin which can be either an email classification or a zero probability tally.\n",
    "Output the email classification and tally the error rate\n",
    "Output the totaled zero probability counts for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classifier_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile classifier_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "from math import log,exp\n",
    "\n",
    "zero_prob_ham = 0\n",
    "zero_prob_spam = 0\n",
    "\n",
    "right = 0\n",
    "wrong = 0\n",
    "\n",
    "# STDIN consists of lines of one of two kinds - one that starts with ':' and one that doesn't\n",
    "# For the line that starts with a colon, accumulate the zero probability counts it has\n",
    "# Otherwise the line is the email id, label, class, log probability ham, log probability spam\n",
    "\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # if this isn't a zero probability count \n",
    "    if fields[0] != ':':\n",
    "        # output the email id, label, prediction\n",
    "        print '{0}\\t{1}\\t{2}\\t{3}\\t{4}'.format(fields[0], fields[1], fields[2], fields[3], fields[4])\n",
    "        \n",
    "        # tally the error rate\n",
    "        if fields[1].strip() != fields[2].strip():\n",
    "            wrong += 1\n",
    "        else:\n",
    "            right += 1\n",
    "    else:\n",
    "        # this is actually the zero probability counters so tally them\n",
    "        zero_prob_ham += int(fields[1])\n",
    "        zero_prob_spam += int(fields[2])\n",
    "    \n",
    "print 'Error Rate: {0}/{1}'.format(wrong, right + wrong)\n",
    "print 'Zero Probabilities: Spam {0} \\tHam {1}'.format(zero_prob_spam, zero_prob_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x classifier_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Test with a small set of 10 lines of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: stdout: Broken pipe\n",
      "0001.1999-12-10.farmer\t0\t0\t-47.0290005534\t-19.0617153626\n",
      "0001.1999-12-10.kaminski\t0\t0\t-30.7856476462\t-21.9636085124\n",
      "0001.2000-01-17.beck\t0\t0\t-3809.99682474\t-2361.1052968\n",
      "0001.2000-06-06.lokay\t0\t0\t-3830.76627631\t-2691.51240111\n",
      "0001.2001-02-07.kitchen\t0\t0\t-352.34209308\t-250.837517623\n",
      "0001.2001-04-02.williams\t0\t0\t-1428.65508644\t-1083.39627997\n",
      "0002.1999-12-13.farmer\t0\t0\t-3046.27089855\t-2131.56357157\n",
      "0002.2001-02-07.kitchen\t0\t0\t-466.724353395\t-329.187502814\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\t-410.383521205\t-628.537594546\n",
      "0002.2003-12-18.GP\t1\t1\t-643.167996685\t-1339.23830337\n",
      "Error Rate: 0/10\n",
      "Zero Probabilities: Spam 556 \tHam 107\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | head | ./email_mapper.py | sort -k1,1 | ./classifier_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.3 - Processing With Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create HDFS folders and copy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/rcordell/enronemail_1h.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell\n",
    "!hdfs dfs -put enronemail_1h.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up HDFS\n",
    "\n",
    "Remove files for repested runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\n",
      "Deleted /user/rcordell/classifier\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model\n",
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop MR Stage 1 - Calculate the model\n",
    "\n",
    "The Hadoop streaming input has two more parameters:\n",
    "\n",
    "- jobconf stream.num.map.output.key.fields=4\n",
    "- jobconf stream.num.reduce.output.key.fields=3\n",
    "\n",
    "These are used to tell Hadoop which values in the tab-delimited key,value pairs make up the key. Otherwise Hadoop only uses the first field. This will give use more complete sorting control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -mapper term_mapper.py \\\n",
    "    -reducer probability_reducer.py \\\n",
    "    -input enronemail_1h.txt \\\n",
    "    -output model \\\n",
    "    -jobconf stream.num.map.output.key.fields=4 \\\n",
    "    -jobconf stream.num.reduce.output.key.fields=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy model file to local file system from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f term_probabilities.txt\n",
    "!hdfs dfs -get /user/rcordell/model/part-00000 term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Hadoop Stage 2 MapReduce\n",
    "\n",
    "Here we only want Hadoop to sort on the first field so we don't change the default behavior as we did in Stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [term_probabilities.txt] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob8125991547498805172.jar tmpDir=null\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -mapper email_mapper.py \\\n",
    "    -reducer classifier_reducer.py \\\n",
    "    -input enronemail_1h.txt \\\n",
    "    -output classifier \\\n",
    "    -file term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the output of the classifier - Error Rate\n",
    "\n",
    "The best I could get the error rate is 1% without Laplace Smoothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 1/100\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"error rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Probability Counts\n",
    "\n",
    "This is the count of how many zero probabilities were encountered for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Probabilities: Spam 4965 \tHam 5694\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"Zero Probabilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.3 - Log Probability Distribution\n",
    "\n",
    "The histograms below are a plot of the log probability distribution for both ham and spam classes.\n",
    "\n",
    "The Ham histogram is must narrower than the Spam histogram. If we were working with normal distributions this would indicate the possibiliy increased power of the classifiers because spam may statistically significantly different from ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm classification_results.txt\n",
    "!hdfs dfs -get /user/rcordell/classifier/part-00000 classification_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x116033ed0>"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2kAAAFCCAYAAABrfJV6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG9TSolyjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115c35350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ham = []\n",
    "spam = []\n",
    "with open('classification_results.txt','r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            _id, label, cls, lh, ls = line.strip().split('\\t')\n",
    "            try:\n",
    "                ham.append(float(lh))\n",
    "            except:\n",
    "                ham.append(0.0)\n",
    "            try:\n",
    "                spam.append(float(ls))\n",
    "            except:\n",
    "                spam.append(0.0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "p = plt.subplot(1, 2, 1)\n",
    "p.hist(ham,100)\n",
    "plt.title('Ham Log Probability Frequencies')\n",
    "\n",
    "p = plt.subplot(1, 2, 2)\n",
    "p.hist(spam,100)\n",
    "plt.title('Spam Log Probability Frequencies')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\n",
      "Deleted /user/rcordell/classifier\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model\n",
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 \n",
    "\n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.4 - Reduce Stage 1 - Add Smoothing\n",
    "\n",
    "To implement Laplace smoothing we need to change the way the conditional probabilities are calculated in the first stage MapReduce so that the model is changed slightly. We don't need to make changes anywhere else so only the Stage 1 Reducer is shown here. The change is down near the bottom where the term conditional probabilities are calculated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting probability_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile probability_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "\n",
    "term_counts = {}\n",
    "spam_doc_ids = []\n",
    "ham_doc_ids = []\n",
    "spam_doc_word_count = 0.0\n",
    "ham_doc_word_count = 0.0\n",
    "spam_term_count = 0.0\n",
    "ham_term_count = 0.0\n",
    "terms = 0.0\n",
    "current_key = None\n",
    "current_count = 0\n",
    "\n",
    "# STDIN consists of single lines of: token <tab> id <tab> class <tab> term_flag <tab> count\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "# perform the accumulation functions for each key, value received\n",
    "def accumulate_key(key , _count, spam_doc_word_count, ham_doc_word_count, term_counts):\n",
    "    # accumulate the key, values in a dictionary\n",
    "    _token = key[0]\n",
    "    _id = key[1]\n",
    "    _label = key[2]\n",
    "    _term = key[3]\n",
    "\n",
    "    # accumulate into ham and spam dictionaries also\n",
    "    if _label == '1':\n",
    "        spam_doc_word_count += _count\n",
    "        if _id not in spam_doc_ids:\n",
    "            spam_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['spam_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0} \n",
    "    else:                \n",
    "        ham_doc_word_count += _count\n",
    "        if _id not in ham_doc_ids:\n",
    "            ham_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['ham_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'ham_count' : _count,\n",
    "                                      'spam_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0}\n",
    "    return spam_doc_word_count, ham_doc_word_count\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    # split the line into the key components and the value\n",
    "    token, email_id, label, term, token_count = line.split('\\t')\n",
    "\n",
    "    # this makes reading the code a little easier\n",
    "    if term == '0':\n",
    "        vocab_word = False\n",
    "    else:\n",
    "        vocab_word = True\n",
    "\n",
    "    # if we've been accumulating for this key, keep accumulating\n",
    "    # this works because the input is sorted on the key\n",
    "    if current_key == (token, email_id, label, term):\n",
    "        current_count += int(token_count)\n",
    "    else:\n",
    "        # we've just received a different key from what we've been accumulating\n",
    "        # wrap up with the current key\n",
    "        if current_key:\n",
    "            spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                                     float(current_count), \n",
    "                                                                     spam_doc_word_count, \n",
    "                                                                     ham_doc_word_count, \n",
    "                                                                     term_counts)\n",
    "        # start a new accumulation    \n",
    "        current_count = int(token_count)\n",
    "        current_key = (token, email_id, label, term)\n",
    "\n",
    "# add the last key we've been accumulating\n",
    "if current_key == (token, email_id, label, term):\n",
    "    spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                             float(current_count), \n",
    "                                                             spam_doc_word_count, \n",
    "                                                             ham_doc_word_count, \n",
    "                                                             term_counts)      \n",
    "                       \n",
    "# now we should have consolidated the intermediate counts and we can compute the rest\n",
    "\n",
    "# count the number of terms\n",
    "term_count = len(term_counts.keys()) * 1.0\n",
    "# compute the prior\n",
    "prior = (len(spam_doc_ids)*1.0)/(1.0*(len(spam_doc_ids) + len(ham_doc_ids)))\n",
    "\n",
    "# calculate the P(term|class) for each term\n",
    "# IMPLEMENT LAPLACE +1 SMOOTHING HERE\n",
    "for term in term_counts:\n",
    "    term_counts[term]['prob_ham'] = (term_counts[term]['ham_count'] + 1)/(ham_doc_word_count + term_count)\n",
    "    term_counts[term]['prob_spam'] = (term_counts[term]['spam_count'] + 1)/(spam_doc_word_count + term_count)\n",
    "    \n",
    "    # output term <tab> probability_ham <tab> ham_count <tab> probability_spam <tab> spam_count <tab> prior\n",
    "    print '{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(term, term_counts[term]['prob_ham'], \n",
    "                                           term_counts[term]['ham_count'],\n",
    "                                           term_counts[term]['prob_spam'], \n",
    "                                           term_counts[term]['spam_count'], \n",
    "                                           prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "Quick sanity check to make sure we didn't break something..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: stdout: Broken pipe\r\n",
      "0001.1999-12-10.farmer\t0\t0\t-44.207621667\t-48.5150042289\r\n",
      "0001.1999-12-10.kaminski\t0\t0\t-29.8576689636\t-31.6972485272\r\n",
      "0001.2000-01-17.beck\t0\t0\t-3717.60394613\t-4306.64332295\r\n",
      "0001.2000-06-06.lokay\t0\t0\t-3744.50706598\t-4162.88761835\r\n",
      "0001.2001-02-07.kitchen\t0\t0\t-345.886680666\t-403.404749371\r\n",
      "0001.2001-04-02.williams\t0\t0\t-1388.79498638\t-1426.89990274\r\n",
      "0002.1999-12-13.farmer\t0\t0\t-2978.22974162\t-3473.16767374\r\n",
      "0002.2001-02-07.kitchen\t0\t0\t-451.556869828\t-463.833433441\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\t-668.372602902\t-610.88070367\r\n",
      "0002.2003-12-18.GP\t1\t1\t-1420.003234\t-1296.97743047\r\n",
      "Error Rate: 0/10\r\n",
      "Zero Probabilities: Spam 0 \tHam 0\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./term_mapper.py | sort -r -k1,1 | ./probability_reducer.py > term_probabilities.txt\n",
    "!cat enronemail_1h.txt | head | ./email_mapper.py | sort -k1,1 | ./classifier_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 - Processing With Hadoop\n",
    "\n",
    "It is assumed that the hadoop processes are still running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/rcordell/model': No such file or directory\n",
      "rm: `/user/rcordell/classifier': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model\n",
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop Stage 1 MapReduce with Smoothing\n",
    "\n",
    "The Hadoop streaming input has two more parameters:\n",
    "\n",
    "- jobconf stream.num.map.output.key.fields=4\n",
    "- jobconf stream.num.reduce.output.key.fields=3\n",
    "\n",
    "These are used to tell Hadoop which values in the tab-delimited key,value pairs make up the key. Otherwise Hadoop only uses the first field. This will give use more complete sorting control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -mapper term_mapper.py \\\n",
    "    -reducer probability_reducer.py \\\n",
    "    -input enronemail_1h.txt \\\n",
    "    -output model \\\n",
    "    -jobconf stream.num.map.output.key.fields=4 \\\n",
    "    -jobconf stream.num.reduce.output.key.fields=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy model file to local file system from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f term_probabilities.txt\n",
    "!hdfs dfs -get /user/rcordell/model/part-00000 term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Hadoop Stage 2 MapReduce\n",
    "\n",
    "Here we only want Hadoop to sort on the first field so we don't change the default behavior as we did in Stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [term_probabilities.txt] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob3208978509498932952.jar tmpDir=null\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -mapper email_mapper.py \\\n",
    "    -reducer classifier_reducer.py \\\n",
    "    -input enronemail_1h.txt \\\n",
    "    -output classifier \\\n",
    "    -file term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the output of the classifier - Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 0/100\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"error rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Probability Counts\n",
    "\n",
    "This is the count of how many zero probabilities were encountered for each class. Notice how there are now no zero probabilities for either class as a result of the Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Probabilities: Spam 0 \tHam 0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"Zero Probabilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.4 - Log Probability Distribution\n",
    "\n",
    "The histograms below are a plot of the log probability distribution for both ham and spam classes.\n",
    "\n",
    "Now the histograms are much more similar in appearance and not nearly as much noticeable difference. The magnitudes are also more similar with the Ham being a couple of orders of magnitude larger than the spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the classification results from HDFS\n",
    "!rm classification_results.txt\n",
    "!hdfs dfs -get /user/rcordell/classifier/part-00000 classification_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11670f210>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2kAAAFCCAYAAABrfJV6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAY78xKCLBmwLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11482e7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ham = []\n",
    "spam = []\n",
    "with open('classification_results.txt','r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            _id, label, cls, lh, ls = line.strip().split('\\t')\n",
    "            try:\n",
    "                ham.append(float(lh))\n",
    "            except:\n",
    "                ham.append(0.0)\n",
    "            try:\n",
    "                spam.append(float(ls))\n",
    "            except:\n",
    "                spam.append(0.0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "p = plt.subplot(1, 2, 1)\n",
    "p.hist(ham,100)\n",
    "plt.title('Ham Log Probability Frequencies')\n",
    "\n",
    "p = plt.subplot(1, 2, 2)\n",
    "p.hist(spam,100)\n",
    "plt.title('Spam Log Probability Frequencies')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\n",
      "Deleted /user/rcordell/classifier\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model\n",
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.5. \n",
    "\n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Reducer Stage 1, +1 Smoothing, Term Counts >= 3\n",
    "\n",
    "Once again we only need to make changes to the Stage 1 Reducer where the model conditional probabilities are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting probability_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile probability_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "\n",
    "term_counts = {}\n",
    "spam_doc_ids = []\n",
    "ham_doc_ids = []\n",
    "spam_doc_word_count = 0.0\n",
    "ham_doc_word_count = 0.0\n",
    "spam_term_count = 0.0\n",
    "ham_term_count = 0.0\n",
    "terms = 0.0\n",
    "current_key = None\n",
    "current_count = 0\n",
    "\n",
    "# STDIN consists of single lines of: token <tab> id <tab> class <tab> term_flag <tab> count\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "# perform the accumulation functions for each key, value received\n",
    "def accumulate_key(key , _count, spam_doc_word_count, ham_doc_word_count, term_counts):\n",
    "    # accumulate the key, values in a dictionary\n",
    "    _token = key[0]\n",
    "    _id = key[1]\n",
    "    _label = key[2]\n",
    "    _term = key[3]\n",
    "\n",
    "    # accumulate into ham and spam dictionaries also\n",
    "    if _label == '1':\n",
    "        spam_doc_word_count += _count\n",
    "        if _id not in spam_doc_ids:\n",
    "            spam_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['spam_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0} \n",
    "    else:                \n",
    "        ham_doc_word_count += _count\n",
    "        if _id not in ham_doc_ids:\n",
    "            ham_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['ham_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'ham_count' : _count,\n",
    "                                      'spam_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0}\n",
    "    return spam_doc_word_count, ham_doc_word_count\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    # split the line into the key components and the value\n",
    "    token, email_id, label, term, token_count = line.split('\\t')\n",
    "\n",
    "    # this makes reading the code a little easier\n",
    "    if term == '0':\n",
    "        vocab_word = False\n",
    "    else:\n",
    "        vocab_word = True\n",
    "\n",
    "    # if we've been accumulating for this key, keep accumulating\n",
    "    # this works because the input is sorted on the key\n",
    "    if current_key == (token, email_id, label, term):\n",
    "        current_count += int(token_count)\n",
    "    else:\n",
    "        # we've just received a different key from what we've been accumulating\n",
    "        # wrap up with the current key\n",
    "        if current_key:\n",
    "            spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                                     float(current_count), \n",
    "                                                                     spam_doc_word_count, \n",
    "                                                                     ham_doc_word_count, \n",
    "                                                                     term_counts)\n",
    "        # start a new accumulation    \n",
    "        current_count = int(token_count)\n",
    "        current_key = (token, email_id, label, term)\n",
    "\n",
    "# add the last key we've been accumulating\n",
    "if current_key == (token, email_id, label, term):\n",
    "    spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                             float(current_count), \n",
    "                                                             spam_doc_word_count, \n",
    "                                                             ham_doc_word_count, \n",
    "                                                             term_counts)      \n",
    "                       \n",
    "# now we should have consolidated the intermediate counts and we can compute the rest\n",
    "\n",
    "# count the number of terms\n",
    "term_count = len(term_counts.keys()) * 1.0\n",
    "# compute the prior\n",
    "prior = (len(spam_doc_ids)*1.0)/(1.0*(len(spam_doc_ids) + len(ham_doc_ids)))\n",
    "\n",
    "# calculate the P(term|class) for each term\n",
    "\n",
    "# HW 2.4 - IMPLEMENT LAPLACE +1 SMOOTHING HERE\n",
    "# HW 2.5 - IMPLEMENT MIN 3 TERM COUNT HERE\n",
    "\n",
    "terms_to_drop = []\n",
    "for term in term_counts:\n",
    "    # if either the spam or ham term count is > 3, keep this term in the model\n",
    "    if term_counts[term]['ham_count'] >= 3 or term_counts[term]['spam_count'] >= 3:\n",
    "        term_counts[term]['prob_ham'] = (term_counts[term]['ham_count'] + 1)/(ham_doc_word_count + term_count)\n",
    "        term_counts[term]['prob_spam'] = (term_counts[term]['spam_count'] + 1)/(spam_doc_word_count + term_count)\n",
    "    else:\n",
    "        # add the term to be removed to the list; we can't remove it while iterating over the dictionary\n",
    "        terms_to_drop.append(term)\n",
    "\n",
    "for term in terms_to_drop:\n",
    "    term_counts.pop(term, None)  \n",
    "\n",
    "# now emit the model\n",
    "for term in term_counts:\n",
    "    # output term <tab> probability_ham <tab> ham_count <tab> probability_spam <tab> spam_count <tab> prior\n",
    "    print '{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(term, term_counts[term]['prob_ham'], \n",
    "                                           term_counts[term]['ham_count'],\n",
    "                                           term_counts[term]['prob_spam'], \n",
    "                                           term_counts[term]['spam_count'], \n",
    "                                           prior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "Quick sanity check to make sure we didn't break something..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: stdout: Broken pipe\r\n",
      "0001.1999-12-10.farmer\t0\t0\t-16.6201146096\t-18.92818397\r\n",
      "0001.1999-12-10.kaminski\t0\t0\t-20.6618332778\t-21.6039260474\r\n",
      "0001.2000-01-17.beck\t0\t0\t-2906.60280384\t-3408.87707355\r\n",
      "0001.2000-06-06.lokay\t0\t0\t-3193.69771685\t-3561.12355773\r\n",
      "0001.2001-02-07.kitchen\t0\t0\t-293.1444572\t-344.231108854\r\n",
      "0001.2001-04-02.williams\t0\t0\t-1044.21881162\t-1052.19466281\r\n",
      "0002.1999-12-13.farmer\t0\t0\t-2441.35194768\t-2883.61228998\r\n",
      "0002.2001-02-07.kitchen\t0\t0\t-324.031565551\t-326.803584842\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\t-463.882016539\t-415.909813035\r\n",
      "0002.2003-12-18.GP\t1\t1\t-908.548713672\t-814.655756641\r\n",
      "Error Rate: 0/10\r\n",
      "Zero Probabilities: Spam 0 \tHam 0\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./term_mapper.py | sort -r -k1,1 | ./probability_reducer.py > term_probabilities.txt\n",
    "!cat enronemail_1h.txt | head | ./email_mapper.py | sort -k1,1 | ./classifier_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.5 - Processing With Hadoop\n",
    "\n",
    "It is assumed that the hadoop processes are still running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/rcordell/model': No such file or directory\n",
      "rm: `/user/rcordell/classifier': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model\n",
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop Stage 1 MapReduce with Smoothing\n",
    "\n",
    "The Hadoop streaming input has two more parameters:\n",
    "\n",
    "- jobconf stream.num.map.output.key.fields=4\n",
    "- jobconf stream.num.reduce.output.key.fields=3\n",
    "\n",
    "These are used to tell Hadoop which values in the tab-delimited key,value pairs make up the key. Otherwise Hadoop only uses the first field. This will give use more complete sorting control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -mapper term_mapper.py \\\n",
    "    -reducer probability_reducer.py \\\n",
    "    -input enronemail_1h.txt \\\n",
    "    -output model \\\n",
    "    -jobconf stream.num.map.output.key.fields=4 \\\n",
    "    -jobconf stream.num.reduce.output.key.fields=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy model file to local file system from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f term_probabilities.txt\n",
    "!hdfs dfs -get /user/rcordell/model/part-00000 term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Hadoop Stage 2 MapReduce\n",
    "\n",
    "Here we only want Hadoop to sort on the first field so we don't change the default behavior as we did in Stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [term_probabilities.txt] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob5789292786380840881.jar tmpDir=null\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -mapper email_mapper.py \\\n",
    "    -reducer classifier_reducer.py \\\n",
    "    -input enronemail_1h.txt \\\n",
    "    -output classifier \\\n",
    "    -file term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the output of the classifier - Error Rate\n",
    "\n",
    "The error rate has increased to 4% with the addition of removing terms with counts < 3 from the model. That indicates that those terms carried enough information to make a difference in the classification rate.\n",
    "\n",
    "The classifier doesn't consider tokens that are not in the vocabulary, so they do not contribute to the calculation of the document class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 4/100\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"error rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Probability Counts\n",
    "\n",
    "This is the count of how many zero probabilities were encountered for each class. Notice how there are now no zero probabilities for either class as a result of the Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Probabilities: Spam 0 \tHam 0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"Zero Probabilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.5 - Log Probability Distribution\n",
    "\n",
    "The histograms below are a plot of the log probability distribution for both ham and spam classes.\n",
    "\n",
    "There doesn't seem to be much change from the Laplace smoothing histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get classification output file from HDFS\n",
    "!rm classification_results.txt\n",
    "!hdfs dfs -get /user/rcordell/classifier/part-00000 classification_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x116dc97d0>"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2kAAAFCCAYAAABrfJV6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11425a550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ham = []\n",
    "spam = []\n",
    "with open('classification_results.txt','r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            _id, label, cls, lh, ls = line.strip().split('\\t')\n",
    "            try:\n",
    "                ham.append(float(lh))\n",
    "            except:\n",
    "                ham.append(0.0)\n",
    "            try:\n",
    "                spam.append(float(ls))\n",
    "            except:\n",
    "                spam.append(0.0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "p = plt.subplot(1, 2, 1)\n",
    "p.hist(ham,100)\n",
    "plt.title('Ham Log Probability Frequencies')\n",
    "\n",
    "p = plt.subplot(1, 2, 2)\n",
    "p.hist(spam,100)\n",
    "plt.title('Spam Log Probability Frequencies')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/classifier\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Yarn and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.6 \n",
    "\n",
    "Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.6 - Scikit-Learn Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# replicate our mapper code here where we take the subject and body together\n",
    "# except now we grab the label field as well to use for the Y values\n",
    "with open('enronemail_1h.txt', 'rU') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            email_id, label, subject, body = line.split('\\t')\n",
    "            X_train.append(subject + ' ' + body)\n",
    "        except ValueError:\n",
    "            email_id, label, body = line.split('\\t')\n",
    "            X_train.append(body)\n",
    "        # extract only words from the combined subject and body text\n",
    "        Y_train.append(int(label))\n",
    "\n",
    "# Use the TfidVectorizer to create the feature vectors\n",
    "# We should override the tokenizer regular expression to make it the same as what we used\n",
    "# in our poor man's mapper code\n",
    "vectorizer = TfidfVectorizer(token_pattern = \"[\\w']+\")\n",
    "vf = vectorizer.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.6 - Multinomial Bayes Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(vf.fit_transform(X_train), Y_train)\n",
    "print 1.0 - clf.score(vf.fit_transform(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.6 - Results\n",
    "\n",
    "The following table summarizes our results of the various classification method training error. The difference between the HW 2.5 Multinomial NB classifier and the Scikit Learn is that the term vectorizor using the Scikit learn uses all terms, whereas the HW2.5 version does not.\n",
    "\n",
    "| Classification Methodology  | Training error |\n",
    "|-----------------------------|----------------|\n",
    "| Multinomial NB no smoothing    |      1%       |\n",
    "| Multinomial NB Laplace +1 smoothing  |      0%       |\n",
    "| Multinomial NB Smoothing & Term Count >= 3  |      4%        |\n",
    "| scikit-learn MultinomialNB  |      0%        |\n",
    "| scikit-learn BernoulliNB    |     18%        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.6.1 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "\n",
    "—  Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "-  Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "\n",
    "Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.6.1 - Bernoulli Bayes Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "clf.fit(vf.fit_transform(X_train), Y_train)\n",
    "print 1.0 - clf.score(vf.fit_transform(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.7 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "\n",
    "The Enron SPAM data in the following folder enron1-Training-Data-RAW is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "\n",
    "--- Line 1 contains the subject\n",
    "--- The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.8 OPTIONAL\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    " -- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).\n",
    " -- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). How do we treat tokens in the test set that do not appear in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.8.1 OPTIONAL\n",
    "—  Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW2.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "-  Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
