{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Spring 2016 Homework Week 2\n",
    "\n",
    "Ron Cordell<br />\n",
    "W261-4<br />\n",
    "ron.cordell@ischool.berkeley.edu<br />\n",
    "January 26, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a race condition in the context of parallel computation? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A race condition in parallel computing can happen when more than one instance of a procedure attempts to modify a shared object. For example, suppose there are two instances of a process that number with its square, and that both instances operate on the same number. There is nothing to control the actual timing of execution of these instances. If one instance of the process performs its work after the other instance then the answer will be the number raised to the 4th power and not the square. If both instances get the number, compute the square, one instance will write the number before the other, and the second one will overwrite the first. In this case, however, we get the correct answer. The problem is that we don't know deterministically which case will happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is MapReduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is a two-stage functional programming recipe for processing large data sets.\n",
    "\n",
    "- The first stage performs a computation over all inputs, called a 'map' from Lisp terminology\n",
    "- The second stage aggregates the intermediate output from the first stage, called a 'reduce'\n",
    "\n",
    "While seemingly simple, map reduce can be applied to a huge number of problems and can be used in multiple map-reduce stages for more complex scenarios. When a map function can be applied very simply using commutative and associative types of operations it lends to supporting embarrasingly parallel scenarios. While MapReduce has been around in functional programming languages like Lisp since the 1960's, it's modern use has come to the forefront as a result of a combination of commidity priced hardware compute and storage nodes and the [Google paper](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it differ from Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop provides a framework in which to execute map-reduce and relieves the programmer of certain tasks while providing additional functionality. For example, Hadoop gathers and sorts the `(key,value)` pairs output by the mapper stage and routes them to the appropriate reducer, ensuring that each reducer has a complete set values for the keys given. This intermediate sort and route is part of the Hadoop Shuffle, which is the backbone of Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is based upon the map-reduce programming paradigm which is designed to allow parallel distributed processing of large sets of data by mapping/transforming them to sets of tuples then combining and reducing/aggregating them to smaller sets of tuples. The map and reduce steps are written by the user. \n",
    "\n",
    "What follows is a simple example of a map reduce application. The map and reduce steps are written in Python while the execution of the steps is executed by bash shell commands. This mapper and reducer count the number of occurances of a specified word in a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Simple Example\n",
    "\n",
    "What follows is a simple example of a map reduce application. The map and reduce steps are written in Python while the execution of the steps is executed by bash shell commands. This mapper and reducer count the number of occurances of a specified word in a text file.\n",
    "\n",
    "#### MapReduce Example - Map\n",
    "\n",
    "This map step reads a file from the local disk line by line, then breaks each line down to individual words.\n",
    "It then compares each word, ignoring case, to the specified search word and accumulates a count for that line. When all the lines have been examined the mapper writes the count to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "# the word to find is the first argument\n",
    "findword = sys.argv[1].lower()\n",
    "\n",
    "# the file to scan is the second argument\n",
    "filename = sys.argv[2]\n",
    "\n",
    "# open the file, read it line by line\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # process each word in the line by filtering out non-words and if it matches\n",
    "        # the search word, incrementing the word count\n",
    "        for word in WORD_RE.findall(line):\n",
    "            # make this a case-insensitve search\n",
    "            if word.lower() == findword:\n",
    "                count += 1\n",
    "            \n",
    "# now that we've counted this line, write out the count\n",
    "with open ('{0}.intermediateCount'.format(filename),'w') as outfile:\n",
    "    outfile.write('{0}\\n'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the appropriate execution mode on the file so we can execute it\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Example - Reduce\n",
    "\n",
    "The reduce step takes as input series of counts and accumulates them into a single count. The reducer recieves its input on STDIN and writes its output to STDOUT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        sum += int(line)\n",
    "    except:\n",
    "        pass\n",
    "sys.stdout.write('{0}'.format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the appropriate execution mode on the file so we can execute it\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce Example - Putting it together\n",
    "\n",
    "The following bash shell script takes the file to scan and breaks it into separate sub-files, each containing a chunk of the original file. It then invokes several instances of the mapper.py program, one for each chunk, and supplies one of the chunked files. The bash script waits for all mappers to finish and then It then takes the output files from each of the mapper.py programs and steams them one after the other to the reducer.py program.\n",
    "\n",
    "The output of the MapReduce is the number of times a word has occurred in the file:\n",
    "\n",
    "     found [59] [COPYRIGHT] in the file [LICENSE.txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pGrepCount.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pGrepCount.sh\n",
    "ORIGINAL_FILE=$1\n",
    "FIND_WORD=$2\n",
    "BLOCK_SIZE=$3\n",
    "CHUNK_FILE_PREFIX=$ORIGINAL_FILE.split\n",
    "SORTED_CHUNK_FILES=$CHUNK_FILE_PREFIX*.sorted\n",
    "usage()\n",
    "{\n",
    "    echo Parallel grep\n",
    "    echo usage: pGrepCount filename word chuncksize\n",
    "    echo greps file file1 in $ORIGINAL_FILE and counts the number of lines\n",
    "    echo Note: file1 will be split in chunks up to $ BLOCK_SIZE chunks each\n",
    "    echo $FIND_WORD each chunk will be grepCounted in parallel\n",
    "}\n",
    "#Splitting $ORIGINAL_FILE INTO CHUNKS\n",
    "split -b $BLOCK_SIZE $ORIGINAL_FILE $CHUNK_FILE_PREFIX\n",
    "#DISTRIBUTE\n",
    "for file in $CHUNK_FILE_PREFIX*\n",
    "do\n",
    "    #grep -i $FIND_WORD $file|wc -l >$file.intermediateCount &\n",
    "    ./mapper.py $FIND_WORD $file >$file.intermediateCount &\n",
    "done\n",
    "wait\n",
    "#MERGEING INTERMEDIATE COUNT CAN TAKE THE FIRST COLUMN AND TOTAL...\n",
    "#numOfInstances=$(cat *.intermediateCount | cut -f 1 | paste -sd+ - |bc)\n",
    "numOfInstances=$(cat *.intermediateCount | ./reducer.py)\n",
    "echo \"found [$numOfInstances] [$FIND_WORD] in the file [$ORIGINAL_FILE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [59] [COPYRIGHT] in the file [LICENSE.txt]\r\n"
     ]
    }
   ],
   "source": [
    "# set the permissions on the bash script so it can execute\n",
    "!chmod a+x pGrepCount.sh\n",
    "\n",
    "# execute the MapReduce example and output the result. We will look for the word 'assistance'\n",
    "# the '4K' tells the bash script to break the original file down into chunks of no more than\n",
    "# 4K bytes in size\n",
    "!./pGrepCount.sh LICENSE.txt COPYRIGHT 4k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1. Sort in Hadoop MapReduce\n",
    "\n",
    "#### Given as input: Records of the form `<integer, “NA”>`, where integer is any integer, and “NA” is just the empty string.  Output: sorted key value pairs of the form `<integer, “NA”>` in decreasing order.  What happens if you have multiple reducers? Do you need additional steps? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple reducers each one will have a locally sorted set of records but won't be able to coordinate with the other reducers to produce a complete sorted list. However, it is possible to customize a partitioner to partition the keys across reducers such that reducer 1 has the lowest key values, reducer 2 has the next lowest, and so on. Then the output of the reducers can be easily merged based on the number of the reducer. Technically this is an additional step. Another way to deal with multiple reducers is to have an additional step of taking the output of each of the reducers and sorting that into a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000. Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Record Generator\n",
    "\n",
    "The `recordgneratory.py` accepts an argument to indicate the number of records to generate. Records are generated by selecting a random integer in the range of 1 to the maximum integer value allowed. That integer is appended with the string \",NA\" and written to the output file as a record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting recordgenerator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile recordgenerator.py\n",
    "#!/usr/bin/python\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# get number of records to generate\n",
    "num_records = int(sys.argv[1])\n",
    "\n",
    "# generator function to create random integers in the range of 1 to sys.maxint\n",
    "def gen(n):\n",
    "    count = 0\n",
    "    while count < n:\n",
    "        yield (random.randint(1, sys.maxint))\n",
    "        count += 1\n",
    "        \n",
    "# Generate a set of num_record key,value pairs with integer keys generated by the generator\n",
    "# and write them to an output file in the form of \"integer,NA\"\n",
    "for i in gen(num_records):\n",
    "    sys.stdout.write('{0},{1}\\n'.format(i,\"NA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x recordgenerator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Map\n",
    "\n",
    "For each line that is read from STDIN we expect a string in the form of:\n",
    "\n",
    "     key,value\n",
    "\n",
    "Write each `key, value` pair to STDOUT as `key<tab>value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# each record comes via STDIN one record at a time\n",
    "for record in sys.stdin:\n",
    "    # clean whitespace and split at the comma\n",
    "    k,v = record.strip().split(',')\n",
    "    # write to STDOUT as key <tab> value\n",
    "    print '{0}\\t{1}'.format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Reduce\n",
    "\n",
    "For each `key<tab>value` read from STDIN write back out to STDOUT. The output is sorted because the input is already sorted for us by the record key, thanks to Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# read each key,value pair from STDIN and write back out to STDOUT\n",
    "# we can do this because the keys are sorted for us by Hadoop\n",
    "for pair in sys.stdin:\n",
    "    k,v = pair.strip().split('\\t')\n",
    "    print '{0}\\t{1}'.format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Test Code\n",
    "\n",
    "Test the mapper and reducer by piping a small test set to the mapper, sort the output of the mapper based on the integer value (key), pipe the result to the reducer, then filter for the top 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8470506191635231078\tNA\r\n",
      "8026439827727903483\tNA\r\n",
      "7914562750245791116\tNA\r\n",
      "7866281371022919675\tNA\r\n",
      "7640348753853655778\tNA\r\n",
      "6499377995366904625\tNA\r\n",
      "5277692479661556743\tNA\r\n",
      "4708012296169908311\tNA\r\n",
      "4590245591349809339\tNA\r\n",
      "4346827061492181849\tNA\r\n"
     ]
    }
   ],
   "source": [
    "!./recordgenerator.py 20 | ./mapper.py | sort -nr -k1,1 | ./reducer.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.1 - Running in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Yarn and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-rcordell-resourcemanager-Rons-iMac-Retina.local.out\n",
      "localhost: no such identity: /Users/rcordell/.ssh/id_dsa: No such file or directory\n",
      "localhost: Saving password to keychain failed\n",
      "localhost: Identity added: /Users/rcordell/.ssh/id_rsa ((null))\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-rcordell-nodemanager-Rons-iMac-Retina.local.out\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-namenode-Rons-iMac-Retina.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-datanode-Rons-iMac-Retina.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-secondarynamenode-Rons-iMac-Retina.local.out\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an HDFS folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and upload record file containing 10000 records to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./recordgenerator.py 10000 > records.txt\n",
    "!hdfs dfs -put records.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop Streaming Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     hadoop jar hadoopstreamingjarfile \\\n",
    "          -D stream.num.map.output.key.fields=n \\\n",
    "          -mapper mapperfile \\\n",
    "          -reducer reducerfile \\\n",
    "          -input inputfile \\\n",
    "          -output outputfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit our MapReduce to Hadoop using Hadoop Streaming. Besides specifying the mapper, reducer, input file and output location, we also specify to use the KeyFieldBasedComparator and set the keycomparator options. This allows us to specify that the sorting will be done on the key field and to treat the key as a numeric value and to reverse sort (descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=-nr \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input records.txt \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First 10 Items\n",
    "\n",
    "The sorted records output of the MapReduce is on the HDFS file system in a file called `part-00000`. Let's look at the top 10 lines in the file, which should be the highest numbered records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9223149662631926773\tNA\r\n",
      "9222462994771806812\tNA\r\n",
      "9221376548595256568\tNA\r\n",
      "9221200099560410483\tNA\r\n",
      "9216651204641081164\tNA\r\n",
      "9216610805780812765\tNA\r\n",
      "9215182420885073012\tNA\r\n",
      "9214763879526561081\tNA\r\n",
      "9214470163579875161\tNA\r\n",
      "9212397989479861727\tNA\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last 10 Items\n",
    "\n",
    "Now let's look at the last 10 items in the file, which should be the lowest record ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9242791484899512\tNA\r\n",
      "8658217614831616\tNA\r\n",
      "7790490907682806\tNA\r\n",
      "7435515921714919\tNA\r\n",
      "4424418503692755\tNA\r\n",
      "4258466038084507\tNA\r\n",
      "3781763814522545\tNA\r\n",
      "3693207216031204\tNA\r\n",
      "2915017580505790\tNA\r\n",
      "1594448856056738\tNA\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000 | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up HDFS output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/recordsOutput\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/records.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/rcordell/records.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Yarn and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2.  WORDCOUNT\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    " \n",
    "CROSSCHECK: \n",
    "    >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "     8    \n",
    "\n",
    "#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.2\n",
    "## Given a file and list of words, read lines and count occurrences of words\n",
    "## Output a key, value => (id, class, token, term_flag) = count\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# extract words to count from first positional argument\n",
    "wordlist = sys.argv[1].split()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split('\\t')      \n",
    "    if len(fields) > 3:\n",
    "        # we are interested in subject and body but need to be resilient for missing fields\n",
    "        text = '{0} {1}'.format(fields[2],fields[3])\n",
    "    elif len(fields) > 2:\n",
    "        text = fields[2]\n",
    "    # extract only words from the combined subject and body text\n",
    "    for word in WORD_RE.findall(text):\n",
    "        if word.lower() in wordlist:\n",
    "            print('{0}\\t{1}'.format(word.lower(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW2.2\n",
    "## given a list of key,value pairs for word, count, aggregate and output the list\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./mapper.py \"assistance\" | ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in Hadoop\n",
    "\n",
    "### Start YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HDFS Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Enron email file to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hadoop Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapreduce.partition.keycomparator.options=-nr -mapper \"mapper.py 'assistance'\" -reducer reducer.py -input enronemail_1h.txt -output countsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/countsOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/countsOutput\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/countsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2.1  \n",
    "\n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "We need to generalize the mapper..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.2\n",
    "## Given a file and list of words, read lines and count occurrences of words\n",
    "## Output a key, value => (id, class, token, term_flag) = count\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "wordlist = []\n",
    "\n",
    "# extract words to count from first positional argument\n",
    "if len(sys.argv) > 1:\n",
    "    wordlist = sys.argv[1].split()    \n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split('\\t')      \n",
    "    if len(fields) > 3:\n",
    "        # we are interested in subject and body but need to be resilient for missing fields\n",
    "        text = '{0} {1}'.format(fields[2],fields[3])\n",
    "    elif len(fields) > 2:\n",
    "        text = fields[2]\n",
    "    # extract only words from the combined subject and body text\n",
    "    # if a wordlist parameter was supplied, count only those words\n",
    "    for word in WORD_RE.findall(text):\n",
    "        if len(wordlist) > 0:\n",
    "            if word.lower() in wordlist:\n",
    "                print('{0}\\t{1}'.format(word.lower(), 1))\n",
    "        else:\n",
    "            # otherwise count all words\n",
    "            print('{0}\\t{1}'.format(word.lower(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW2.2\n",
    "## given a list of key,value pairs for word, count, aggregate and output the list\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "all_words = []\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # append result to list\n",
    "            all_words.append((current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    all_words.append((current_word, current_count))\n",
    "    \n",
    "# sort the list by value\n",
    "sorted_words = sorted(all_words, key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "# write out the top 10\n",
    "for i,word in enumerate(sorted_words):\n",
    "    print '{0}\\t{1}'.format(word[0],word[1])\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1247\r\n",
      "to\t963\r\n",
      "and\t668\r\n",
      "of\t566\r\n",
      "a\t542\r\n",
      "you\t432\r\n",
      "in\t417\r\n",
      "your\t394\r\n",
      "ect\t382\r\n",
      "for\t373\r\n",
      "on\t271\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./mapper.py | sort -r -k1,1 | ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in Hadoop\n",
    "\n",
    "### Start YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HDFS Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Enron email file to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hadoop Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output countsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1247\r\n",
      "to\t963\r\n",
      "and\t668\r\n",
      "of\t566\r\n",
      "a\t542\r\n",
      "you\t432\r\n",
      "in\t417\r\n",
      "your\t394\r\n",
      "ect\t382\r\n",
      "for\t373\r\n",
      "on\t271\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/countsOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/countsOutput\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/countsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the log posterior probabilities (i.e., log(Pr(Class|Doc))) for each class over the training set. Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Stage 1 - Term Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting term_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile term_mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.3\n",
    "## Given input on STDIN read lines and count occurrences of words\n",
    "## Output a key, value => (token, email id, class, term_flag) = count\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "all_words = False\n",
    "## Words in the word list are space delimited\n",
    "## If there are no parameters or the first parameter is '*' \n",
    "## then use all tokens as vocabulary terms\n",
    "if len(sys.argv) > 1:\n",
    "    wordlist = sys.argv[1].lower().split(' ')\n",
    "    if wordlist[0] == '*':\n",
    "        all_words = True\n",
    "else:\n",
    "    all_words = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        email_id, label, subject, body = line.split('\\t')\n",
    "    except ValueError:\n",
    "        email_id, label, body = line.split('\\t')\n",
    "    \n",
    "    # extract only words from the combined subject and body text\n",
    "    for token in WORD_RE.findall(subject + ' ' + body):\n",
    "        term = '0'\n",
    "        if all_words:\n",
    "            term = '1'\n",
    "        elif token.lower() in wordlist:\n",
    "            term = '1'\n",
    "        print('{0}\\t{1}\\t{2}\\t{3}\\t{4}'.format(token, email_id, label, term, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x term_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Stage 1 - Term Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting probability_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile probability_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "counts = {}\n",
    "term_counts = {}\n",
    "spam_doc_ids = []\n",
    "ham_doc_ids = []\n",
    "spam_doc_word_count = 0.0\n",
    "ham_doc_word_count = 0.0\n",
    "spam_term_count = 0.0\n",
    "ham_term_count = 0.0\n",
    "terms = 0.0\n",
    "current_key = None\n",
    "current_count = 0\n",
    "\n",
    "# STDIN consists of single lines of: token <tab> id <tab> class <tab> term_flag <tab> count\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "def accumulate_key(key , _count, spam_doc_word_count, ham_doc_word_count, term_counts, counts):\n",
    "    # accumulate the key, values in a dictionary\n",
    "    _token = key[0]\n",
    "    _id = key[1]\n",
    "    _label = key[2]\n",
    "    _term = key[3]\n",
    "    \n",
    "    if key in counts:\n",
    "        counts[key] += _count\n",
    "    else:\n",
    "        counts[key] = _count\n",
    "\n",
    "    # accumulate into ham and spam dictionaries also\n",
    "    if _label == '1':\n",
    "        spam_doc_word_count += _count\n",
    "        if _id not in spam_doc_ids:\n",
    "            spam_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['spam_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0} \n",
    "    else:                \n",
    "        ham_doc_word_count += _count\n",
    "        if _id not in ham_doc_ids:\n",
    "            ham_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['ham_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0}\n",
    "    return spam_doc_word_count, ham_doc_word_count\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    token, email_id, label, term, token_count = line.split('\\t')\n",
    "\n",
    "    if term == '0':\n",
    "        vocab_word = False\n",
    "    else:\n",
    "        vocab_word = True\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key before it is passed to the reducer\n",
    "    if current_key == (token, email_id, label, term):\n",
    "        current_count += int(token_count)\n",
    "    else:\n",
    "        if current_key:\n",
    "            spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                                     float(current_count), \n",
    "                                                                     spam_doc_word_count, \n",
    "                                                                     ham_doc_word_count, \n",
    "                                                                     term_counts, counts)\n",
    "            \n",
    "        current_count = int(token_count)\n",
    "        current_key = (token, email_id, label, term)\n",
    "\n",
    "# add the last key, value that we've received\n",
    "if current_key == (token, email_id, label, term):\n",
    "    spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                             float(current_count), \n",
    "                                                             spam_doc_word_count, \n",
    "                                                             ham_doc_word_count, \n",
    "                                                             term_counts, counts)      \n",
    "                       \n",
    "# now we should have consolidated the intermediate counts and we can compute the rest\n",
    "\n",
    "# count the number of terms\n",
    "term_count = len(term_counts.keys()) * 1.0\n",
    "# compute the prior\n",
    "prior = (len(spam_doc_ids)*1.0)/(1.0*(len(spam_doc_ids) + len(ham_doc_ids)))\n",
    "\n",
    "# calculate the P(term|class) for each term\n",
    "for term in term_counts:\n",
    "    term_counts[term]['prob_ham'] = (term_counts[term]['ham_count'])/(ham_doc_word_count + term_count)\n",
    "    term_counts[term]['prob_spam'] = (term_counts[term]['spam_count'])/(spam_doc_word_count + term_count)\n",
    "    \n",
    "    # output term <tab> probability_ham <tab> ham_count <tab> probability_spam <tab> spam_count <tab> prior\n",
    "    print '{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(term, term_counts[term]['prob_ham'], \n",
    "                                           term_counts[term]['ham_count'],\n",
    "                                           term_counts[term]['prob_spam'], \n",
    "                                           term_counts[term]['spam_count'], \n",
    "                                           prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x probability_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stage 1 Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat enronemail_1h.txt | ./term_mapper.py | sort -r -k1,1 | ./probability_reducer.py > term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Stage 2 - Email Word Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting email_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile email_mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.3\n",
    "## Given input on STDIN read lines and count occurrences of words\n",
    "## Output a key, value => (token, email id, class, term_flag) = count\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "prior = 0.0\n",
    "terms = {}\n",
    "# open the file with the term probabilities\n",
    "with open('term_probabilities.txt','r') as termfile:\n",
    "    for line in termfile.readlines():\n",
    "        term, ham_prob, ham_count, spam_prob, spam_count, prior = line.strip().split('\\t')\n",
    "        terms[term] = {'ham_prob'  : float(ham_prob),  'ham_count'  : float(ham_count),\n",
    "                       'spam_prob' : float(spam_prob), 'spam_count' : float(spam_count)}\n",
    "        prior = float(prior)\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tokens = {}\n",
    "    try:\n",
    "        email_id, label, subject, body = line.split('\\t')\n",
    "    except ValueError:\n",
    "        email_id, label, body = line.split('\\t')\n",
    "    \n",
    "    # extract only words from the combined subject and body text\n",
    "    for token in WORD_RE.findall(subject + ' ' + body):\n",
    "        if token.lower() in terms:\n",
    "            if token.lower() in tokens:\n",
    "                tokens[token.lower()]['count'] += 1\n",
    "            else:\n",
    "                tokens[token.lower()] = {'count' : 1}\n",
    "                \n",
    "    # emit the accumulated terms for the email\n",
    "    if len(tokens) > 0:\n",
    "        for token in tokens:\n",
    "            print('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}'.format(email_id, label, token,\n",
    "                                                terms[token]['ham_prob'],\n",
    "                                                terms[token]['spam_prob'],\n",
    "                                                prior,\n",
    "                                                tokens[token]['count']))\n",
    "    else:\n",
    "        # if an email has no vocabulary terms then emit a single record indicating a zero count\n",
    "        print('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}'.format(email_id, label, '*',\n",
    "                                            0.0,\n",
    "                                            0.0,\n",
    "                                            prior,\n",
    "                                            0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x email_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Stage 2 - Email Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classifier_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile classifier_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "emails = {}\n",
    "current_email = None\n",
    "prob_spam = 0.0\n",
    "prob_ham  = 0.0\n",
    "zero_prob_ham = 0\n",
    "zero_prob_spam = 0\n",
    "pred_label = 0\n",
    "right = 0\n",
    "wrong = 0\n",
    "\n",
    "# STDIN consists of single lines of: id <tab> label <tab> token <tab> p_ham <tab> p_spam <tab> prior <tab> count\n",
    "# Assume the mapper has already aggregated terms in each email\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    email_id, label, token, _p_ham, _p_spam, _prior, _token_count = line.split('\\t')\n",
    "    \n",
    "    # convert strings to numeric\n",
    "    p_ham = float(_p_ham)\n",
    "    p_spam = float(_p_spam)\n",
    "    prior = float(_prior)\n",
    "    token_count = float(_token_count)\n",
    "        \n",
    "    # accumulate by email id\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key before it is passed to the reducer\n",
    "    if current_email == email_id:\n",
    "        if p_ham != 0.0:\n",
    "            prob_ham += log(p_ham) * token_count\n",
    "        else:\n",
    "            zero_prob_ham += 1\n",
    "        if p_spam != 0.0:\n",
    "            prob_spam += log(p_spam) * token_count\n",
    "        else:\n",
    "            zero_prob_spam += 1\n",
    "    else:\n",
    "        if current_email:\n",
    "            # emit the classification\n",
    "            if prob_spam > prob_ham:\n",
    "                pred_label = '1'\n",
    "            else:\n",
    "                pred_label = '0'\n",
    "            if label == pred_label:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "            print '{0}\\t{1}\\t{2}'.format(email_id, label, pred_label)\n",
    "            \n",
    "            # add to the list of posteriors\n",
    "            emails[current_email] = {'lp_ham' : prob_ham, 'lp_spam' : prob_spam}\n",
    "        \n",
    "         # if this new email has no terms then we predict ham but have no posteriors\n",
    "        if token == '*':\n",
    "            print '{0}\\t{1}\\t{2}'.format(email_id, label, '0')\n",
    "            if label == '0':\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "            emails[email_id] = {'lp_ham' : '-', 'lp_spam' : '-'}\n",
    "            current_email = None\n",
    "        else:\n",
    "            # otherwise make this the new current email and reset the probabilities\n",
    "            current_email = email_id\n",
    "            if p_ham != 0.0:\n",
    "                prob_ham = log(1-prior) + (log(p_ham) * token_count)\n",
    "            else:\n",
    "                zero_prob_ham += 1\n",
    "            if p_spam != 0.0:\n",
    "                prob_spam = log(prior) + (log(p_spam) * token_count)\n",
    "            else:\n",
    "                zero_prob_spam += 1\n",
    "\n",
    "\n",
    "# add the last key, value that we've received\n",
    "if current_email == email_id:\n",
    "    # emit the classification\n",
    "    if prob_spam > prob_ham:\n",
    "        pred_label = '1'\n",
    "    else:\n",
    "        pred_label = '0'\n",
    "    if label == pred_label:\n",
    "        right += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "    print '{0}\\t{1}\\t{2}'.format(email_id, label, pred_label)    \n",
    "    emails[email_id] = {'lp_ham' : prob_ham, 'lp_spam' : prob_spam} \n",
    "    \n",
    "print 'Error Rate: {0}/{1}'.format(wrong, right + wrong)\n",
    "print 'Zero Probabilities: Spam {0} \\tHam {1}'.format(zero_prob_spam, zero_prob_ham)\n",
    "for e in emails:\n",
    "    print '{0}\\t{1}\\t{2}\\t{3}'.format(':',e, emails[e]['lp_ham'], emails[e]['lp_spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x classifier_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t1\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0002.2004-08-01.BG\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t1\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t1\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t1\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t1\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\n",
      "0010.2003-12-18.GP\t1\t1\n",
      "0010.2004-08-01.BG\t1\t1\n",
      "0011.1999-12-14.farmer\t0\t1\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t1\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0013.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0016.1999-12-15.farmer\t0\t1\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t1\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\n",
      "0018.2003-12-18.GP\t1\t0\n",
      "0018.2003-12-18.GP\t1\t0\n",
      "Error Rate: 48/100\n",
      "Zero Probabilities: Spam 0 \tHam 5298\n",
      ":\t0010.2003-12-18.GP\t-5589.42260646\t-57.9169161776\n",
      ":\t0010.2001-06-28.SA_and_HP\t-5582.66896563\t-3534.06260807\n",
      ":\t0001.2000-01-17.beck\t-3075.62262467\t-4267.44484079\n",
      ":\t0018.1999-12-14.kaminski\t-875.582397534\t-1028.13806721\n",
      ":\t0005.1999-12-12.kaminski\t-495.61355681\t-869.87561965\n",
      ":\t0011.2001-06-29.SA_and_HP\t-12166.6126843\t-16571.4272673\n",
      ":\t0008.2004-08-01.BG\t-3382.93845117\t-6141.87252592\n",
      ":\t0009.1999-12-14.farmer\t-6059.27761475\t-665.234403899\n",
      ":\t0017.2003-12-18.GP\t-570.831245165\t-219.728768793\n",
      ":\t0011.2001-06-28.SA_and_HP\t-3933.77470954\t-3526.04843769\n",
      ":\t0015.2001-07-05.SA_and_HP\t-657.786176683\t-966.656375719\n",
      ":\t0015.2001-02-12.kitchen\t-4463.05740084\t-5460.5985123\n",
      ":\t0009.2001-06-26.SA_and_HP\t-821.128777798\t-1373.48335389\n",
      ":\t0017.1999-12-14.kaminski\t-316.678760785\t-399.935902784\n",
      ":\t0012.2000-01-17.beck\t-2953.28023319\t-3255.79079856\n",
      ":\t0003.2000-01-17.beck\t-1239.20285935\t-1531.23273119\n",
      ":\t0004.2001-06-12.SA_and_HP\t-706.460766449\t-924.740851926\n",
      ":\t0008.2001-06-12.SA_and_HP\t-706.460766449\t-924.740851926\n",
      ":\t0007.2001-02-09.kitchen\t-1541.7711822\t-1823.56314705\n",
      ":\t0016.2004-08-01.BG\t-13439.0326566\t-683.332682392\n",
      ":\t0015.2000-06-09.lokay\t-129.280324018\t-153.284889743\n",
      ":\t0005.1999-12-14.farmer\t-941.651381488\t-1395.84247013\n",
      ":\t0016.1999-12-15.farmer\t-679.656887692\t-797.45318903\n",
      ":\t0013.2004-08-01.BG\t-950.653518711\t-1567.85420848\n",
      ":\t0005.2003-12-18.GP\t-4140.49513768\t-7886.63061803\n",
      ":\t0012.2001-02-09.kitchen\t-398.315903407\t-534.638206981\n",
      ":\t0003.2001-02-08.kitchen\t-1321.35596083\t-1573.92764125\n",
      ":\t0009.2001-02-09.kitchen\t-5716.39662683\t-6441.26434061\n",
      ":\t0006.2001-02-08.kitchen\t-8463.71869725\t-10557.1164388\n",
      ":\t0014.2003-12-19.GP\t-110.779390598\t-179.016861573\n",
      ":\t0010.1999-12-14.farmer\t-947.134990916\t-1833.33225662\n",
      ":\t0010.2004-08-01.BG\t-6500.69676872\t-2450.91893237\n",
      ":\t0014.1999-12-14.kaminski\t-1786.95238985\t-2171.41337468\n",
      ":\t0006.1999-12-13.kaminski\t-4544.83476027\t-522.824256281\n",
      ":\t0011.1999-12-14.farmer\t-1614.8262919\t-2107.74610405\n",
      ":\t0013.1999-12-14.kaminski\t-1324.93424947\t-1615.8654452\n",
      ":\t0001.2001-02-07.kitchen\t-361.538166813\t-399.809222881\n",
      ":\t0008.2001-02-09.kitchen\t-3035.59962147\t-4606.74478681\n",
      ":\t0007.2003-12-18.GP\t-567.86129971\t-1313.8342645\n",
      ":\t0017.2004-08-02.BG\t-1873.89178637\t-2738.95421301\n",
      ":\t0014.2004-08-01.BG\t-496.41639499\t-844.3766959\n",
      ":\t0006.2003-12-18.GP\t-409.289908567\t-1179.74315062\n",
      ":\t0016.2001-07-05.SA_and_HP\t-657.786176683\t-966.656375719\n",
      ":\t0008.2003-12-18.GP\t-665.056152937\t-1055.67263665\n",
      ":\t0014.2001-07-04.SA_and_HP\t-3777.74685829\t-3737.57130506\n",
      ":\t0001.2001-04-02.williams\t-1189.30098401\t-1432.25624092\n",
      ":\t0012.2000-06-08.lokay\t-726.501217948\t-938.3990004\n",
      ":\t0014.1999-12-15.farmer\t-1155.16781976\t-1370.21820694\n",
      ":\t0009.2000-06-07.lokay\t-2222.30067337\t-3019.04022831\n",
      ":\t0001.1999-12-10.farmer\t-28.3013134851\t-46.7781806525\n",
      ":\t0008.2001-06-25.SA_and_HP\t-2853.86647459\t-4591.68478098\n",
      ":\t0017.2001-04-03.williams\t-397.877517008\t-460.459720533\n",
      ":\t0014.2001-02-12.kitchen\t-963.039097224\t-1586.99757737\n",
      ":\t0016.2001-07-06.SA_and_HP\t-12679.6228099\t-17486.9395627\n",
      ":\t0015.1999-12-15.farmer\t-872.788399651\t-994.475848827\n",
      ":\t0009.1999-12-13.kaminski\t-5709.60455782\t-8768.18025707\n",
      ":\t0001.2000-06-06.lokay\t-3323.93690665\t-4109.84380722\n",
      ":\t0011.2004-08-01.BG\t-645.313414179\t-699.538470066\n",
      ":\t0004.2004-08-01.BG\t-418.187087379\t-825.407181222\n",
      ":\t0018.2003-12-18.GP\t-2470.70843907\t-3617.52854909\n",
      ":\t0002.1999-12-13.farmer\t-2530.97096045\t-3462.63894679\n",
      ":\t0016.2003-12-19.GP\t-12961.9409263\t-825.461464016\n",
      ":\t0004.1999-12-14.farmer\t-993.97067235\t-1474.32027764\n",
      ":\t0015.2003-12-19.GP\t-1413.13126518\t-1341.0092708\n",
      ":\t0006.2004-08-01.BG\t-536.840487934\t-1098.12913\n",
      ":\t0009.2003-12-18.GP\t-1099.53754143\t-804.432057413\n",
      ":\t0007.1999-12-14.farmer\t-540.255360351\t-774.587386295\n",
      ":\t0005.2000-06-06.lokay\t-243.525432667\t-445.501917999\n",
      ":\t0010.1999-12-14.kaminski\t-175.646788388\t-225.546521267\n",
      ":\t0007.2000-01-17.beck\t-2936.69863298\t-3270.33893261\n",
      ":\t0003.1999-12-14.farmer\t-83.3830740633\t-98.0194301477\n",
      ":\t0003.2004-08-01.BG\t-930.317356246\t-808.402609339\n",
      ":\t0017.2004-08-01.BG\t-631.136439513\t-947.935906514\n",
      ":\t0013.2001-06-30.SA_and_HP\t-22630.8913075\t-29509.4704124\n",
      ":\t0003.1999-12-10.kaminski\t-397.679707989\t-473.957760622\n",
      ":\t0012.1999-12-14.farmer\t-3203.3482952\t-3790.60780673\n",
      ":\t0004.1999-12-10.kaminski\t-934.151203584\t-1225.36282583\n",
      ":\t0018.2001-07-13.SA_and_HP\t-2440.15758757\t-3448.4221328\n",
      ":\t0002.2001-02-07.kitchen\t-391.685616309\t-468.772884493\n",
      ":\t0007.2004-08-01.BG\t-1026.61439825\t-1525.64828123\n",
      ":\t0012.1999-12-14.kaminski\t-4204.90751999\t-1312.62846813\n",
      ":\t0005.2001-06-23.SA_and_HP\t-108.16027103\t-196.021587665\n",
      ":\t0007.1999-12-13.kaminski\t-1487.17387931\t-1734.15433259\n",
      ":\t0017.2000-01-17.beck\t-2933.97196672\t-3265.1912633\n",
      ":\t0006.2001-06-25.SA_and_HP\t-227.27627292\t-397.424465053\n",
      ":\t0006.2001-04-03.williams\t-240.032464766\t-320.371534129\n",
      ":\t0005.2001-02-08.kitchen\t-659.058178283\t-1002.96811187\n",
      ":\t0002.2003-12-18.GP\t-625.021173192\t-1336.88525876\n",
      ":\t0003.2003-12-18.GP\t-560.157577999\t-871.673008751\n",
      ":\t0013.2001-04-03.williams\t-583.96638459\t-699.297563737\n",
      ":\t0004.2001-04-02.williams\t-1588.84867632\t-728.259479408\n",
      ":\t0010.2001-02-09.kitchen\t-3263.72054799\t-3505.6364176\n",
      ":\t0001.1999-12-10.kaminski\t-48.6186163097\t-31.9245573659\n",
      ":\t0013.1999-12-14.farmer\t-1911.60771942\t-1990.81406235\n",
      ":\t0015.1999-12-14.kaminski\t-551.363791058\t-764.742250544\n",
      ":\t0012.2003-12-19.GP\t-78.0401795346\t-163.518350606\n",
      ":\t0016.2001-02-12.kitchen\t-1025.41543404\t-1350.2720968\n",
      ":\t0002.2004-08-01.BG\t-1283.67320206\t-872.934355125\n",
      ":\t0002.2001-05-25.SA_and_HP\t-393.686923046\t-625.037795943\n",
      ":\t0011.2003-12-18.GP\t-261.591112774\t-541.998047052\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./email_mapper.py | sort -k1,1 | ./classifier_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing With Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HDFS Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/rcordell/enronemail_1h.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 1 MapReduce Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper 'term_mapper.py \"assistance\"' -reducer probability_reducer.py -input enronemail_1h.txt -output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy model file to local file system from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f term_probabilities.txt\n",
    "!hdfs dfs -get /user/rcordell/model/part-00000 term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 2 MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [term_probabilities.txt] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob2847928753436973760.jar tmpDir=null\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper email_mapper.py -reducer classifier_reducer.py -input enronemail_1h.txt -output classifier -file term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of the classifier - Error Rate, Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 42/100\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"error rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Probability Counts - Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Probabilities: Spam 0 \tHam 0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"Zero Probabilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability Distribution - Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm classification_results.txt\n",
    "!hdfs dfs -get /user/rcordell/classifier/part-00000 classification_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiR1lklKktRZJilJUmeZpCRJnfX/AQq3u9Qz1s1xAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112c77f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ham = []\n",
    "spam = []\n",
    "with open('classification_results.txt','r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            flag, _id, lh, ls = line.strip().split('\\t')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            ham.append(float(lh))\n",
    "        except:\n",
    "            ham.append(0.0)\n",
    "        try:\n",
    "            spam.append(float(ls))\n",
    "        except:\n",
    "            spam.append(0.0)\n",
    "\n",
    "index = np.arange(len(ham))\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.35\n",
    "opacity = 0.4\n",
    "rects_h = plt.bar(index, ham, bar_width, color = 'b', alpha=opacity)\n",
    "rects_s = plt.bar(index, spam, bar_width, color = 'r', alpha=opacity)\n",
    "plt.xlabel('Email')\n",
    "plt.ylabel('Log Probability')\n",
    "plt.title('Posterior Log Probabilities')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/classifier\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier With All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper term_mapper.py -reducer probability_reducer.py -input enronemail_1h.txt -output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy model file to local file system from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f term_probabilities.txt\n",
    "!hdfs dfs -get /user/rcordell/model/part-00000 term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 2 MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [term_probabilities.txt] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob8685106766213357030.jar tmpDir=null\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper email_mapper.py -reducer classifier_reducer.py -input enronemail_1h.txt -output classifier -file term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of the classifier - Error Rate, All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 52/100\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"error rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Probability Counts, All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Probabilities: Spam 0 \tHam 5438\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"Zero Probabilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability Distribution - All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm classification_results.txt\n",
    "!hdfs dfs -get /user/rcordell/classifier/part-00000 classification_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d693d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ham = []\n",
    "spam = []\n",
    "with open('classification_results.txt','r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            flag, _id, lh, ls = line.strip().split('\\t')\n",
    "            try:\n",
    "                ham.append(float(lh))\n",
    "            except:\n",
    "                ham.append(0.0)\n",
    "            try:\n",
    "                spam.append(float(ls))\n",
    "            except:\n",
    "                spam.append(0.0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "index = np.arange(len(ham))\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.35\n",
    "opacity = 0.4\n",
    "rects_h = plt.bar(index, ham, bar_width, color = 'b', alpha=opacity)\n",
    "rects_s = plt.bar(index, spam, bar_width, color = 'r', alpha=opacity)\n",
    "plt.xlabel('Email')\n",
    "plt.ylabel('Log Probability')\n",
    "plt.title('Posterior Log Probabilities')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/classifier\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 \n",
    "\n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Stage 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting term_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile term_mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.4\n",
    "## Given input on STDIN read lines and count occurrences of words\n",
    "## Output a key, value => (token, email id, class, term_flag) = count\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## Words in the word list are space delimited\n",
    "wordlist = sys.argv[1].lower().split(' ')\n",
    "\n",
    "all_words = False\n",
    "if wordlist[0] == '*':\n",
    "    all_words = True\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        email_id, label, subject, body = line.split('\\t')\n",
    "    except ValueError:\n",
    "        email_id, label, body = line.split('\\t')\n",
    "    \n",
    "    # extract only words from the combined subject and body text\n",
    "    for token in WORD_RE.findall(subject + ' ' + body):\n",
    "        term = '0'\n",
    "        if all_words:\n",
    "            term = '1'\n",
    "        elif token.lower() in wordlist:\n",
    "            term = '1'\n",
    "        print('{0}\\t{1}\\t{2}\\t{3}\\t{4}'.format(token, email_id, label, term, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x term_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer Stage 1, +1 Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting probability_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile probability_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "counts = {}\n",
    "term_counts = {}\n",
    "spam_doc_ids = []\n",
    "ham_doc_ids = []\n",
    "spam_doc_word_count = 0.0\n",
    "ham_doc_word_count = 0.0\n",
    "spam_term_count = 0.0\n",
    "ham_term_count = 0.0\n",
    "terms = 0.0\n",
    "current_key = None\n",
    "current_count = 0\n",
    "\n",
    "# STDIN consists of single lines of: token <tab> id <tab> class <tab> term_flag <tab> count\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "def accumulate_key(key , _count, spam_doc_word_count, ham_doc_word_count, term_counts, counts):\n",
    "    # accumulate the key, values in a dictionary\n",
    "    _token = key[0]\n",
    "    _id = key[1]\n",
    "    _label = key[2]\n",
    "    _term = key[3]\n",
    "    \n",
    "    if key in counts:\n",
    "        counts[key] += _count\n",
    "    else:\n",
    "        counts[key] = _count\n",
    "\n",
    "    # accumulate into ham and spam dictionaries also\n",
    "    if _label == '1':\n",
    "        spam_doc_word_count += _count\n",
    "        if _id not in spam_doc_ids:\n",
    "            spam_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['spam_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0} \n",
    "    else:                \n",
    "        ham_doc_word_count += _count\n",
    "        if _id not in ham_doc_ids:\n",
    "            ham_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['ham_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0}\n",
    "    return spam_doc_word_count, ham_doc_word_count\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    token, email_id, label, term, token_count = line.split('\\t')\n",
    "\n",
    "    if term == '0':\n",
    "        vocab_word = False\n",
    "    else:\n",
    "        vocab_word = True\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key before it is passed to the reducer\n",
    "    if current_key == (token, email_id, label, term):\n",
    "        current_count += int(token_count)\n",
    "    else:\n",
    "        if current_key:\n",
    "            spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                                     float(current_count), \n",
    "                                                                     spam_doc_word_count, \n",
    "                                                                     ham_doc_word_count, \n",
    "                                                                     term_counts, counts)\n",
    "            \n",
    "        current_count = int(token_count)\n",
    "        current_key = (token, email_id, label, term)\n",
    "\n",
    "# add the last key, value that we've received\n",
    "if current_key == (token, email_id, label, term):\n",
    "    spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                             float(current_count), \n",
    "                                                             spam_doc_word_count, \n",
    "                                                             ham_doc_word_count, \n",
    "                                                             term_counts, counts)      \n",
    "                       \n",
    "# now we should have consolidated the intermediate counts and we can compute the rest\n",
    "\n",
    "# count the number of terms\n",
    "term_count = len(term_counts.keys()) * 1.0\n",
    "# compute the prior\n",
    "prior = (len(spam_doc_ids)*1.0)/(1.0*(len(spam_doc_ids) + len(ham_doc_ids)))\n",
    "\n",
    "# calculate the P(term|class) for each term\n",
    "# Add +1 Smoothing\n",
    "for term in term_counts:\n",
    "    term_counts[term]['prob_ham'] = (term_counts[term]['ham_count']+1)/(ham_doc_word_count + term_count)\n",
    "    term_counts[term]['prob_spam'] = (term_counts[term]['spam_count']+1)/(spam_doc_word_count + term_count)\n",
    "    \n",
    "    # output term <tab> probability_ham <tab> ham_count <tab> probability_spam <tab> spam_count <tab> prior\n",
    "    print '{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(term, term_counts[term]['prob_ham'], \n",
    "                                           term_counts[term]['ham_count'],\n",
    "                                           term_counts[term]['prob_spam'], \n",
    "                                           term_counts[term]['spam_count'], \n",
    "                                           prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x probability_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stage 1 MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t0.000210896309315\t2.0\t0.000481334902129\t8.0\t0.44\r\n"
     ]
    }
   ],
   "source": [
    "!cat enronemail_1h.txt | ./term_mapper.py \"assistance\" | sort -r -k1,1 | ./probability_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Stage 2, +1 Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile email_mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW2.3\n",
    "## Given input on STDIN read lines and count occurrences of words\n",
    "## Output a key, value => (token, email id, class, term_flag) = count\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "prior = 0.0\n",
    "terms = {}\n",
    "# open the file with the term probabilities\n",
    "with open('term_probabilities.txt','r') as termfile:\n",
    "    for line in termfile.readlines():\n",
    "        term, ham_prob, ham_count, spam_prob, spam_count, prior = line.strip().split('\\t')\n",
    "        terms[term] = {'ham_prob'  : float(ham_prob),  'ham_count'  : float(ham_count),\n",
    "                       'spam_prob' : float(spam_prob), 'spam_count' : float(spam_count)}\n",
    "        prior = float(prior)\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tokens = {}\n",
    "    try:\n",
    "        email_id, label, subject, body = line.split('\\t')\n",
    "    except ValueError:\n",
    "        email_id, label, body = line.split('\\t')\n",
    "    \n",
    "    # extract only words from the combined subject and body text\n",
    "    for token in WORD_RE.findall(subject + ' ' + body):\n",
    "        if token.lower() in terms:\n",
    "            if token.lower() in tokens:\n",
    "                tokens[token.lower()]['count'] += 1\n",
    "            else:\n",
    "                tokens[token.lower()] = {'count' : 1}\n",
    "                \n",
    "    # emit the accumulated terms for the email\n",
    "    if len(tokens) > 0:\n",
    "        for token in tokens:\n",
    "            print('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}'.format(email_id, label, token,\n",
    "                                                terms[token]['ham_prob'],\n",
    "                                                terms[token]['spam_prob'],\n",
    "                                                prior,\n",
    "                                                tokens[token]['count']))\n",
    "    else:\n",
    "        # if an email has no vocabulary terms then emit a single record indicating a zero count\n",
    "        print('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}'.format(email_id, label, '*',\n",
    "                                            0.0,\n",
    "                                            0.0,\n",
    "                                            prior,\n",
    "                                            0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x email_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer Stage 2, +1 Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classifier_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile classifier_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "emails = {}\n",
    "current_email = None\n",
    "prob_spam = 0.0\n",
    "prob_ham  = 0.0\n",
    "zero_prob_ham = 0\n",
    "zero_prob_spam = 0\n",
    "pred_label = 0\n",
    "right = 0\n",
    "wrong = 0\n",
    "\n",
    "# STDIN consists of single lines of: id <tab> label <tab> token <tab> p_ham <tab> p_spam <tab> prior <tab> count\n",
    "# Assume the mapper has already aggregated terms in each email\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    email_id, label, token, _p_ham, _p_spam, _prior, _token_count = line.split('\\t')\n",
    "    \n",
    "    # convert strings to numeric\n",
    "    p_ham = float(_p_ham)\n",
    "    p_spam = float(_p_spam)\n",
    "    prior = float(_prior)\n",
    "    token_count = float(_token_count)\n",
    "        \n",
    "    # accumulate by email id\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key before it is passed to the reducer\n",
    "    if current_email == email_id:\n",
    "        if p_ham != 0.0:\n",
    "            prob_ham += log(p_ham) * token_count\n",
    "        else:\n",
    "            zero_prob_ham += 1\n",
    "        if p_spam != 0.0:\n",
    "            prob_spam += log(p_spam) * token_count\n",
    "        else:\n",
    "            zero_prob_spam += 1\n",
    "    else:\n",
    "        if current_email:\n",
    "            # emit the classification\n",
    "            if prob_spam > prob_ham:\n",
    "                pred_label = '1'\n",
    "            else:\n",
    "                pred_label = '0'\n",
    "            if label == pred_label:\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "            print '{0}\\t{1}\\t{2}'.format(email_id, label, pred_label)\n",
    "            \n",
    "            # add to the list of posteriors\n",
    "            emails[current_email] = {'lp_ham' : prob_ham, 'lp_spam' : prob_spam}\n",
    "        \n",
    "         # if this new email has no terms then we predict ham but have no posteriors\n",
    "        if token == '*':\n",
    "            print '{0}\\t{1}\\t{2}'.format(email_id, label, '0')\n",
    "            if label == '0':\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "            emails[email_id] = {'lp_ham' : '-', 'lp_spam' : '-'}\n",
    "            current_email = None\n",
    "        else:\n",
    "            # otherwise make this the new current email and reset the probabilities\n",
    "            current_email = email_id\n",
    "            if p_ham != 0.0:\n",
    "                prob_ham = log(1-prior) + (log(p_ham) * token_count)\n",
    "            else:\n",
    "                zero_prob_ham += 1\n",
    "            if p_spam != 0.0:\n",
    "                prob_spam = log(prior) + (log(p_spam) * token_count)\n",
    "            else:\n",
    "                zero_prob_spam += 1\n",
    "\n",
    "\n",
    "# add the last key, value that we've received\n",
    "if current_email == email_id:\n",
    "    # emit the classification\n",
    "    if prob_spam > prob_ham:\n",
    "        pred_label = '1'\n",
    "    else:\n",
    "        pred_label = '0'\n",
    "    if label == pred_label:\n",
    "        right += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "    print '{0}\\t{1}\\t{2}'.format(email_id, label, pred_label)    \n",
    "    emails[email_id] = {'lp_ham' : prob_ham, 'lp_spam' : prob_spam} \n",
    "    \n",
    "print 'Error Rate: {0}/{1}'.format(wrong, right + wrong)\n",
    "print 'Zero Probabilities: Spam {0} \\tHam {1}'.format(zero_prob_spam, zero_prob_ham)\n",
    "for e in emails:\n",
    "    print '{0}\\t{1}\\t{2}\\t{3}'.format(':',e, emails[e]['lp_ham'], emails[e]['lp_spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x classifier_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat enronemail_1h.txt | ./email_mapper.py | sort -k1,1 | ./classifier_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing With Hadoop\n",
    "\n",
    "It is assumed that the hadoop processes are still running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 1 MapReduce - +1 Smoothing, Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper 'term_mapper.py \"assistance\"' -reducer probability_reducer.py -input enronemail_1h.txt -output model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy model file to local file system from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f term_probabilities.txt\n",
    "!hdfs dfs -get /user/rcordell/model/part-00000 term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 2 MapReduce - +1 Smoothing, Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [term_probabilities.txt] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob4108528525587479737.jar tmpDir=null\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper email_mapper.py -reducer classifier_reducer.py -input enronemail_1h.txt -output classifier -file term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of the classifier - Error Rate - +1 Smoothing, Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 42/100\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"error rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Probability Counts - +1 Smoothing, Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Probabilities: Spam 0 \tHam 0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"Zero Probabilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability Distribution, +1 Smoothing, Single Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm classification_results.txt\n",
    "!hdfs dfs -get /user/rcordell/classifier/part-00000 classification_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUpKkzjJJSZI6yyQlSeosk5QkqbP+P7pkexduxJ1MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112320f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ham = []\n",
    "spam = []\n",
    "with open('classification_results.txt','r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            flag, _id, lh, ls = line.strip().split('\\t')\n",
    "            try:\n",
    "                ham.append(float(lh))\n",
    "            except:\n",
    "                ham.append(0.0)\n",
    "            try:\n",
    "                spam.append(float(ls))\n",
    "            except:\n",
    "                spam.append(0.0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "index = np.arange(len(ham))\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.35\n",
    "opacity = 0.4\n",
    "rects_h = plt.bar(index, ham, bar_width, color = 'b', alpha=opacity)\n",
    "rects_s = plt.bar(index, spam, bar_width, color = 'r', alpha=opacity)\n",
    "plt.xlabel('Email')\n",
    "plt.ylabel('Log Probability')\n",
    "plt.title('Posterior Log Probabilities')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/classifier\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 1 MapReduce - +1 Smoothing, All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper 'term_mapper.py \"*\"' -reducer probability_reducer.py -input enronemail_1h.txt -output model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy model file to local file system from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f term_probabilities.txt\n",
    "!hdfs dfs -get /user/rcordell/model/part-00000 term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 2 MapReduce - +1 Smoothing, All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [term_probabilities.txt] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob3501786193234961595.jar tmpDir=null\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper email_mapper.py -reducer classifier_reducer.py -input enronemail_1h.txt -output classifier -file term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of the classifier - Error Rate - +1 Smoothing, All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 33/100\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"error rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Probability Counts - +1 Smoothing, All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Probabilities: Spam 0 \tHam 0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"Zero Probabilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability Distribution +1 Smoothing, All Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm classification_results.txt\n",
    "!hdfs dfs -get /user/rcordell/classifier/part-00000 classification_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZlZYf1/y5kKVzuMcpIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1121c74d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ham = []\n",
    "spam = []\n",
    "with open('classification_results.txt','r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            flag, _id, lh, ls = line.strip().split('\\t')\n",
    "            try:\n",
    "                ham.append(float(lh))\n",
    "            except:\n",
    "                ham.append(0.0)\n",
    "            try:\n",
    "                spam.append(float(ls))\n",
    "            except:\n",
    "                spam.append(0.0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "index = np.arange(len(ham))\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.35\n",
    "opacity = 0.4\n",
    "rects_h = plt.bar(index, ham, bar_width, color = 'b', alpha=opacity)\n",
    "rects_s = plt.bar(index, spam, bar_width, color = 'r', alpha=opacity)\n",
    "plt.xlabel('Email')\n",
    "plt.ylabel('Log Probability')\n",
    "plt.title('Posterior Log Probabilities')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/classifier\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop YARN and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.5. \n",
    "\n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Stage 1\n",
    "\n",
    "No changes are required so we use the Stage 1 mapper from HW2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer Stage 1, +1 Smoothing, Term Counts >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting probability_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile probability_reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW2.5\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "## Drop term counts < 3\n",
    "\n",
    "import sys\n",
    "counts = {}\n",
    "term_counts = {}\n",
    "spam_doc_ids = []\n",
    "ham_doc_ids = []\n",
    "spam_doc_word_count = 0.0\n",
    "ham_doc_word_count = 0.0\n",
    "spam_term_count = 0.0\n",
    "ham_term_count = 0.0\n",
    "terms = 0.0\n",
    "current_key = None\n",
    "current_count = 0\n",
    "\n",
    "# STDIN consists of single lines of: token <tab> id <tab> class <tab> term_flag <tab> count\n",
    "# Assume that the proper parameters have been set such that Hadoop knows to treat the first 4 fields\n",
    "# as the key so that they are properly sorted when they reach the reducer\n",
    "# See http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/ for an example\n",
    "\n",
    "def accumulate_key(key , _count, spam_doc_word_count, ham_doc_word_count, term_counts, counts):\n",
    "    # accumulate the key, values in a dictionary\n",
    "    _token = key[0]\n",
    "    _id = key[1]\n",
    "    _label = key[2]\n",
    "    _term = key[3]\n",
    "    \n",
    "    if key in counts:\n",
    "        counts[key] += _count\n",
    "    else:\n",
    "        counts[key] = _count\n",
    "\n",
    "    # accumulate into ham and spam dictionaries also\n",
    "    if _label == '1':\n",
    "        spam_doc_word_count += _count\n",
    "        if _id not in spam_doc_ids:\n",
    "            spam_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['spam_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0} \n",
    "    else:                \n",
    "        ham_doc_word_count += _count\n",
    "        if _id not in ham_doc_ids:\n",
    "            ham_doc_ids.append(_id)\n",
    "        if _term == '1':\n",
    "            if _token in term_counts:\n",
    "                term_counts[_token]['ham_count'] += _count\n",
    "            else:\n",
    "                term_counts[_token] = {'spam_count' : _count,\n",
    "                                     'ham_count' : 0.0,\n",
    "                                      'prob_ham'  : 0.0,\n",
    "                                      'prob_spam' : 0.0}\n",
    "    return spam_doc_word_count, ham_doc_word_count\n",
    "\n",
    "# process the STDIN stream\n",
    "for line in sys.stdin:\n",
    "    token, email_id, label, term, token_count = line.split('\\t')\n",
    "\n",
    "    if term == '0':\n",
    "        vocab_word = False\n",
    "    else:\n",
    "        vocab_word = True\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key before it is passed to the reducer\n",
    "    if current_key == (token, email_id, label, term):\n",
    "        current_count += int(token_count)\n",
    "    else:\n",
    "        if current_key:\n",
    "            spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                                     float(current_count), \n",
    "                                                                     spam_doc_word_count, \n",
    "                                                                     ham_doc_word_count, \n",
    "                                                                     term_counts, counts)\n",
    "            \n",
    "        current_count = int(token_count)\n",
    "        current_key = (token, email_id, label, term)\n",
    "\n",
    "# add the last key, value that we've received\n",
    "if current_key == (token, email_id, label, term):\n",
    "    spam_doc_word_count, ham_doc_word_count = accumulate_key(current_key, \n",
    "                                                             float(current_count), \n",
    "                                                             spam_doc_word_count, \n",
    "                                                             ham_doc_word_count, \n",
    "                                                             term_counts, counts)      \n",
    "                       \n",
    "# now we should have consolidated the intermediate counts and we can compute the rest\n",
    "\n",
    "# count the number of terms\n",
    "term_count = len(term_counts.keys()) * 1.0\n",
    "# compute the prior\n",
    "prior = (len(spam_doc_ids)*1.0)/(1.0*(len(spam_doc_ids) + len(ham_doc_ids)))\n",
    "\n",
    "# calculate the P(term|class) for each term\n",
    "# Add +1 Smoothing\n",
    "for term in term_counts:\n",
    "    # drop term counts less than 3\n",
    "    # so I'm going to make a call and say that both the spam and ham counts for the term must be less than 3\n",
    "    # in order to drop the term\n",
    "    if term_counts[term]['ham_count'] >= 3 or term_counts[term]['spam_count'] >= 3:\n",
    "        term_counts[term]['prob_ham'] = (term_counts[term]['ham_count']+1)/(ham_doc_word_count + term_count)\n",
    "        term_counts[term]['prob_spam'] = (term_counts[term]['spam_count']+1)/(spam_doc_word_count + term_count)    \n",
    "        # output term <tab> probability_ham <tab> ham_count <tab> probability_spam <tab> spam_count <tab> prior\n",
    "        print '{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(term, term_counts[term]['prob_ham'], \n",
    "                                               term_counts[term]['ham_count'],\n",
    "                                               term_counts[term]['prob_spam'], \n",
    "                                               term_counts[term]['spam_count'], \n",
    "                                               prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x probability_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Stage 1 MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat enronemail_1h.txt | ./term_mapper.py \"*\" | sort -r -k1,1 | ./probability_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Stage 2, +1 Smoothing\n",
    "\n",
    "No changes are required in the Stage 2 mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer Stage 2, +1 Smoothing\n",
    "\n",
    "No changes are required in the Stage 2 reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing With Hadoop\n",
    "\n",
    "It is assumed that the hadoop processes are still running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 1 MapReduce - +1 Smoothing, All Terms Occurring >= 3 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper 'term_mapper.py \"*\"' -reducer probability_reducer.py -input enronemail_1h.txt -output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy model file to local file system from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f term_probabilities.txt\n",
    "!hdfs dfs -get /user/rcordell/model/part-00000 term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Hadoop Stage 2 MapReduce - +1 Smoothing, All Terms Occurring >= 3 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [term_probabilities.txt] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob8811802586518225810.jar tmpDir=null\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar -mapper email_mapper.py -reducer classifier_reducer.py -input enronemail_1h.txt -output classifier -file term_probabilities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the output of the classifier - Error Rate - +1 Smoothing, All Terms Occuring >= 3 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 32/100\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"error rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Probability Counts - +1 Smoothing, All Terms Occurring >= 3 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Probabilities: Spam 0 \tHam 0\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/classifier/part-00000 | grep -i \"Zero Probabilities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability Distribution +1 Smoothing, All Terms Occuring >= 3 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm classification_results.txt\n",
    "!hdfs dfs -get /user/rcordell/classifier/part-00000 classification_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1117128d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ham = []\n",
    "spam = []\n",
    "with open('classification_results.txt','r') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            flag, _id, lh, ls = line.strip().split('\\t')\n",
    "            try:\n",
    "                ham.append(float(lh))\n",
    "            except:\n",
    "                ham.append(0.0)\n",
    "            try:\n",
    "                spam.append(float(ls))\n",
    "            except:\n",
    "                spam.append(0.0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "index = np.arange(len(ham))\n",
    "fig, ax = plt.subplots()\n",
    "bar_width = 0.35\n",
    "opacity = 0.4\n",
    "rects_h = plt.bar(index, ham, bar_width, color = 'b', alpha=opacity)\n",
    "rects_s = plt.bar(index, spam, bar_width, color = 'r', alpha=opacity)\n",
    "plt.xlabel('Email')\n",
    "plt.ylabel('Log Probability')\n",
    "plt.title('Posterior Log Probabilities')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/model\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/rcordell/classifier\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.6 \n",
    "\n",
    "Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# replicate our mapper code here where we take the subject and body together\n",
    "# except now we grab the label field as well to use for the Y values\n",
    "with open('enronemail_1h.txt', 'rU') as infile:\n",
    "    for line in infile.readlines():\n",
    "        try:\n",
    "            email_id, label, subject, body = line.split('\\t')\n",
    "            X_train.append(subject + ' ' + body)\n",
    "        except ValueError:\n",
    "            email_id, label, body = line.split('\\t')\n",
    "            X_train.append(body)\n",
    "        # extract only words from the combined subject and body text\n",
    "        Y_train.append(int(label))\n",
    "\n",
    "# Use the TfidVectorizer to create the feature vectors\n",
    "# We should override the tokenizer regular expression to make it the same as what we used\n",
    "# in our poor man's mapper code\n",
    "vectorizer = TfidfVectorizer(token_pattern = \"[\\w']+\")\n",
    "vf = vectorizer.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Bayes Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(vf.fit_transform(X_train), Y_train)\n",
    "print 1.0 - clf.score(vf.fit_transform(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.6.1 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "\n",
    "—  Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "-  Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "\n",
    "Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "clf.fit(vf.fit_transform(X_train), Y_train)\n",
    "print 1.0 - clf.score(vf.fit_transform(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.7 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "\n",
    "The Enron SPAM data in the following folder enron1-Training-Data-RAW is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "\n",
    "--- Line 1 contains the subject\n",
    "--- The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.8 OPTIONAL\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    " -- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).\n",
    " -- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). How do we treat tokens in the test set that do not appear in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.8.1 OPTIONAL\n",
    "—  Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW2.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "-  Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
