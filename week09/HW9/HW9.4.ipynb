{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcordell/Documents/MIDS/W261/W261env/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Jupyter requires this for MRJob to reload classes properly\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 9.4: Topic-specific PageRank implementation using MRJob\n",
    "\n",
    "Modify your PageRank implementation to produce a topic specific PageRank implementation,\n",
    "as described in:\n",
    "\n",
    "http://www-cs-students.stanford.edu/~taherh/papers/topic-sensitive-pagerank.pdf\n",
    "\n",
    "Note in this article that there is a special caveat to ensure that the transition matrix is irreducible.\n",
    "This caveat lies in footnote 3 on page 3:\n",
    "\n",
    "\tA minor caveat: to ensure that M is irreducible when p\n",
    "\tcontains any 0 entries, nodes not reachable from nonzero\n",
    "\tnodes in p should be removed. In practice this is not problematic.\n",
    "\n",
    "and must be adhered to for convergence to be guaranteed.\n",
    "\n",
    "Run topic specific PageRank on the following randomly generated network of 100 nodes:\n",
    "\n",
    "s3://ucb-mids-mls-networks/randNet.txt (also available on Dropbox)\n",
    "\n",
    "which are organized into ten topics, as described in the file:\n",
    "\n",
    "s3://ucb-mids-mls-networks/randNet_topics.txt  (also available on Dropbox)\n",
    "\n",
    "Since there are 10 topics, your result should be 11 PageRank vectors\n",
    "(one for the vanilla PageRank implementation in 9.1, and one for each topic\n",
    "with the topic specific implementation). Print out the top ten ranking nodes \n",
    "and their topics for each of the 11 versions, and comment on your result. \n",
    "Assume a teleportation factor of 0.15 in all your analyses.\n",
    "\n",
    "One final and important comment here:  please consider the \n",
    "requirements for irreducibility with topic-specific PageRank.\n",
    "In particular, the literature ensures irreducibility by requiring that\n",
    "nodes not reachable from in-topic nodes be removed from the network.\n",
    "\n",
    "This is not a small task, especially as it it must be performed\n",
    "separately for each of the (10) topics.\n",
    "\n",
    "So, instead of using this method for irreducibility, \n",
    "please comment on why the literature's method is difficult to implement,\n",
    "and what what extra computation it will require.\n",
    "Then for your code, please use the alternative, \n",
    "non-uniform damping vector:\n",
    "\n",
    "$v_{ji} = \\beta(\\frac{1}{|T_{j}|})$; if node i lies in topic $T_j$\n",
    "\n",
    "$v_{ji} = (1-\\beta)\\frac{1}{N - |T_{j}|}$; if node i lies outside of topic $T_j$\n",
    "\n",
    "for beta in (0,1) close to 1. \n",
    "\n",
    "With this approach, you will not have to delete any nodes.\n",
    "If beta > 0.5, PageRank is topic-sensitive, \n",
    "and if beta < 0.5, the PageRank is anti-topic-sensitive. \n",
    "For any value of beta irreducibility should hold,\n",
    "so please try beta=0.99, and perhaps some other values locally,\n",
    "on the smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "This first step transforms the original adjacency list to create a list of the form:\n",
    "\n",
    "    node \\t [page_rank, [topic_ranks], [outlinks]]\n",
    "    \n",
    "The `[topic_ranks]` is the bias vector and is initialized as each element as the inverse of the number of nodes classified as that topic, whereas `page_rank` is initialized as 1/Number of nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrJobTransform94.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrJobTransform94.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "        \n",
    "class MrJobTransform94(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MrJobTransform94, self).configure_options()    \n",
    "        self.add_passthrough_option(\n",
    "            '--nodes', dest='nodes', type='int', default=0, \n",
    "            help=\"Number of nodes\")\n",
    "        self.add_passthrough_option(\n",
    "            '--maptasks', dest='mappers', type='int', default=2, \n",
    "            help=\"mapper task instances\")\n",
    "        self.add_passthrough_option(\n",
    "            '--reducetasks', dest='reducers', type='int', default=1, \n",
    "            help=\"reducer task instances\")\n",
    "      \n",
    "\n",
    "    # each line is from the adjacency list node_id \\t {neighbor: weight}\n",
    "    # where {neighbor:weight} is one of m elements in the dictionary\n",
    "    # emit node \\t [page_rank, num_outlinks, [outlinks]]\n",
    "    def mapper(self, _, line):\n",
    "        node, adj_list = re.split('\\t',line.strip())\n",
    "        node = node.strip('\"')\n",
    "        neighbors = eval(adj_list)\n",
    "        yield node, [neighbors.keys()]\n",
    "        for neighbor in neighbors:\n",
    "            yield neighbor, []\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.rank = 1.0/self.options.nodes\n",
    "        self.topic_count = [0.0]*10  # 10 is the number of topics\n",
    "        with open('/Users/rcordell/Documents/MIDS/W261/week09/HW9/Data/randNet_topics.txt','r') as topic_file:\n",
    "            for line in topic_file.readlines():\n",
    "                node, topic = re.split('\\t',line.strip())\n",
    "                self.topic_count[int(topic)-1]+=1\n",
    "        # initialized topic vector is 1/|T_j|\n",
    "        self.topic_rank = [1.0/x for x in self.topic_count]\n",
    "                \n",
    "        \n",
    "    # node, [page_rank, [outlinks]]\n",
    "    def reducer(self, node, outlinks_list):\n",
    "        combined_outlinks = []\n",
    "        for outlinks in outlinks_list:\n",
    "            for outlink in outlinks:\n",
    "                combined_outlinks = combined_outlinks + outlink\n",
    "        self.increment_counter('transformer', 'nodes', 1) \n",
    "        yield node, [self.rank, self.topic_rank ,combined_outlinks]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       reducer_init=self.reducer_init,\n",
    "                       reducer=self.reducer,\n",
    "                       jobconf = {\n",
    "                        'mapreduce.job.maps' : self.options.mappers,\n",
    "                        'mapreduce.job.reduces' : self.options.reducers}\n",
    "                    )]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrJobTransform94.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Count: 100\n"
     ]
    }
   ],
   "source": [
    "from MrJobTransform94 import MrJobTransform94\n",
    "              \n",
    "def transform(qfile):\n",
    "    mr_job = MrJobTransform94(args=[qfile,\n",
    "                                    '-r','local',\n",
    "                                    '--nodes', '100',\n",
    "                                    '--maptasks', '4',\n",
    "                                    '--reducetasks', '1'])\n",
    "    with open('Data/randNet-in.txt','w') as outfile:\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "                q = mr_job.parse_output_line(line)\n",
    "                outfile.write(line)\n",
    "            counters = runner.counters()\n",
    "            print 'Node Count: {0}'.format(counters[0]['transformer']['nodes'])\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    transform('Data/randNet.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second step is the 2-stage map reduce that calculates the page rank and the topic rank vectors over several iterations.\n",
    "\n",
    "Page rank is calculated as $\\alpha \\frac{1}{|N|} + (1-\\alpha)(\\frac{m}{N} + p)$ where $p$ is the page rank as accumulated from the contributions of incoming links to that node.\n",
    "\n",
    "Topic rank is calculated as $\\sum_{i}v_{ij}$ where $v_{ij} = \\beta(\\frac{1}{|T_j|}) p$ for outlink $i$ that is of topic $j$ and $= (1-\\beta)(\\frac{1}{N-|T_j|})p$ for the outlink $i$ that is not of topic $j$\n",
    "\n",
    "I believe that I am calculating the topic rank incorrectly....\n",
    "\n",
    "This MR job attempts to calculate all the topic vectors at the same time in a single pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrJob94.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrJob94.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "import re\n",
    "       \n",
    "class MrJob94(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MrJob94, self).configure_options()    \n",
    "        self.add_passthrough_option(\n",
    "            '--nodes', dest='nodes', type='int', default=0, \n",
    "            help=\"Number of nodes\")    \n",
    "        self.add_passthrough_option(\n",
    "            '--damping', dest='damping', type='float', default=0.85, \n",
    "            help=\"damping factor\") \n",
    "        self.add_passthrough_option(\n",
    "            '--beta', dest='beta', type='float', default=0.99, \n",
    "            help=\"damping factor\") \n",
    "        self.add_passthrough_option(\n",
    "            '--maptasks', dest='mappers', type='int', default=2, \n",
    "            help=\"mapper task instances\")\n",
    "        self.add_passthrough_option(\n",
    "            '--reducetasks', dest='reducers', type='int', default=1, \n",
    "            help=\"reducer task instances\")\n",
    "\n",
    "    def pr_mapper_init(self):\n",
    "        self.node_topics = [0]*100\n",
    "        self.topic_counts = [0]*10\n",
    "        with open('/Users/rcordell/Documents/MIDS/W261/week09/HW9/Data/randNet_topics.txt','r') as topic_file:\n",
    "            for line in topic_file.readlines():\n",
    "                node, topic = re.split('\\t',line.strip())\n",
    "                self.node_topics[int(node)-1] = int(topic)\n",
    "                self.topic_counts[int(topic)-1] += 1\n",
    "        self.topic_freqs = [1/x for x in self.topic_counts]\n",
    "\n",
    "\n",
    "    def pr_mapper(self, _, line):\n",
    "        node, value_list = re.split('\\t',line.strip())\n",
    "        node = node.strip('\"')\n",
    "        v = eval(value_list)\n",
    "\n",
    "        # yield the graph structure\n",
    "        yield node, [0.0, v[1], v[2]]\n",
    "        \n",
    "        # distrubute the page rank and topic ranks to all the outlink nodes\n",
    "        if len(v[2]) > 0:\n",
    "            for outlink in v[2]:\n",
    "                yield outlink, [v[0]/len(v[2]), [x/len(v[2]) for x in v[1]]]\n",
    "        else:\n",
    "            # this takes care of mass that doesn't get propogated when \n",
    "            # there are no outlink nodes to distribute it to\n",
    "            yield node, [v[0], v[1]]\n",
    "\n",
    "            \n",
    "    def pr_reducer_init(self):\n",
    "        self.pr_mass = 0.0\n",
    "        \n",
    "    # node_id \\t page_rank \n",
    "    # OR\n",
    "    # node_id \\t [page_rank, [topic_rank],[outlinked nodes]]\n",
    "    def pr_reducer(self, node, neighbors):\n",
    "        adj_list = []\n",
    "        rank = 0.0\n",
    "        topic_ranks = [0.0]*10\n",
    "        for neighbor in neighbors:\n",
    "            if len(neighbor) == 3:\n",
    "                # this is the graph\n",
    "                adj_list = neighbor\n",
    "            else:\n",
    "                rank += neighbor[0]\n",
    "                topic_ranks = [a+b for a,b in zip(topic_ranks,neighbor[1])]\n",
    "        self.pr_mass += rank\n",
    "        yield node, [rank, topic_ranks, adj_list[2]]\n",
    "   \n",
    "    def pr_reducer_final(self):\n",
    "        print '-',self.pr_mass\n",
    "        pass\n",
    "\n",
    "    # find the dangling nodes and emit their mass with the key '*'\n",
    "    def dangling_mapper(self, node, rank):\n",
    "        if len(rank[2]) == 0:            \n",
    "            yield '*', rank[0]\n",
    "            yield node, [0.0, rank[1], rank[2]]\n",
    "        else:\n",
    "            yield node, rank\n",
    "            \n",
    "    \n",
    "    def dangling_reducer_init(self):\n",
    "        self.conserved_mass = 0.0\n",
    "        self.mass = 0.0\n",
    "        self.node_topics = [0]*100\n",
    "        self.topic_counts = [0]*10\n",
    "        with open('/Users/rcordell/Documents/MIDS/W261/week09/HW9/Data/randNet_topics.txt','r') as topic_file:\n",
    "            for line in topic_file.readlines():\n",
    "                node, topic = re.split('\\t',line.strip())\n",
    "                self.node_topics[int(node)-1] = int(topic)\n",
    "                self.topic_counts[int(topic)-1] += 1\n",
    "        self.topic_freqs = [1/x for x in self.topic_counts]\n",
    "        \n",
    "            \n",
    "    # sum the dangling node mass\n",
    "    def dangling_reducer(self, key, mass):\n",
    "        if key == '*':\n",
    "            self.conserved_mass = sum(mass)\n",
    "        else:\n",
    "            # distrubute the conserved mass\n",
    "            # mass is really [rank, count, [outlinks]] in this case\n",
    "            distributed_mass = self.conserved_mass/self.options.nodes\n",
    "            teleport_prob = (1.0-self.options.damping)/self.options.nodes\n",
    "            for item in mass:\n",
    "                # compute the basic page rank\n",
    "                new_rank = teleport_prob + (self.options.damping*(item[0] + distributed_mass))\n",
    "                \n",
    "                # compute the topic rank vector\n",
    "                for i, topic_rank in enumerate(item[1]):\n",
    "                    weight = 0.0\n",
    "                    for outlink in item[2]:\n",
    "                        if i == self.node_topics[int(outlink)-1]:\n",
    "                            weight += self.options.beta / self.topic_counts[i]\n",
    "                        else:\n",
    "                            weight += (1.0 - self.options.beta)*(1.0/(self.options.nodes - self.topic_counts[i]))\n",
    "                    item[1][i] = item[1][i] + weight * new_rank      \n",
    "                self.mass += new_rank\n",
    "                yield key, [new_rank, item[1], item[2]]\n",
    "    \n",
    "    def dangling_reducer_final(self):\n",
    "        print '*',self.mass\n",
    "        pass\n",
    "             \n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.pr_mapper_init,\n",
    "                       mapper=self.pr_mapper,\n",
    "                       reducer_init=self.pr_reducer_init,\n",
    "                       reducer=self.pr_reducer,\n",
    "#                       reducer_final=self.pr_reducer_final,\n",
    "                       jobconf = {\n",
    "                        'mapreduce.job.maps' : self.options.mappers,\n",
    "                        'mapreduce.job.reduces' : self.options.reducers}), \n",
    "                MRStep(mapper=self.dangling_mapper,\n",
    "                       reducer_init=self.dangling_reducer_init,\n",
    "                       reducer=self.dangling_reducer,\n",
    "#                       reducer_final=self.dangling_reducer_final,\n",
    "                       jobconf = {\n",
    "                        'mapreduce.job.maps' : self.options.mappers,\n",
    "                        'mapreduce.job.reduces' : self.options.reducers})] \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrJob94.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MrJob94 import MrJob94\n",
    "from shutil import copyfile\n",
    "import sys\n",
    "    \n",
    "def page_rank(infile, outfile, node_count, damping_factor, beta):\n",
    "    mr_job = MrJob94(args=[infile,\n",
    "                          '--nodes', str(node_count),\n",
    "                          '--damping', str(damping_factor),\n",
    "                          '--beta', str(beta)])\n",
    "\n",
    "    with open(outfile,'w') as rankfile:\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "#                q = mr_job.parse_output_line(line)\n",
    "#                print q\n",
    "                rankfile.write(line)\n",
    "\n",
    "def driver(infile, outfile, damping_factor, beta):\n",
    "    \n",
    "    iterations = 0     \n",
    "    while iterations < 100:\n",
    "        page_rank(infile, outfile, 100, damping_factor, beta)\n",
    "        copyfile(outfile, infile)\n",
    "        iterations += 1\n",
    "#        print 'Iteration: {0}'.format(iterations)  \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    driver('Data/randNet-in.txt','Data/randNet-out.txt', 0.85, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
