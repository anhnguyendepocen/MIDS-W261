{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an experiment with the PageRank MR [implementation](http://michaelnielsen.org/blog/using-mapreduce-to-compute-pagerank/) by Michael Nielson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The article Michael Nielson wrote contains a map reduce algorithm, and I'm putting here so I can get the original code working as designed. The goal of this exercise is to understand the implmentation in detail and then adapt it. The first adaptation step is to use MrJob instead of the `map_reduce` function here, and the second step is to adapt the algorithm to work with the toy example as part of HW9.1.\n",
    "\n",
    "-----------------\n",
    "\n",
    "## Part 1: The Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing map_reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile map_reduce.py\n",
    "# map_reduce.py\n",
    "# Defines a single function, map_reduce, which takes an input\n",
    "# dictionary i and applies the user-defined function mapper to each\n",
    "# (input_key,input_value) pair, producing a list of intermediate \n",
    "# keys and intermediate values.  Repeated intermediate keys then \n",
    "# have their values grouped into a list, and the user-defined \n",
    "# function reducer is applied to the intermediate key and list of \n",
    "# intermediate values.  The results are returned as a list.\n",
    "\n",
    "import itertools\n",
    "\n",
    "def map_reduce(i,mapper,reducer):\n",
    "    intermediate = []\n",
    "    for (key,value) in i.items():\n",
    "        intermediate.extend(mapper(key,value))\n",
    "    groups = {}\n",
    "    for key, group in itertools.groupby(sorted(intermediate), lambda x: x[0]):\n",
    "        groups[key] = list([y for x, y in group])\n",
    "    return [reducer(intermediate_key,groups[intermediate_key]) for intermediate_key in groups] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Change in l1 norm: 1.159352642857\n",
      "Iteration: 2\n",
      "Change in l1 norm: 0.407339540567\n",
      "Iteration: 3\n",
      "Change in l1 norm: 0.164121655282\n",
      "Iteration: 4\n",
      "Change in l1 norm: 0.079544297575\n",
      "Iteration: 5\n",
      "Change in l1 norm: 0.035531329063\n",
      "Iteration: 6\n",
      "Change in l1 norm: 0.013737477305\n",
      "Iteration: 7\n",
      "Change in l1 norm: 0.006959669613\n",
      "Iteration: 8\n",
      "Change in l1 norm: 0.003339450145\n",
      "Iteration: 9\n",
      "Change in l1 norm: 0.001642446968\n",
      "Iteration: 10\n",
      "Change in l1 norm: 0.000736898241\n",
      "Iteration: 11\n",
      "Change in l1 norm: 0.000382751385\n",
      "Iteration: 12\n",
      "Change in l1 norm: 0.000189308194\n",
      "Iteration: 13\n",
      "Change in l1 norm: 0.000094928583\n"
     ]
    }
   ],
   "source": [
    "# pagerank_mr.py\n",
    "#\n",
    "# Computes PageRank, using a simple MapReduce library.\n",
    "#\n",
    "# MapReduce is used in two separate ways: (1) to compute\n",
    "# the inner product between the vector of dangling pages\n",
    "# (i.e., pages with no outbound links) and the current\n",
    "# estimated PageRank vector; and (2) to actually carry\n",
    "# out the update of the estimated PageRank vector.\n",
    "#\n",
    "# For a web of one million webpages the program consumes\n",
    "# about one gig of RAM, and takes an hour or so to run,\n",
    "# on a (slow) laptop with 3 gig of RAM, running Vista and\n",
    "# Python 2.5.\n",
    "\n",
    "import map_reduce\n",
    "import numpy.random\n",
    "import random\n",
    "\n",
    "def paretosample(n,power=2.0):\n",
    "    # Returns a sample from a truncated Pareto distribution\n",
    "    # with probability mass function p(l) proportional to\n",
    "    # 1/l^power.  The distribution is truncated at l = n.\n",
    "\n",
    "    m = n+1\n",
    "    while m > n: m = numpy.random.zipf(power)\n",
    "    return m\n",
    "\n",
    "def initialize(n,power):\n",
    "    # Returns a Python dictionary representing a web\n",
    "    # with n pages, and where each page k is linked to by\n",
    "    # L_k random other pages.  The L_k are independent and\n",
    "    # identically distributed random variables with a\n",
    "    # shifted and truncated Pareto probability mass function\n",
    "    # p(l) proportional to 1/(l+1)^power.\n",
    "\n",
    "    # The representation used is a Python dictionary with\n",
    "    # keys 0 through n-1 representing the different pages.\n",
    "    # i[j][0] is the estimated PageRank, initially set at 1/n,\n",
    "    # i[j][1] the number of outlinks, and i[j][2] a list of\n",
    "    # the outlinks.\n",
    "\n",
    "    # This dictionary is used to supply (key,value) pairs to\n",
    "    # both mapper tasks defined below.\n",
    "\n",
    "    # initialize the dictionary\n",
    "    i = {} \n",
    "    for j in xrange(n): i[j] = [1.0/n,0,[]]\n",
    "\n",
    "    # For each page, generate inlinks according to the Pareto\n",
    "    # distribution. Note that this is somewhat tedious, because\n",
    "    # the Pareto distribution governs inlinks, NOT outlinks,\n",
    "    # which is what our representation is adapted to represent.\n",
    "    # A smarter representation would give easy\n",
    "    # access to both, while remaining memory efficient.\n",
    "    for k in xrange(n):\n",
    "        lk = paretosample(n+1,power)-1\n",
    "        values = random.sample(xrange(n),lk)\n",
    "        for j in values:\n",
    "            i[j][1] += 1 # increment the outlink count for page j\n",
    "            i[j][2].append(k) # insert the link from j to k\n",
    "    return i\n",
    "\n",
    "def ip_mapper(input_key,input_value):\n",
    "    # The mapper used to compute the inner product between\n",
    "    # the vector of dangling pages and the current estimated\n",
    "    # PageRank.  The input is a key describing a webpage, and\n",
    "    # the corresponding data, including the estimated pagerank.\n",
    "    # The mapper returns [(1,pagerank)] if the page is dangling,\n",
    "    # and otherwise returns nothing.\n",
    "\n",
    "    if input_value[1] == 0: return [(1,input_value[0])]\n",
    "    else: return []\n",
    "\n",
    "def ip_reducer(input_key,input_value_list):\n",
    "    # The reducer used to compute the inner product.  Simply\n",
    "    # sums the pageranks listed in the input value list, which\n",
    "    # are all the pageranks for dangling pages.\n",
    "\n",
    "    return sum(input_value_list)\n",
    "\n",
    "def pr_mapper(input_key,input_value):\n",
    "    # The mapper used to update the PageRank estimate.  Takes\n",
    "    # as input a key for a webpage, and as a value the corresponding\n",
    "    # data, as described in the function initialize.  It returns a\n",
    "    # list with all outlinked pages as keys, and corresponding values\n",
    "    # just the PageRank of the origin page, divided by the total\n",
    "    # number of outlinks from the origin page.  Also appended to\n",
    "    # that list is a pair with key the origin page, and value 0.\n",
    "    # This is done to ensure that every single page ends up with at\n",
    "    # least one corresponding (intermediate_key,intermediate_value)\n",
    "    # pair output from a mapper.\n",
    "  \n",
    "    return [(input_key,0.0)]+[(outlink,input_value[0]/input_value[1])\n",
    "        for outlink in input_value[2]]\n",
    "\n",
    "def pr_reducer_inter(intermediate_key,intermediate_value_list,\n",
    "                     s,ip,n):\n",
    "    # This is a helper function used to define the reducer used\n",
    "    # to update the PageRank estimate.  Note that the helper differs\n",
    "    # from a standard reducer in having some additional inputs:\n",
    "    # s (the PageRank parameter), ip (the value of the inner product\n",
    "    # between the dangling pages vector and the estimated PageRank),\n",
    "    # and n, the number of pages.  Other than that the code is\n",
    "    # self-explanatory.\n",
    "  \n",
    "    return (intermediate_key,\n",
    "        s*sum(intermediate_value_list)+s*ip/n+(1.0-s)/n)\n",
    "\n",
    "def pagerank(i,s=0.85,tolerance=0.00001):\n",
    "    # Returns the PageRank vector for the web described by i,\n",
    "    # using parameter s.  The criterion for convergence is that\n",
    "    # we stop when M^(j+1)P-M^jP has length less than tolerance,\n",
    "    # in l1 norm.\n",
    "  \n",
    "    n = len(i)\n",
    "    iteration = 1\n",
    "    change = 2 # initial estimate of error\n",
    "    while change > tolerance:\n",
    "        print \"Iteration: \"+str(iteration)\n",
    "        # Run the MapReduce job used to compute the inner product\n",
    "        # between the vector of dangling pages and the estimated\n",
    "        # PageRank.\n",
    "        ip_list = map_reduce.map_reduce(i,ip_mapper,ip_reducer)\n",
    "\n",
    "        # the if-else clause is needed in case there are no dangling\n",
    "        # pages, in which case MapReduce returns ip_list as the empty\n",
    "        # list.  Otherwise, set ip equal to the first (and only)\n",
    "        # member of the list returned by MapReduce.\n",
    "        if ip_list == []: ip = 0\n",
    "        else: ip = ip_list[0]\n",
    "\n",
    "        # Dynamically define the reducer used to update the PageRank\n",
    "        # vector, using the current values for s, ip, and n.\n",
    "        pr_reducer = lambda x,y: pr_reducer_inter(x,y,s,ip,n)\n",
    "\n",
    "        # Run the MapReduce job used to update the PageRank vector.\n",
    "        new_i = map_reduce.map_reduce(i,pr_mapper,pr_reducer)\n",
    "\n",
    "        # Compute the new estimate of error.\n",
    "        change = sum([abs(new_i[j][1]-i[j][0]) for j in xrange(n)])\n",
    "        print \"Change in l1 norm: {0:1.12f}\".format(change)\n",
    "\n",
    "        # Update the estimate PageRank vector.\n",
    "        for j in xrange(n): i[j][0] = new_i[j][1]\n",
    "        iteration += 1\n",
    "    return i\n",
    "\n",
    "n = 1000 # works up to about 1000000 pages\n",
    "i = initialize(n,2.0)\n",
    "new_i = pagerank(i,0.85,0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Adapting to MrJob\n",
    "\n",
    "Let's start by adapting the current algorithm to MrJob. One of the first things we have to do is execute the commands that make Jupyter notebooks reload the Python modules each time we execure to pick up changes in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jupyter requires this for MRJob to reload classes properly\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting initweb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile initweb.py\n",
    "import numpy.random\n",
    "import random\n",
    "\n",
    "class initweb():\n",
    "    @staticmethod\n",
    "    def paretosample(n,power=2.0):\n",
    "        # Returns a sample from a truncated Pareto distribution\n",
    "        # with probability mass function p(l) proportional to\n",
    "        # 1/l^power.  The distribution is truncated at l = n.\n",
    "\n",
    "        m = n+1\n",
    "        while m > n: m = numpy.random.zipf(power)\n",
    "        return m\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize(n,power):\n",
    "        # Returns a Python dictionary representing a web\n",
    "        # with n pages, and where each page k is linked to by\n",
    "        # L_k random other pages.  The L_k are independent and\n",
    "        # identically distributed random variables with a\n",
    "        # shifted and truncated Pareto probability mass function\n",
    "        # p(l) proportional to 1/(l+1)^power.\n",
    "\n",
    "        # The representation used is a Python dictionary with\n",
    "        # keys 0 through n-1 representing the different pages.\n",
    "        # i[j][0] is the estimated PageRank, initially set at 1/n,\n",
    "        # i[j][1] the number of outlinks, and i[j][2] a list of\n",
    "        # the outlinks.\n",
    "\n",
    "        # This dictionary is used to supply (key,value) pairs to\n",
    "        # both mapper tasks defined below.\n",
    "\n",
    "        # initialize the dictionary\n",
    "        i = {} \n",
    "        for j in xrange(n): i[j] = [1.0/n,0,[]]\n",
    "\n",
    "        # For each page, generate inlinks according to the Pareto\n",
    "        # distribution. Note that this is somewhat tedious, because\n",
    "        # the Pareto distribution governs inlinks, NOT outlinks,\n",
    "        # which is what our representation is adapted to represent.\n",
    "        # A smarter representation would give easy\n",
    "        # access to both, while remaining memory efficient.\n",
    "        for k in xrange(n):\n",
    "            lk = initweb.paretosample(n+1,power)-1\n",
    "            values = random.sample(xrange(n),lk)\n",
    "            for j in values:\n",
    "                i[j][1] += 1 # increment the outlink count for page j\n",
    "                i[j][2].append(k) # insert the link from j to k\n",
    "        return i\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_file(n, power, filename):\n",
    "        # Calls the initialize function and then persists the dictionary\n",
    "        # as an adjacency list to the file specified by filename\n",
    "        dictionary = initweb.initialize(n, power)\n",
    "        with open(filename, 'w') as outfile:\n",
    "            for key in dictionary:\n",
    "                outfile.write('{0}\\t{1}\\n'.format(key,dictionary[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrJobDangler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrJobDangler.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "# We'll put the core map and reduce steps in an extension\n",
    "# of the MRJob class as we would for any implementation\n",
    "class MrJobDangler(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MrJobDangler, self).configure_options()    \n",
    "    \n",
    "    def ip_mapper(self, _, line):\n",
    "        # The mapper used to compute the inner product between\n",
    "        # the vector of dangling pages and the current estimated\n",
    "        # PageRank.  The input is a key describing a webpage, and\n",
    "        # the corresponding data, including the estimated pagerank.\n",
    "        # The mapper returns [(1,pagerank)] if the page is dangling,\n",
    "        # and otherwise returns nothing.\n",
    "        #\n",
    "        # Modification for MrJob is to read from the input splits\n",
    "        # a line at a time to format as the dictionary entry\n",
    "        \n",
    "        page, info = re.split('\\t',line.strip())\n",
    "        key = page.strip('\"')\n",
    "        input_value = eval(info)\n",
    "\n",
    "        # first we have to emit the graph as is for the second stage\n",
    "        yield key, input_value\n",
    "        \n",
    "        # then we emit the dangling page information\n",
    "        if input_value[1] == 0: yield '*',input_value[0]\n",
    "        else: yield '*',0\n",
    "            \n",
    "    def ip_combiner(self, input_key, input_value_list):\n",
    "        # The combiner is mentioned but not implemented in the article\n",
    "        # but we can implement it here simply enough.\n",
    "        # Sum the values for the key and emit the key, sum(values)\n",
    "        # Pass through anything that doesn't have * as the key\n",
    "        if input_key =='*': yield input_key, sum(input_value_list)\n",
    "        else: yield input_key, input_value_list\n",
    "\n",
    "    def ip_reducer(self, input_key,input_value_list):\n",
    "        # The reducer used to compute the inner product.  Simply\n",
    "        # sums the pageranks listed in the input value list, which\n",
    "        # are all the pageranks for dangling pages.\n",
    "        \n",
    "        # Since we're sending this to the second stage map reduce \n",
    "        # and we're not intercepting this like the original algorithm\n",
    "        # we need to emit it in a way that identifies it as the dangling\n",
    "        # page probability mass. Since we have to broadcast that mass\n",
    "        # to all the instances of the second stage we will emit it as \n",
    "        # a counter by multiplying it by 10^12 and taking the integer\n",
    "        # portion\n",
    "        if input_key == '*': \n",
    "            dangling_mass = int(1000000000000 * sum(input_value_list))\n",
    "            self.increment_counter('Page Rank', 'Dangling Mass', amount=dangling_mass)\n",
    "        else: \n",
    "            for value in input_value_list:\n",
    "                    yield input_key, value\n",
    "                \n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.ip_mapper,\n",
    "                       reducer=self.ip_reducer)\n",
    "                    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MrJobDangler.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrJobRanker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrJobRanker.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "# We'll put the core map and reduce steps in an extension\n",
    "# of the MRJob class as we would for any implementation\n",
    "class MrJobRanker(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MrJobRanker, self).configure_options()    \n",
    "        self.add_passthrough_option(\n",
    "            '--n', dest='n', type='int', default=0, \n",
    "            help=\"Number of nodes\")  \n",
    "        self.add_passthrough_option(\n",
    "            '--ip', dest='ip', type='float', default=0.85, \n",
    "            help=\"dangling mass\") \n",
    "        self.add_passthrough_option(\n",
    "            '--s', dest='s', type='float', default=0.85, \n",
    "            help=\"damping factor\") \n",
    "            \n",
    "    def mapper(self, _, line):\n",
    "        # we're not getting the data directly but from an intermediate\n",
    "        # input file split, so we need to recover the variables from \n",
    "        # each line presented to the mapper\n",
    "        node, info_list = re.split('\\t', line.strip())\n",
    "        input_key = node.strip('\"')\n",
    "        input_value = eval(info_list)\n",
    "\n",
    "        yield input_key, input_value\n",
    "\n",
    "        # next send out the rank distributed across the outlinks\n",
    "        # make sure the key is a string \n",
    "        for outlink in input_value[2]:\n",
    "            yield str(outlink), input_value[0]/input_value[1]\n",
    "        \n",
    "    def reducer(self, intermediate_key, intermediate_value_list):\n",
    "        \n",
    "        # Michael used a nice trick to send in a function for the reducer\n",
    "        # in the map_reduce function that he wrote. We need to pull this \n",
    "        # apart just a bit\n",
    "\n",
    "        # For the n variable we have the parameter as self.options.n\n",
    "        # For the s variable we have the parameter as self.options.s\n",
    "        # For the ip variable we have the parameter as self.options.ip\n",
    "        # but we need to divide the ip by the scalar we used to make it \n",
    "        # an integer counter value\n",
    "        \n",
    "        info_list = []\n",
    "        page_rank = 0.0\n",
    "        ip = float(self.options.ip)/1000000000000.0\n",
    "        for intermediate_value in intermediate_value_list:\n",
    "            if isinstance(intermediate_value, list):\n",
    "                # this is our graph. Save the info list\n",
    "                info_list = intermediate_value\n",
    "            else:\n",
    "                # this is one of the values to accumulate for page rank\n",
    "                page_rank += intermediate_value\n",
    "        page_rank = self.options.s*page_rank + \\\n",
    "                    self.options.s*ip/self.options.n + \\\n",
    "                    (1.0-self.options.s)/self.options.n\n",
    "        yield intermediate_key, [page_rank, len(info_list[2]), info_list[2]]\n",
    "\n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       reducer=self.reducer)\n",
    "                    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MrJobRanker.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n"
     ]
    }
   ],
   "source": [
    "from MrJobDangler import MrJobDangler\n",
    "from MrJobRanker import MrJobRanker\n",
    "from initweb import initweb\n",
    "\n",
    "def pagerank(infile,outfile, n=1000,s=0.85,tolerance=0.00001):\n",
    "    # Returns the PageRank vector for the web described by i,\n",
    "    # using parameter s.  The criterion for convergence is that\n",
    "    # we stop when M^(j+1)P-M^jP has length less than tolerance,\n",
    "    # in l1 norm.\n",
    "\n",
    "    mr_job = MrJobDangler(args=[infile])\n",
    "       \n",
    "    iteration = 1\n",
    "\n",
    "    while iteration < 20:\n",
    "        print \"Iteration: \"+str(iteration)\n",
    "        # Run the MapReduce job used to compute the inner product\n",
    "        # between the vector of dangling pages and the estimated\n",
    "        # PageRank.\n",
    "        # ip_list = map_reduce.map_reduce(i,ip_mapper,ip_reducer)\n",
    "        \n",
    "        with open(outfile, 'w') as rankfile:\n",
    "            with mr_job.make_runner() as runner:\n",
    "                runner.run()\n",
    "                for line in runner.stream_output():\n",
    "                    rankfile.write(line)\n",
    "                counters = runner.counters()\n",
    "                if 'Page Rank' in counters[0]:\n",
    "                    ip = counters[0]['Page Rank']['Dangling Mass']            \n",
    "\n",
    "\n",
    "        mr_job_rank = MrJobRanker(args=[outfile,\n",
    "                                        '--n', str(n),\n",
    "                                        '--s', str(s),\n",
    "                                        '--ip', str(ip)\n",
    "                                       ])\n",
    "        \n",
    "        with open(infile, 'w') as outfinal:\n",
    "            with mr_job_rank.make_runner() as runner:\n",
    "                runner.run()\n",
    "                for line in runner.stream_output():\n",
    "                    outfinal.write(line)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "n = 1000 # works up to about 1000000 pages\n",
    "input_file = 'pagerank-in.txt'\n",
    "output_file = 'pagerank-out.txt'\n",
    "\n",
    "initweb.initialize_file(n, 2.0, input_file)\n",
    "#i = initialize(n,2.0)\n",
    "pagerank(input_file, output_file,n,0.85,0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Toy Graph from HW9\n",
    "\n",
    "Now the algorithm seems to work correctly with MRJob but I have to say it's a bit of a hack on this first iteration. Since the code that Michael wrote has access to the intermediate map_reduce values I chose to replicate that by outputting the dangling mass value as a counter value. The counter value is then recovered by the driver and used as a parameter to the second stage MR. The problem with all that is that there is now an intermediate write to file between the first and second stage MR, and the data has to stream through the driver - not exactly scalable. But I can work on the scalability part in another iteration. For now I want to compare the toy graph calculation to my PageRank MR calculation from HW9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrJobTransform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrJobTransform.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "#\n",
    "# Visit all nodes of the graph to calculate the minimum distance of the graph \n",
    "#         \n",
    "class MrJobTransform(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MrJobTransform, self).configure_options()    \n",
    "        self.add_passthrough_option(\n",
    "            '--nodes', dest='nodes', type='int', default=0, \n",
    "            help=\"Number of nodes\")\n",
    "        self.add_passthrough_option(\n",
    "            '--maptasks', dest='mappers', type='int', default=2, \n",
    "            help=\"mapper task instances\")\n",
    "        self.add_passthrough_option(\n",
    "            '--reducetasks', dest='reducers', type='int', default=1, \n",
    "            help=\"reducer task instances\")\n",
    "        \n",
    "\n",
    "    # each line is from the adjacency list node_id \\t {neighbor: weight}\n",
    "    # where {neighbor:weight} is one of m elements in the dictionary\n",
    "    # emit node \\t [page_rank, num_outlinks, [outlinks]]\n",
    "    def mapper(self, _, line):\n",
    "        node, adj_list = re.split('\\t',line.strip())\n",
    "        node = node.strip('\"')\n",
    "        neighbors = eval(adj_list)\n",
    "        yield node, [neighbors.keys()]\n",
    "        for neighbor in neighbors:\n",
    "            yield neighbor, []\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.rank = 1.0/self.options.nodes\n",
    "        \n",
    "    # node, [page_rank, [outlinks]]\n",
    "    def reducer(self, node, outlinks_list):\n",
    "        combined_outlinks = []\n",
    "        for outlinks in outlinks_list:\n",
    "            for outlink in outlinks:\n",
    "                combined_outlinks = combined_outlinks + outlink\n",
    "        self.increment_counter('transformer', 'nodes', 1) \n",
    "        yield node, [self.rank, len(combined_outlinks), combined_outlinks]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       reducer_init=self.reducer_init,\n",
    "                       reducer=self.reducer,\n",
    "                       jobconf = {\n",
    "                        'mapreduce.job.maps' : self.options.mappers,\n",
    "                        'mapreduce.job.reduces' : self.options.reducers}\n",
    "                    )]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrJobTransform.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Count: 11\n"
     ]
    }
   ],
   "source": [
    "from MrJobTransform import MrJobTransform\n",
    "              \n",
    "def transform(qfile):\n",
    "    mr_job = MrJobTransform(args=[qfile,\n",
    "                                     '-r','local', \n",
    "                                     '--nodes', '1000', \n",
    "                                     '--maptasks', '2',\n",
    "                                     '--reducetasks', '1',\n",
    "                                    ])\n",
    "\n",
    "    with open('pagerank-in.txt','w') as rankfile:\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "                rankfile.write(line)\n",
    "            counters = runner.counters()\n",
    "            print 'Node Count: {0}'.format(counters[0]['transformer']['nodes'])\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    transform('HW9/Data/PageRank-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n",
      "Iteration: 47\n",
      "Iteration: 48\n",
      "Iteration: 49\n"
     ]
    }
   ],
   "source": [
    "from MrJobDangler import MrJobDangler\n",
    "from MrJobRanker import MrJobRanker\n",
    "from initweb import initweb\n",
    "\n",
    "def pagerank(infile,outfile, n=1000,s=0.85,tolerance=0.00001):\n",
    "\n",
    "    mr_job = MrJobDangler(args=[infile])\n",
    "       \n",
    "    iteration = 1\n",
    "\n",
    "    while iteration < 50:\n",
    "        print \"Iteration: \"+str(iteration)\n",
    "        # Run the MapReduce job used to compute the inner product\n",
    "        # between the vector of dangling pages and the estimated\n",
    "        # PageRank.\n",
    "        # ip_list = map_reduce.map_reduce(i,ip_mapper,ip_reducer)\n",
    "        \n",
    "        with open(outfile, 'w') as rankfile:\n",
    "            with mr_job.make_runner() as runner:\n",
    "                runner.run()\n",
    "                for line in runner.stream_output():\n",
    "                    rankfile.write(line)\n",
    "                counters = runner.counters()\n",
    "                if 'Page Rank' in counters[0]:\n",
    "                    ip = counters[0]['Page Rank']['Dangling Mass']            \n",
    "\n",
    "\n",
    "        mr_job_rank = MrJobRanker(args=[outfile,\n",
    "                                        '--n', str(n),\n",
    "                                        '--s', str(s),\n",
    "                                        '--ip', str(ip)\n",
    "                                       ])\n",
    "        \n",
    "        with open(infile, 'w') as outfinal:\n",
    "            with mr_job_rank.make_runner() as runner:\n",
    "                runner.run()\n",
    "                for line in runner.stream_output():\n",
    "                    outfinal.write(line)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "n = 11 # works up to about 1000000 pages\n",
    "input_file = 'pagerank-in.txt'\n",
    "output_file = 'pagerank-out.txt'\n",
    "\n",
    "#initweb.initialize_file(n, 2.0, input_file)\n",
    "#i = initialize(n,2.0)\n",
    "pagerank(input_file, output_file,n,0.85,0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The output of this algorithm comes to the correct convergence for the toy graph as presented for HW9. I think one difference is that I realized that the dangling mass is not distributed correctly when you have more than one mapper and reducer; it only gets distributed on the keys for a single reducer. There may be more to it than that, however, since when I look at the structure that I put together for the 2 stage MR jobs it _should_ work correctly, but it's slightly off.\n",
    "\n",
    "Comparing the actual calculation of in the second stage reducer, the calculations are the same, as is where the dangling mass is calculated. Therefore it must be that it's the distribution of the mass across the mapper and reducer instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
