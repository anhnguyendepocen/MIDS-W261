{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR Book 16.3 Concept Code\n",
    "\n",
    "This code implements the Bernoulli EM algorithm as described in the IR Book, example 16.3. This exercise is useful to ensure an understanding of the algorithm and to check functionality. There are a few bits that aren't explained will in the book but become apparent when working with this code. \n",
    "\n",
    "- During the computation of the $r_{n,k}$ classifications it is best to use log(probabilities) in order to prevent underflow\n",
    "\n",
    "- The use of $\\epsilon$ as a smoothing parameter is crucial to the good behavior of the algorithm and to avoid either a divide by zero or a log(0) problem. In the IR book $\\epsilon$ is set to 0.0001. Setting to smaller values causes the algorithm to take more iterations to converge to the same solution as in the book.\n",
    "\n",
    "In a Bernoulli Mixture Model a document is a vector of Booleans indicating the presence of a term.\n",
    "\n",
    "The conditional probability of a document given a set of parameters is given by:\n",
    "\n",
    "$$P(d \\;|\\;\\theta) = \\sum_{k=1}^{K}\\alpha_k(\\prod_{tm \\in d} q_{mk})(\\prod_{tm \\notin d}(1 - q_{mk}))$$\n",
    "\n",
    "This is the sum for each class of the product of the probabilities of the terms in a document with 1 minus the probabilities of the terms not in the document.\n",
    "\n",
    "The probability that a document from cluster $\\omega_{k}$ containts term $t_{m}$ is given by:\n",
    "\n",
    "$$q_{mk} = P(U_{m} = 1 \\; | \\;\\omega_{k})$$\n",
    "\n",
    "The prior $\\alpha_{k}$ of cluster $\\omega_{k}$ is the probability document $d$ is in $\\omega_{k}$ if we have no other information about it.\n",
    "\n",
    "When we don't know the classifications of the documents we can use Expectation Maximization iteratively to arrive at classifications, $r_{nk}$.\n",
    "\n",
    "**E Step**\n",
    "\n",
    "$$\\tag{1} r_{nk} = \\frac{\\alpha_{k}\\left(\\prod_{tm \\in d_n}q_{mk})(\\prod_{tm \\notin d_n}(1-q_{mk})\\right)}{\\sum_{k=1}^{K}\\alpha_{k}\\left(\\prod_{tm \\in d_n}q_{mk})(\\prod_{tm \\notin d_n}(1-q_{mk})\\right)}$$\n",
    "\n",
    "The actual computation as implemented by taking the sum of the log probabilities as opposed to the products of the probabilities themselves. This is necessary because once you are dealing with many terms, multiplying a lot of small numbers results in numeric underflow. Also, in order to prevent taking the log(0), which will happen in the first iteration, a very small number, $\\delta$, is added to the probability before taking the log. So we end up with:\n",
    "\n",
    "$$\\tag{1a} \\frac{\\alpha_{k}exp{\\left(\\sum_{tm \\in d_n}log(q_{mk}) + \\sum_{tm \\notin d_n}log(1-q_{mk}) \\right)}}{\\sum_{k}\\alpha_{k} exp{\\left(\\sum_{tm \\in d_n} log(q_{mk}) + \\sum_{tm \\notin d_n}log(1-q_{mk}) \\right)}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\tag{1a} exp\\left[log(\\alpha_{k}) +\\sum_{tm \\in d_n}log(q_{mk}) + \\sum_{tm \\notin d_n}log(1-q_{mk}) -log\\sum_{k} exp\\left( log(\\alpha_{k}) + \\sum_{tm \\in d_n} log(q_{mk}) + \\sum_{tm \\notin d_n}log(1-q_{mk})\\right)\\right]$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\tag{1a} \\frac{\\alpha_{k}exp{\\left(\\sum_{tm \\in d_n}log(q_{mk}+\\delta) + \\sum_{tm \\notin d_n}log(1-q_{mk}+\\delta) \\right)}}{\\sum_{k}\\alpha_{k} exp{\\left(\\sum_{tm \\in d_n} \\left(log(q_{mk}+\\delta) + \\sum_{tm \\notin d_n}log(1-q_{mk}+\\delta) \\right) \\right)}}$$\n",
    "\n",
    "In the code below, a single pass is made to calculate the $r$ class soft assignments so that the terms are calculated only once so it may not be obvious what's going on.\n",
    "\n",
    "_Note: the $\\delta$ values are important for the algorithm to converge. If you take a straight calculation of the $r_{1,1}$ term in the first iteration you will end up with a divide by zero using the original equations. By including the $\\delta$ term this is avoided and you'll see that the result is very close to 1_\n",
    "\n",
    "**M Step**\n",
    "\n",
    "$$\\tag{2} q_{mk} = \\frac{\\sum_{n=1}^{N}(r_{nk} + \\epsilon)I(t_m \\in d_n)}{\\sum_{i=1}^{N}(r_{nk} + \\epsilon)}$$\n",
    "\n",
    "$I(t_{m} \\in d_{n}) = 1$ if term is an element of document n and 0 otherwise.\n",
    "\n",
    "$\\epsilon$ is for smoothing\n",
    "\n",
    "Finally, priors are updated per iteration as:\n",
    "\n",
    "$$\\tag{3} \\alpha_k = \\frac{\\sum_{n=1}^{N}r_{nk}}{N}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save this to a file - it will be handy for MR testing later\n",
    "with open('test.txt','w') as outfile:\n",
    "    outfile.write('hot chocolate cocoa beans\\n')\n",
    "    outfile.write('cocoa ghana africa\\n')\n",
    "    outfile.write('beans harvest ghana\\n')\n",
    "    outfile.write('cocoa butter\\n')\n",
    "    outfile.write('butter truffles\\n')\n",
    "    outfile.write('sweet chocolate\\n')\n",
    "    outfile.write('sweet sugar\\n')\n",
    "    outfile.write('sugar cane brazil\\n')\n",
    "    outfile.write('sweet sugar beet\\n')\n",
    "    outfile.write('sweet cake icing\\n')\n",
    "    outfile.write('cake black forest\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:83: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:91: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "# read the documents. Each document consists of a list of words.\n",
    "documents = []\n",
    "with open('test.txt', 'r') as docfile:\n",
    "    for line in docfile.readlines():\n",
    "        documents.append(re.split(' ', line.strip()))\n",
    "\n",
    "classes = 2\n",
    "r = [[None] * len(documents), [None] * len(documents)]\n",
    "\n",
    "# set our initial conditions (r_6,1 = 1.0 and r_7,1 = 0.0 and the converse for the other class)\n",
    "r[0][5] = r[1][6] = 1.0\n",
    "r[0][6] = r[1][5] = 0.0\n",
    "\n",
    "# initialize the priors\n",
    "alpha = [0.0] * classes\n",
    "\n",
    "# delta is to keep the arithmetic well behaved - it is not the smoothing factor\n",
    "delta = 0\n",
    "\n",
    "# epsilon is for smoothing in Equation 2 and 3 (16.16 in the IR Book)\n",
    "epsilon = 0.0001\n",
    "\n",
    "# conditional term probabilities\n",
    "qm = {}\n",
    "\n",
    "# compute alphas - Equation 3\n",
    "def compute_alphas(alphas):\n",
    "    for k in range(classes):\n",
    "        alphas[k] = sum([x+epsilon for x in r[k] if x is not None])/len([x for x in r[k] if x is not None])\n",
    "\n",
    "\n",
    "# compute inverted postings list\n",
    "# this is handy for the computation of the qm's\n",
    "def compute_postings():\n",
    "    postings = {}\n",
    "    for i in range(len(documents)):\n",
    "        if r[0][i] is not None:\n",
    "            for word in documents[i]:\n",
    "                if word not in postings:\n",
    "                    postings[word] = [i]\n",
    "                else:\n",
    "                    if i not in postings[word]:\n",
    "                        postings[word].append(i)\n",
    "    return postings\n",
    "\n",
    "# compute qm's - Equation 2\n",
    "def compute_next_qms(qm):\n",
    "    for k in range(classes):\n",
    "        for i in range(len(r[k])):\n",
    "            if r[k][i] is not None:\n",
    "                for word in documents[i]:\n",
    "                    # use smoothing value epsilon\n",
    "                    if word not in qm:\n",
    "                        qm[word] = {k: sum([r[k][j]+epsilon for j in postings[word]]) / \\\n",
    "                                    sum([x+epsilon for x in r[k] if x is not None])}\n",
    "                    else:\n",
    "                        qm[word][k] = sum([r[k][j]+epsilon for j in postings[word]]) / \\\n",
    "                                    sum([x+epsilon for x in r[k] if x is not None])\n",
    "\n",
    "# compute qm's - Equation 2 version 2\n",
    "def compute_qms(qm):\n",
    "    # for each of the classes\n",
    "    for k in range(classes):\n",
    "        # for each of the terms\n",
    "        for term in postings:\n",
    "            numerator = 0.0\n",
    "            # for each of the documents in the postings list for the term\n",
    "            for doc in postings[term]:\n",
    "                numerator += r[k][doc]\n",
    "        qm[k][term]=numerator/sum(filter(None,r[k]))\n",
    "        \n",
    "            \n",
    "\n",
    "# compute next iteration of r's - Equation 1a\n",
    "# note - need to do log(probability)\n",
    "def compute_next_r(rs):\n",
    "    for i in range(len(documents)):\n",
    "        vocab_words_in_doc = []\n",
    "        p = np.zeros((classes, 2))\n",
    "        \n",
    "        # find all vocab words in the doc. Note: there may be none.\n",
    "        for word in documents[i]:\n",
    "            if word in qm:\n",
    "                vocab_words_in_doc.append(word)\n",
    "                for k in range(classes):\n",
    "                    # prevent math errors by not taking log(0)\n",
    "                    p[k][0] += np.log(qm[word][k] + delta)\n",
    "        if len(vocab_words_in_doc) > 0:\n",
    "            # find all vocab words not in doc for all classes at the same time\n",
    "            for word in qm:\n",
    "                if word not in vocab_words_in_doc:\n",
    "                    for k in range(classes):\n",
    "                        # prevent math errors by not taking log(0)\n",
    "                        p[k][1] += np.log(1-qm[word][k] + delta)                        \n",
    "            # compute the denominator of Equation 1a for all classes\n",
    "            denom = 0.0\n",
    "            for k in range(classes):\n",
    "                denom += alpha[k]*np.exp(p[k][0]+p[k][1])\n",
    "                \n",
    "            # compute the new r of Equation 1a for all classes\n",
    "            for k in range(classes):\n",
    "                rs[k][i] = alpha[k]*np.exp(p[k][0]+p[k][1])/denom\n",
    "\n",
    "        else:\n",
    "            # set to prior in case of no information\n",
    "            for k in range(classes):\n",
    "                rs[k][i] = alpha[k]\n",
    "\n",
    "# iterate\n",
    "for _ in range(25):\n",
    "    compute_alphas(alpha)\n",
    "    postings = compute_postings()\n",
    "    compute_next_qms(qm)\n",
    "    compute_next_r(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft classifications\n",
      "[[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      " [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]]\n",
      "\n",
      "Priors (alpha) [ nan  nan]\n"
     ]
    }
   ],
   "source": [
    "# if you take out rounding you can see that these values are not exact\n",
    "print 'Soft classifications\\n',np.around(r, 2)\n",
    "print '\\nPriors (alpha)',np.around(alpha,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "# read the documents. Each document consists of a list of words. From reading each document,\n",
    "# construct a list of unique terms and reconstruct the document as a list of indexes into\n",
    "# the unique terms. This is an alternative to creating a boolean vector where the index of the\n",
    "# boolean corresponds to the index of the term. If there are duplicated terms in a document\n",
    "# we only include one instance as we only care about whether the term is in the doc\n",
    "#\n",
    "# documents is a dictionary so that it uses a document id as a key\n",
    "documents = {}\n",
    "terms = []\n",
    "with open('test.txt', 'r') as docfile:\n",
    "    i = 1\n",
    "    for line in docfile.readlines():\n",
    "        doc = []\n",
    "        doc_words = re.split(' ', line.strip())\n",
    "        for word in doc_words:\n",
    "            if word not in terms:\n",
    "                terms.append(word)\n",
    "            if terms.index(word) not in doc:\n",
    "                doc.append(terms.index(word))\n",
    "        documents[i] = doc\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = 2\n",
    "\n",
    "r = {}\n",
    "for k in range(classes):\n",
    "    for doc in documents:\n",
    "        if doc not in r:\n",
    "            r[doc] = {k : None}\n",
    "        else:\n",
    "            r[doc][k] = None\n",
    "\n",
    "\n",
    "# set our initial conditions (r_6,1 = 1.0 and r_7,1 = 0.0 and the converse for the other class)\n",
    "r[6][0] = r[7][1] = 1.0\n",
    "r[6][1] = r[7][0] = 0.0\n",
    "\n",
    "# initialize the priors\n",
    "alpha = [0.0] * classes\n",
    "\n",
    "delta = 1E-12\n",
    "\n",
    "# epsilon is for smoothing in Equation 2 and 3 (16.16 in the IR Book)\n",
    "epsilon = 0.0001\n",
    "\n",
    "# conditional term probabilities\n",
    "qm = np.zeros((classes, len(terms)))\n",
    "\n",
    "# the vocabulary under consideration changes with each iteration\n",
    "vocab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute alphas - Equation 3\n",
    "def compute_alphas():\n",
    "    for k in range(classes):\n",
    "        alpha[k] = 0.0\n",
    "        n = 0\n",
    "        for doc in r:\n",
    "            if r[doc][k] is not None:\n",
    "                alpha[k] += r[doc][k]\n",
    "                n = n+1\n",
    "        alpha[k] = alpha[k]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute inverted postings list\n",
    "# this is handy for the computation of the qm's\n",
    "def compute_postings():\n",
    "    postings = {}\n",
    "    for doc in documents:\n",
    "        for term_idx in documents[doc]:\n",
    "            if term_idx not in postings:\n",
    "                postings[term_idx] = [doc]\n",
    "            else:\n",
    "                if doc not in postings[term_idx]:\n",
    "                    postings[term_idx].append(doc)\n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [0, 1, 2, 3],\n",
       " 2: [2, 4, 5],\n",
       " 3: [3, 6, 4],\n",
       " 4: [2, 7],\n",
       " 5: [7, 8],\n",
       " 6: [9, 1],\n",
       " 7: [9, 10],\n",
       " 8: [10, 11, 12],\n",
       " 9: [9, 10, 13],\n",
       " 10: [9, 14, 15],\n",
       " 11: [14, 16, 17]}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hot',\n",
       " 'chocolate',\n",
       " 'cocoa',\n",
       " 'beans',\n",
       " 'ghana',\n",
       " 'africa',\n",
       " 'harvest',\n",
       " 'butter',\n",
       " 'truffles',\n",
       " 'sweet',\n",
       " 'sugar',\n",
       " 'cane',\n",
       " 'brazil',\n",
       " 'beet',\n",
       " 'cake',\n",
       " 'icing',\n",
       " 'black',\n",
       " 'forest']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {0: None, 1: None},\n",
       " 2: {0: None, 1: None},\n",
       " 3: {0: None, 1: None},\n",
       " 4: {0: None, 1: None},\n",
       " 5: {0: None, 1: None},\n",
       " 6: {0: 1.0, 1: 0.0},\n",
       " 7: {0: 0.0, 1: 1.0},\n",
       " 8: {0: None, 1: None},\n",
       " 9: {0: None, 1: None},\n",
       " 10: {0: None, 1: None},\n",
       " 11: {0: None, 1: None}}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postings = compute_postings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1],\n",
       " 1: [1, 6],\n",
       " 2: [1, 2, 4],\n",
       " 3: [1, 3],\n",
       " 4: [2, 3],\n",
       " 5: [2],\n",
       " 6: [3],\n",
       " 7: [4, 5],\n",
       " 8: [5],\n",
       " 9: [6, 7, 9, 10],\n",
       " 10: [7, 8, 9],\n",
       " 11: [8],\n",
       " 12: [8],\n",
       " 13: [9],\n",
       " 14: [10, 11],\n",
       " 15: [10],\n",
       " 16: [11],\n",
       " 17: [11]}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute qm's - Equation 2 version 2\n",
    "def compute_qms():\n",
    "    # for each of the classes\n",
    "    for k in range(classes):\n",
    "        # compute the denominator\n",
    "        denominator = 0.0\n",
    "        for doc in r:\n",
    "            if r[doc][k] is not None:\n",
    "                denominator += r[doc][k]+epsilon\n",
    "        # compute the numerator across all terms\n",
    "        for term in postings:\n",
    "            numerator = 0.0\n",
    "            # for each of the documents in the postings list for the term\n",
    "            for doc in postings[term]:\n",
    "                if r[doc][k] is not None:\n",
    "                    numerator += r[doc][k] + epsilon\n",
    "#                print k,term,doc, numerator\n",
    "            qm[k][term]=numerator/denominator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa 1  0.0\n",
      "africa 2  0.0\n",
      "brazil 1  0.0\n",
      "brazil 2  0.0\n",
      "cocoa 1  0.0\n",
      "cocoa 2  0.0\n",
      "sugar 1  0.0\n",
      "sugar 2  1.0\n",
      "sweet 1  1.0\n",
      "sweet 2  1.0\n"
     ]
    }
   ],
   "source": [
    "compute_qms()\n",
    "print 'africa 1 ',np.round(qm[0][terms.index('africa')],2)\n",
    "print 'africa 2 ',np.round(qm[1][terms.index('africa')],2)\n",
    "print 'brazil 1 ',np.round(qm[0][terms.index('brazil')],2)\n",
    "print 'brazil 2 ',np.round(qm[1][terms.index('brazil')],2)\n",
    "print 'cocoa 1 ',np.round(qm[0][terms.index('cocoa')],2)\n",
    "print 'cocoa 2 ',np.round(qm[1][terms.index('cocoa')],2)\n",
    "print 'sugar 1 ',np.round(qm[0][terms.index('sugar')],2)\n",
    "print 'sugar 2 ',np.round(qm[1][terms.index('sugar')],2)\n",
    "print 'sweet 1 ',np.round(qm[0][terms.index('sweet')],2)\n",
    "print 'sweet 2 ',np.round(qm[1][terms.index('sweet')],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(qm,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0.5])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_alphas()\n",
    "np.around(alpha,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_vocabulary():\n",
    "    # find the documents with a classification score and only use the terms in \n",
    "    # those documents to compute the next r values\n",
    "    for doc in r:\n",
    "        # doesn't matter which class it is\n",
    "        if r[doc][0] is not None:\n",
    "            for term in documents[doc]:\n",
    "                if term not in vocab:\n",
    "                    vocab.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_r():    \n",
    "    # compute the denominator once since it is summed across all classes\n",
    "    denominator = 0.0\n",
    "    for k in range(classes):\n",
    "        dt = 0.0\n",
    "        for term in vocab:\n",
    "            dt += np.log(qm[k][term]+delta) + np.log(1-qm[k][term]+delta)\n",
    "            print '-----',term, qm[k][term]\n",
    "        denominator = denominator + alpha[k]*np.exp(dt)\n",
    "\n",
    "    print denominator\n",
    "    # compute the new document soft class assignment, r, for each class, k\n",
    "    for k in range(classes):        \n",
    "        for doc in documents:\n",
    "            # if this document has an terms in the vocabulary, compute the new r\n",
    "            if not set(documents[doc]).isdisjoint(vocab):\n",
    "                print doc\n",
    "                numerator = 0.0\n",
    "                for term in vocab:\n",
    "                    if term in documents[doc]:\n",
    "                        numerator += np.log(qm[k][term]+delta)\n",
    "                    else:\n",
    "                        numerator += np.log(1.0 - qm[k][term]+delta)\n",
    "                r[doc][k] = (alpha[k]*np.exp(numerator))/denominator\n",
    "                print doc,' ',k,' ',(alpha[k]*np.exp(numerator))/denominator\n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 1, 10]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_vocabulary()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 9 0.0\n",
      "----- 1 0.0\n",
      "----- 10 0.0\n",
      "----- 9 0.0\n",
      "----- 1 0.0\n",
      "----- 10 0.0\n",
      "0.0\n",
      "1\n",
      "1   0   nan\n",
      "6\n",
      "6   0   nan\n",
      "7\n",
      "7   0   nan\n",
      "8\n",
      "8   0   nan\n",
      "9\n",
      "9   0   nan\n",
      "10\n",
      "10   0   nan\n",
      "1\n",
      "1   1   nan\n",
      "6\n",
      "6   1   nan\n",
      "7\n",
      "7   1   nan\n",
      "8\n",
      "8   1   nan\n",
      "9\n",
      "9   1   nan\n",
      "10\n",
      "10   1   nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {0: nan, 1: nan},\n",
       " 2: {0: None, 1: None},\n",
       " 3: {0: None, 1: None},\n",
       " 4: {0: None, 1: None},\n",
       " 5: {0: None, 1: None},\n",
       " 6: {0: nan, 1: nan},\n",
       " 7: {0: nan, 1: nan},\n",
       " 8: {0: nan, 1: nan},\n",
       " 9: {0: nan, 1: nan},\n",
       " 10: {0: nan, 1: nan},\n",
       " 11: {0: None, 1: None}}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_r()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:83: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:91: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# compute next iteration of r's - Equation 1a\n",
    "# note - need to do log(probability)\n",
    "def compute_next_r(rs):\n",
    "    for i in range(len(documents)):\n",
    "        vocab_words_in_doc = []\n",
    "        p = np.zeros((classes, 2))\n",
    "        \n",
    "        # find all vocab words in the doc. Note: there may be none.\n",
    "        for word in documents[i]:\n",
    "            if word in qm:\n",
    "                vocab_words_in_doc.append(word)\n",
    "                for k in range(classes):\n",
    "                    # prevent math errors by not taking log(0)\n",
    "                    p[k][0] += np.log(qm[word][k] + delta)\n",
    "        if len(vocab_words_in_doc) > 0:\n",
    "            # find all vocab words not in doc for all classes at the same time\n",
    "            for word in qm:\n",
    "                if word not in vocab_words_in_doc:\n",
    "                    for k in range(classes):\n",
    "                        # prevent math errors by not taking log(0)\n",
    "                        p[k][1] += np.log(1-qm[word][k] + delta)                        \n",
    "            # compute the denominator of Equation 1a for all classes\n",
    "            denom = 0.0\n",
    "            for k in range(classes):\n",
    "                denom += alpha[k]*np.exp(p[k][0]+p[k][1])\n",
    "                \n",
    "            # compute the new r of Equation 1a for all classes\n",
    "            for k in range(classes):\n",
    "                rs[k][i] = alpha[k]*np.exp(p[k][0]+p[k][1])/denom\n",
    "\n",
    "        else:\n",
    "            # set to prior in case of no information\n",
    "            for k in range(classes):\n",
    "                rs[k][i] = alpha[k]\n",
    "\n",
    "# iterate\n",
    "for _ in range(25):\n",
    "    compute_alphas(alpha)\n",
    "    postings = compute_postings()\n",
    "    compute_next_qms(qm)\n",
    "    compute_next_r(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
