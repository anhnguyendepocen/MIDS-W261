{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR Book 16.3 Concept Code\n",
    "\n",
    "This code implements the Bernoulli EM algorithm as described in the IR Book, example 16.3. This exercise is useful to ensure an understanding of the algorithm and to check functionality. There are a few bits that aren't explained will in the book but become apparent when working with this code. \n",
    "\n",
    "- During the computation of the $r_{n,k}$ classifications it is best to use log(probabilities) in order to prevent underflow\n",
    "\n",
    "- The use of $\\epsilon$ as a smoothing parameter is crucial to the good behavior of the algorithm and to avoid either a divide by zero or a log(0) problem. In the IR book $\\epsilon$ is set to 0.0001. Setting to smaller values causes the algorithm to take more iterations to converge to the same solution as in the book.\n",
    "\n",
    "In a Bernoulli Mixture Model a document is a vector of Booleans indicating the presence of a term.\n",
    "\n",
    "The conditional probability of a document given a set of parameters is given by:\n",
    "\n",
    "$$P(d \\;|\\;\\theta) = \\sum_{k=1}^{K}\\alpha_k(\\prod_{tm \\in d} q_{mk})(\\prod_{tm \\notin d}(1 - q_mk))$$\n",
    "\n",
    "This is the sum for each class of the product of the probabilities of the terms in a document with 1 minus the probabilities of the terms not in the document.\n",
    "\n",
    "The probability that a document from cluster $\\omega_{k}$ containts term $t_{m}$ is given by:\n",
    "\n",
    "$$q_{mk} = P(U_{m} = 1 \\; | \\;\\omega_{k})$$\n",
    "\n",
    "The prior $\\alpha_{k}$ of cluster $\\omega_{k}$ is the probability document $d$ is in $\\omega_{k}$ if we have no other information about it.\n",
    "\n",
    "When we don't know the classifications of the documents we can use Expectation Maximization iteratively to arrive at classifications, $r_{nk}$.\n",
    "\n",
    "**E Step**\n",
    "\n",
    "$$\\tag{1} r_{nk} = \\frac{\\alpha_{k}(\\prod_{tm \\in d_n}q_{mk})(\\prod_{tm \\in d_n}1-q_{mk})}{\\sum_{k=1}^{K}\\alpha_{k}(\\prod_{tm \\in d_n}q_{mk})(\\prod_{tm \\in d_n}1-q_{mk})}$$\n",
    "\n",
    "The actual computation as implemented by taking the sum of the log probabilities as opposed to the products of the probabilities themselves. This is necessary because once you are dealing with many terms, multiplying a lot of small numbers results in numeric underflow. Also, in order to prevent taking the log(0), which will happen in the first iteration, a very small number, $\\epsilon$, is added to the probability before taking the log. So we end up with:\n",
    "\n",
    "$$\\tag{1a} \\frac{\\alpha_{k}e^{\\left(\\sum_{tm \\in d_n}log(q_{mk}+\\epsilon) + \\sum_{tm \\notin d_n}log(1-q_{mk}+\\epsilon) \\right)}}{\\sum_{k}\\alpha_{k} e^{\\left(\\sum_{tm \\in d_n} \\left(log(q_{mk}+\\epsilon) + \\sum_{tm \\notin d_n}log(1-q_{mk}+\\epsilon) \\right) \\right)}}$$\n",
    "\n",
    "In the code below, a single pass is made to calculate the $r$ class soft assignments so that the terms are calculated only once so it may not be obvious what's going on.\n",
    "\n",
    "_Note: the $\\epsilon$ values are important for the algorithm to converge. If you take a straight calculation of the $r_{1,1}$ term in the first iteration you will end up with a divide by zero using the original equations. By including the $\\epsilon$ term this is avoided and you'll see that the result is very close to 1_\n",
    "\n",
    "**M Step**\n",
    "\n",
    "$$\\tag{2} q_{mk} = \\frac{\\sum_{n=1}^{N}r_{nk}I(t_m \\in d_n)}{\\sum_{i=1}^{N}r_{nk}}$$\n",
    "\n",
    "$I(t_{m} \\in d_{n}) = 1$ if term is an element of document n and 0 otherwise.\n",
    "\n",
    "Finally, priors are updated per iteration as:\n",
    "\n",
    "$$\\tag{3} \\alpha_k = \\frac{\\sum_{n=1}^{N}r_{nk}}{N}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
