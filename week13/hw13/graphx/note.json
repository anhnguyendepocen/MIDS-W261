{"paragraphs":[{"text":"%md\n\n## HW 13.1: Spark implementation of basic PageRank\n\nWrite a basic Spark implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input.\nMake sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iterationso that the output of each iteration is correctly normalized (sums to 1).\n\n[**NOTE:** The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,chooses the next page to which it will move by clicking at random, with probability d, one of the hyperlinks in the current page. This probability is represented by a so-called ‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page]\n\nIn your Spark solution, please use broadcast variables and caching to make sure your code is as efficient as possible.\n\nAs you build your code, use the test data from S3 or [DropBox](https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0) under subfolder HW7.\n\n    s3://ucb-mids-mls-networks/PageRank-test.txt\n    \nwith teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck your work with the true result, displayed in the first image in the [Wikipedia](https://en.wikipedia.org/wiki/PageRank)\n\nHere are the contents of the PageRank-test.txt file:\n\n    B       {'C': 1}\n    C       {'B': 1}\n    D       {'A': 1, 'B': 1}\n    E       {'D': 1, 'B': 1, 'F': 1}\n    F       {'B': 1, 'E': 1}\n    G       {'B': 1, 'E': 1}\n    H       {'B': 1, 'E': 1}\n    I       {'B': 1, 'E': 1}\n    J       {'E': 1}\n    K       {'E': 1}\n\nAnd here for reference are the corresponding PageRank probabilities:\n\n    A,0.033\n    B,0.384\n    C,0.343\n    D,0.039\n    E,0.081\n    F,0.039\n    G,0.016\n    H,0.016\n    I,0.016\n    J,0.016\n    K,0.016\n\nRun this experiment locally first. Report the local configuration that you used and how long in minutes and seconds it takes to complete your job.\n\nRepeat this experiment on AWS. Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job. (in your notebook, cat the cluster config file)\n\n","dateUpdated":"Apr 29, 2016 3:08:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530994_1100382634","id":"20160429-150850_1681937453","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>HW 13.1: Spark implementation of basic PageRank</h2>\n<p>Write a basic Spark implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input.\n<br  />Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iterationso that the output of each iteration is correctly normalized (sums to 1).</p>\n<p>[<strong>NOTE:</strong> The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,chooses the next page to which it will move by clicking at random, with probability d, one of the hyperlinks in the current page. This probability is represented by a so-called ‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page]</p>\n<p>In your Spark solution, please use broadcast variables and caching to make sure your code is as efficient as possible.</p>\n<p>As you build your code, use the test data from S3 or <a href=\"https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0\">DropBox</a> under subfolder HW7.</p>\n<pre><code>s3://ucb-mids-mls-networks/PageRank-test.txt\n</code></pre>\n<p>with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck your work with the true result, displayed in the first image in the <a href=\"https://en.wikipedia.org/wiki/PageRank\">Wikipedia</a></p>\n<p>Here are the contents of the PageRank-test.txt file:</p>\n<pre><code>B       {'C': 1}\nC       {'B': 1}\nD       {'A': 1, 'B': 1}\nE       {'D': 1, 'B': 1, 'F': 1}\nF       {'B': 1, 'E': 1}\nG       {'B': 1, 'E': 1}\nH       {'B': 1, 'E': 1}\nI       {'B': 1, 'E': 1}\nJ       {'E': 1}\nK       {'E': 1}\n</code></pre>\n<p>And here for reference are the corresponding PageRank probabilities:</p>\n<pre><code>A,0.033\nB,0.384\nC,0.343\nD,0.039\nE,0.081\nF,0.039\nG,0.016\nH,0.016\nI,0.016\nJ,0.016\nK,0.016\n</code></pre>\n<p>Run this experiment locally first. Report the local configuration that you used and how long in minutes and seconds it takes to complete your job.</p>\n<p>Repeat this experiment on AWS. Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job. (in your notebook, cat the cluster config file)</p>\n"},"dateCreated":"Apr 29, 2016 3:08:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:50"},{"text":"%pyspark\n\ndef line_splitter(line):\n    node, adj_list = re.split('\\t',line.strip())\n    node = node.strip('\"')\n    neighbors = eval(adj_list)\n    node_list = []\n    node_list.append((node, neighbors.keys()))\n    for neighbor in neighbors:\n        node_list.append((neighbor, []))\n    return node_list\n\ndef adjustRank(rank, mass):\n    adj_rank = 0.0\n    if rank is not None:\n        adj_rank = rank\n    return d*adj_rank + d*mass/n + t\n\n\n\n# damping parameter\nd = 0.85\n\n#D = sc.textFile(\"/user/rcordell/hw13/in/PageRank-test.txt\")\nD = sc.textFile(\"s3://hadoopspark/hw13/in/PageRank-test.txt\")\n\ngraph = D.flatMap(lambda line: line_splitter(line)).reduceByKey(lambda a,b:a+b).cache()\n\n# compute the number of nodes\nn = graph.count()\n\n# compute teleportation factor\nt = (1.0-d)/n\n\n# prime the pump with the initial page rank for each node = 1/n\nadj_list = graph.map(lambda (node, outlinks): (node, (1.0/n, outlinks)))\n\nfor i in range(0,30):\n    dangling_mass = adj_list.filter(lambda x: len(x[1][1])==0).map(lambda x: x[1][0]).reduce(lambda x,y:x+y)\n\n    distributed_mass = adj_list.filter(lambda (node, (rank,outlinks)): len(outlinks) > 0)\\\n        .map(lambda (node, (rank, outlinks)): (rank/len(outlinks), outlinks))\\\n        .flatMapValues(lambda x:x)\\\n        .map(lambda (rank, outlink): (outlink, rank))\\\n        .reduceByKey(lambda x,y: x+y)\n\n    adj_list=graph.leftOuterJoin(distributed_mass)\\\n        .map(lambda (node, (outlinks, rank)):(node, (rank,outlinks)))\\\n        .map(lambda (node, (rank, outlinks)):(node, (adjustRank(rank, dangling_mass), outlinks)) )\n        \n\nfor node in  adj_list.sortBy(lambda x: -x[1][0]).collect():\n    print node[0], node[1][0]\n","dateUpdated":"Apr 29, 2016 3:10:15 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530996_1098074141","id":"20160429-150850_629942030","result":{"code":"SUCCESS","type":"TEXT","msg":"B 0.383410412554\nC 0.343378600107\nE 0.0808856932689\nD 0.0390870921233\nF 0.0390870921233\nA 0.0327814931824\nG 0.0161694790207\nI 0.0161694790207\nK 0.0161694790207\nH 0.0161694790207\nJ 0.0161694790207\n"},"dateCreated":"Apr 29, 2016 3:08:50 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:51","dateFinished":"Apr 29, 2016 3:10:25 PM","dateStarted":"Apr 29, 2016 3:10:15 PM","focus":true},{"text":"%md\n\n### PageRank Test\n\nAWS EMR cluster of 1 Master and 2 Task machines, m3.xlarge takes ~9 seconds\n\n### PageRank with Indices Test\n\nBelow we test PageRank and an index file like we would find with the wikipedia data. This test is to understand how to join the article index\nwith the resulting page rank data. The format of the indices-test.txt file is:\n\n    (article name) \\t (index) \\t (in degree) \\t (out degree)\n    \nThe test file looks like:\n\n    ('A', ('! $var = &quot;&quot;', (0.032405885313249345, [])))\n    ('I', ('!!! (Chk Chk Chk)', (0.016456494269789405, ['B', 'E'])))\n    ('E', ('! -attention-', (0.07436994570121233, ['B', 'D', 'F'])))\n    ('H', ('!! (disambiguation)', (0.016456494269789405, ['B', 'E'])))\n    ('D', ('! (disambiguation)', (0.037527978897670906, ['A', 'B'])))\n    ('C', ('! (album)', (0.33644293048159163, ['B'])))\n    ('K', ('!!! (nlow)', (0.016456494269789405, ['E', 'L'])))\n    ('G', ('!! (chess)', (0.016456494269789405, ['B', 'E'])))\n    ('J', ('!!! (album)', (0.016456494269789405, ['E'])))\n    ('B', ('! (CONFIG.SYS directive)', (0.3751220708839311, ['C'])))\n    ('F', ('! Time Zone', (0.037527978897670906, ['B', 'E'])))\n","dateUpdated":"Apr 29, 2016 3:08:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530996_1098074141","id":"20160429-150850_341318500","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>PageRank Test</h3>\n<p>AWS EMR cluster of 1 Master and 2 Task machines, m3.xlarge takes ~9 seconds</p>\n<h3>PageRank with Indices Test</h3>\n<p>Below we test PageRank and an index file like we would find with the wikipedia data. This test is to understand how to join the article index\n<br  />with the resulting page rank data. The format of the indices-test.txt file is:</p>\n<pre><code>(article name) \\t (index) \\t (in degree) \\t (out degree)\n</code></pre>\n<p>The test file looks like:</p>\n<pre><code>('A', ('! $var = &amp;quot;&amp;quot;', (0.032405885313249345, [])))\n('I', ('!!! (Chk Chk Chk)', (0.016456494269789405, ['B', 'E'])))\n('E', ('! -attention-', (0.07436994570121233, ['B', 'D', 'F'])))\n('H', ('!! (disambiguation)', (0.016456494269789405, ['B', 'E'])))\n('D', ('! (disambiguation)', (0.037527978897670906, ['A', 'B'])))\n('C', ('! (album)', (0.33644293048159163, ['B'])))\n('K', ('!!! (nlow)', (0.016456494269789405, ['E', 'L'])))\n('G', ('!! (chess)', (0.016456494269789405, ['B', 'E'])))\n('J', ('!!! (album)', (0.016456494269789405, ['E'])))\n('B', ('! (CONFIG.SYS directive)', (0.3751220708839311, ['C'])))\n('F', ('! Time Zone', (0.037527978897670906, ['B', 'E'])))\n</code></pre>\n"},"dateCreated":"Apr 29, 2016 3:08:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:52"},{"text":"%pyspark\n\nimport re\n\ndef line_splitter(line):\n    node, adj_list = re.split('\\t',line.strip())\n    node = node.strip('\"')\n    neighbors = eval(adj_list)\n    node_list = []\n    node_list.append((node, neighbors.keys()))\n    for neighbor in neighbors:\n        node_list.append((neighbor, []))\n    return node_list\n\ndef index_line_splitter(line):\n    article, index, in_degree, out_degree = re.split('\\t', line.strip())\n    return (index, article)\n\ndef adjustRank(rank, mass):\n    adj_rank = 0.0\n    if rank is not None:\n        adj_rank = rank\n    return d*adj_rank + d*mass/n + t\n\n\ndef page_rank(graph_path, index_path=None, d=0.85):\n    D = sc.textFile(graph_path, use_unicode=False)\n    I = sc.textFile(index_path, use_unicode=False)\n    \n    graph = D.flatMap(lambda line: line_splitter(line)).reduceByKey(lambda a,b:a+b).cache()\n    \n    # compute the number of nodes\n    n = graph.count()\n    \n    # compute teleportation factor\n    t = (1.0-d)/n\n    \n    # prime the pump with the initial page rank for each node = 1/n\n    adj_list = graph.map(lambda (node, outlinks): (node, (1.0/n, outlinks)))\n    \n    for i in range(0,30):\n        dangling_mass = adj_list.filter(lambda x: len(x[1][1])==0).map(lambda x: x[1][0]).reduce(lambda x,y:x+y)\n    \n        distributed_mass = adj_list.filter(lambda (node, (rank,outlinks)): len(outlinks) > 0)\\\n            .map(lambda (node, (rank, outlinks)): (rank/len(outlinks), outlinks))\\\n            .flatMapValues(lambda x:x)\\\n            .map(lambda (rank, outlink): (outlink, rank))\\\n            .reduceByKey(lambda x,y: x+y)\n    \n        adj_list=graph.leftOuterJoin(distributed_mass)\\\n            .map(lambda (node, (outlinks, rank)):(node, (rank,outlinks)))\\\n            .map(lambda (node, (rank, outlinks)):(node, (adjustRank(rank, dangling_mass), outlinks)) )\n    \n    if index_path:\n        article_list = I.map(lambda line: index_line_splitter(line))\n        articles = article_list.join(adj_list)\n\n        for node in articles.sortBy(lambda x: -x[1][1][0]).collect():\n            print node[1], node[1][0]\n    else:\n        # take the Top N based on PageRank\n        for node in  adj_list.sortBy(lambda x: -x[1][0]).take(20):\n            print node[0], node[1][0]\n\npage_rank(\"s3://hadoopspark/hw13/in/PageRank-test.txt\",\"s3://hadoopspark/hw13/in/test1.txt\")","dateUpdated":"Apr 29, 2016 3:08:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","lineNumbers":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530996_1098074141","id":"20160429-150850_611208363","result":{"code":"SUCCESS","type":"TEXT","msg":"('! (CONFIG.SYS directive)', (0.3834104125542339, ['C'])) ! (CONFIG.SYS directive)\n('! (album)', (0.34337860010661836, ['B'])) ! (album)\n('! -attention-', (0.08088569326887081, ['B', 'D', 'F'])) ! -attention-\n('! (disambiguation)', (0.039087092123285226, ['A', 'B'])) ! (disambiguation)\n('! Time Zone', (0.039087092123285226, ['B', 'E'])) ! Time Zone\n('! $var = &quot;&quot;', (0.03278149318242106, [])) ! $var = &quot;&quot;\n('!!! (Chk Chk Chk)', (0.016169479020655377, ['B', 'E'])) !!! (Chk Chk Chk)\n('!! (disambiguation)', (0.016169479020655377, ['B', 'E'])) !! (disambiguation)\n('!!! (nlow)', (0.016169479020655377, ['E'])) !!! (nlow)\n('!! (chess)', (0.016169479020655377, ['B', 'E'])) !! (chess)\n('!!! (album)', (0.016169479020655377, ['E'])) !!! (album)\n"},"dateCreated":"Apr 29, 2016 3:08:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"text":"%md\n\n## Wikipedia Page Rank\n\nRun your Spark PageRank implementation on the Wikipedia dataset for 10 iterations,\nand display the top 100 ranked nodes (with alpha = 0.85).\n","dateUpdated":"Apr 29, 2016 3:08:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530996_1098074141","id":"20160429-150850_1164673340","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Wikipedia Page Rank</h2>\n<p>Run your Spark PageRank implementation on the Wikipedia dataset for 10 iterations,\n<br  />and display the top 100 ranked nodes (with alpha = 0.85).</p>\n"},"dateCreated":"Apr 29, 2016 3:08:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"%pyspark\n\nimport re\n\ndef line_splitter(line):\n    node, adj_list = re.split('\\t',line.strip())\n    node = node.strip('\"')\n    neighbors = eval(adj_list)\n    node_list = []\n    node_list.append((node, neighbors.keys()))\n    for neighbor in neighbors:\n        node_list.append((neighbor, []))\n    return node_list\n\ndef index_line_splitter(line):\n    article, index, in_degree, out_degree = re.split('\\t', line.strip())\n    return (index, article)\n\ndef adjustRank(rank, mass):\n    adj_rank = 0.0\n    if rank is not None:\n        adj_rank = rank\n    return d*adj_rank + d*mass/n + t\n\n\ndef page_rank(graph_path, index_path=None, d=0.85):\n    D = sc.textFile(graph_path, use_unicode=False)\n    I = sc.textFile(index_path, use_unicode=False)\n    \n    graph = D.flatMap(lambda line: line_splitter(line)).reduceByKey(lambda a,b:a+b).cache()\n    \n    # compute the number of nodes\n    n = graph.count()\n    \n    # compute teleportation factor\n    t = (1.0-d)/n\n    \n    # prime the pump with the initial page rank for each node = 1/n\n    adj_list = graph.map(lambda (node, outlinks): (node, (1.0/n, outlinks)))\n    \n    for i in range(0,10):\n        dangling_mass = adj_list.filter(lambda x: len(x[1][1])==0).map(lambda x: x[1][0]).reduce(lambda x,y:x+y)\n    \n        distributed_mass = adj_list.filter(lambda (node, (rank,outlinks)): len(outlinks) > 0)\\\n            .map(lambda (node, (rank, outlinks)): (rank/len(outlinks), outlinks))\\\n            .flatMapValues(lambda x:x)\\\n            .map(lambda (rank, outlink): (outlink, rank))\\\n            .reduceByKey(lambda x,y: x+y)\n    \n        adj_list=graph.leftOuterJoin(distributed_mass)\\\n            .map(lambda (node, (outlinks, rank)):(node, (rank,outlinks)))\\\n            .map(lambda (node, (rank, outlinks)):(node, (adjustRank(rank, dangling_mass), outlinks)) )\n    \n    if index_path:\n        article_list = I.map(lambda line: index_line_splitter(line))\n        articles = article_list.join(adj_list)\n\n        for node in articles.sortBy(lambda x: -x[1][1][0]).take(100):\n            print node[0], node[1][0]\n    else:\n        # take the Top N based on PageRank\n        for node in  adj_list.sortBy(lambda x: -x[1][0]).take(100):\n            print node[0], node[1][0]\n\npage_rank(\"s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt\")","dateUpdated":"Apr 29, 2016 3:08:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530996_1098074141","id":"20160429-150850_351382975","dateCreated":"Apr 29, 2016 3:08:50 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"dateUpdated":"Apr 29, 2016 3:08:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530997_1097689392","id":"20160429-150850_113631575","dateCreated":"Apr 29, 2016 3:08:50 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%md\n## GraphX PageRank\n\nThis Scala code works with the converted Wikipedia PageRank edge list and loads it into GraphX\nfor computing the page rank","dateUpdated":"Apr 29, 2016 3:08:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530997_1097689392","id":"20160429-150850_692918842","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>GraphX PageRank</h2>\n<p>This Scala code works with the converted Wikipedia PageRank edge list and loads it into GraphX\n<br  />for computing the page rank</p>\n"},"dateCreated":"Apr 29, 2016 3:08:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"text":"import org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nval graph = GraphLoader.edgeListFile(sc, \"s3://hadoopspark/hw13/in/all-edges-out.txt\")\nval ranks = graph.staticPageRank(10).vertices.sortBy(_._2, false)\nranks.take(50).foreach{ println }","dateUpdated":"Apr 29, 2016 3:08:50 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530997_1097689392","id":"20160429-150850_83159813","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\ngraph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@61f77bf0\nranks: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Double)] = MapPartitionsRDD[931] at sortBy at <console>:38\n(13455888,6191.387707991102)\n(1184351,2845.767493979813)\n(4695850,2714.4591331096585)\n(5051368,2437.910196936772)\n(1384888,1923.8587550570285)\n(6113490,1900.402227905197)\n(2437837,1891.7338718319434)\n(7902219,1889.6490995891645)\n(13425865,1828.5567821293098)\n(6076759,1817.4577301668883)\n(4196067,1793.9896254737716)\n(6172466,1695.4154776856496)\n(14112583,1628.2317495927646)\n(10390714,1541.5403621746545)\n(15164193,1466.1982843737317)\n(3191491,1442.0454607565)\n(6416278,1397.4806723884976)\n(6237129,1393.3747640501877)\n(7835160,1391.6383866464646)\n(1516699,1377.3708908455212)\n(13725487,1333.7890726005573)\n(9276255,1319.3362649449389)\n(7576704,1316.296888968637)\n(10469541,1293.1822585193127)\n(5154210,1269.5245383130193)\n(12836211,1200.4139991100799)\n(7990491,1197.4173560817123)\n(4198751,1137.543305222643)\n(2797855,1116.6214713749014)\n(11253108,1106.438220278375)\n(9386580,1086.2563346744137)\n(3603527,1086.1348573285893)\n(12074312,1064.028127101078)\n(3069099,1059.4616358329035)\n(14881689,1048.6903631518792)\n(2155467,1040.4164501320897)\n(1441065,1013.9010446645578)\n(14503460,991.1275202931216)\n(2396749,934.601209771263)\n(3191268,918.5246294304637)\n(10566120,916.757304782066)\n(11147327,898.8254723380543)\n(2614581,898.3100857953059)\n(1637982,884.6988766447676)\n(11245362,865.5878960022119)\n(12430985,862.4957649599534)\n(9355455,833.771695120441)\n(10527224,818.0947320405093)\n(14112408,805.2370628749544)\n(2614578,799.8263567369135)\n"},"dateCreated":"Apr 29, 2016 3:08:50 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461944171117_-2136192464","id":"20160429-153611_527396981","dateCreated":"Apr 29, 2016 3:36:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:353","text":"%md\n### GraphX 50 Iterations Static Page Rank - Wikipedia Data Set\n\nResults: 1337 seconds for 50 iterations on an EMR cluster of 1 Master and 5 Task Nodes, m3.xlarge","dateUpdated":"Apr 29, 2016 3:38:13 PM","dateFinished":"Apr 29, 2016 3:38:13 PM","dateStarted":"Apr 29, 2016 3:38:13 PM","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>GraphX 50 Iterations Static Page Rank - Wikipedia Data Set</h3>\n<p>Results: 1337 seconds for 50 iterations on an EMR cluster of 1 Master and 5 Task Nodes, m3.xlarge</p>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942718703_-1644707178","id":"20160429-151158_1074902495","dateCreated":"Apr 29, 2016 3:11:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:296","text":"import org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nval graph = GraphLoader.edgeListFile(sc, \"s3://hadoopspark/hw13/in/all-edges-out.txt\")\nval ranks = graph.staticPageRank(50).vertices.sortBy(_._2, false)\nranks.take(100).foreach{ println }","dateUpdated":"Apr 29, 2016 3:12:22 PM","dateFinished":"Apr 29, 2016 3:34:39 PM","dateStarted":"Apr 29, 2016 3:12:22 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\ngraph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@3855ec2f\nranks: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Double)] = MapPartitionsRDD[1147] at sortBy at <console>:38\n(13455888,6247.506024573392)\n(1184351,2846.9239936690783)\n(4695850,2734.330652975124)\n(5051368,2456.8690847021862)\n(1384888,1924.072620494774)\n(2437837,1909.3118817484838)\n(6113490,1900.6011792429658)\n(7902219,1897.3801181944955)\n(13425865,1851.4863730468433)\n(6076759,1828.25671853491)\n(4196067,1809.9147602543185)\n(6172466,1700.5258943118106)\n(14112583,1647.7788280747352)\n(10390714,1550.2349427044678)\n(15164193,1468.6805265780845)\n(3191491,1445.0087716185078)\n(6416278,1407.267578211767)\n(6237129,1406.3033212656328)\n(7835160,1394.3645478636693)\n(1516699,1389.7014526616854)\n(13725487,1336.5749171352736)\n(9276255,1323.268240759558)\n(7576704,1316.4775797047228)\n(10469541,1295.702149575017)\n(5154210,1271.8818370815968)\n(12836211,1222.6810257948173)\n(7990491,1212.3476235121093)\n(4198751,1150.0819196767488)\n(2797855,1128.540735417043)\n(11253108,1115.5914090454469)\n(9386580,1101.5403646744212)\n(3603527,1089.8847955437072)\n(12074312,1073.0055894537204)\n(3069099,1062.9753781027139)\n(14881689,1048.8204189368994)\n(2155467,1046.0664468313728)\n(1441065,1020.1131967433649)\n(14503460,997.2689307926865)\n(2396749,943.1027348398742)\n(3191268,918.837100076678)\n(10566120,917.0806808315045)\n(2614581,902.7978793582932)\n(11147327,902.729122566704)\n(1637982,884.9663884185131)\n(12430985,869.024440308671)\n(11245362,865.723576161459)\n(9355455,842.1463455847476)\n(10527224,818.1082641195127)\n(14112408,815.513302371431)\n(9391762,804.3479295841383)\n(2614578,803.7090739326829)\n(8697871,799.5276839024389)\n(6172167,798.1970460810702)\n(981395,791.7693925382324)\n(6171937,764.0717287008076)\n(5490435,762.2068787106202)\n(11582765,740.9870553474699)\n(14725161,724.4658564333001)\n(12067030,716.6366031078696)\n(9562547,714.7674286314759)\n(994890,707.0099095465812)\n(9997298,686.9037939065736)\n(9394907,686.1650637318508)\n(13280859,679.6812425861698)\n(10345830,673.7457036064279)\n(4978429,663.716385871631)\n(12447593,662.2563506255321)\n(8019937,655.2460685311348)\n(11148415,636.1993170308632)\n(13432150,632.0215625675958)\n(4344962,628.8319915091492)\n(1175360,606.3190625885975)\n(12038331,603.9904216300304)\n(14565507,594.4471840688329)\n(4624519,588.3768206317151)\n(1523975,582.3913800258495)\n(14981725,576.6186885870572)\n(13328060,575.9648482593647)\n(1332806,558.6544875127522)\n(10399499,556.5728966543314)\n(14963657,555.8525520585326)\n(2578813,548.9039828609114)\n(2826544,548.0166127661013)\n(1575979,544.2502175208448)\n(1813634,543.5238421616273)\n(2778099,530.5083061607137)\n(13853369,516.9478660416212)\n(9924814,513.9816055259574)\n(4568647,494.9036421973505)\n(12785678,489.4683647682875)\n(7467127,489.32208109082865)\n(9742161,488.588372678095)\n(3328327,485.5618862689593)\n(10246542,484.1583260837738)\n(3591832,484.0320134799048)\n(5274313,483.8483237320706)\n(14727077,482.6437575356402)\n(14709489,480.5293792888549)\n(5908108,479.5488618209376)\n(3973000,479.2627118417771)\n"}},{"text":"%md\n### GraphX PageRank With Index Join\n\nNow that we've got the GraphX code working with PageRank, let's pull in the article index and join it to the graph to get more interesting and useful output.\n\nThis first run is for the top 20 in 10 iterations of PageRank","dateUpdated":"Apr 29, 2016 3:58:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530997_1097689392","id":"20160429-150850_1802606541","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>GraphX PageRank With Index Join</h3>\n<p>Now that we've got the GraphX code working with PageRank, let's pull in the article index and join it to the graph to get more interesting and useful output.</p>\n<p>This first run is for the top 20 in 10 iterations of PageRank</p>\n"},"dateCreated":"Apr 29, 2016 3:08:50 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:59","dateFinished":"Apr 29, 2016 3:48:34 PM","dateStarted":"Apr 29, 2016 3:48:34 PM","focus":true},{"text":"import org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nval graph = GraphLoader.edgeListFile(sc, \"s3://hadoopspark/hw13/in/all-edges-out.txt\")\nval ranks = graph.staticPageRank(10).vertices\nval articles = sc.textFile(\"s3://ucb-mids-mls-networks/wikipedia/indices.txt\").map { line =>\n  val fields = line.split(\"\\t\")\n  (fields(1).toLong, fields(0))\n}\nval ranksByArticle = articles.join(ranks).map {\n  case (id, (article, rank)) => (article, rank)\n}\nranksByArticle.sortBy(_._2,false).take(20).foreach{ println }","dateUpdated":"Apr 29, 2016 3:53:27 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461942530997_1097689392","id":"20160429-150850_1856326358","dateCreated":"Apr 29, 2016 3:08:50 PM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:60","dateFinished":"Apr 29, 2016 5:06:26 PM","dateStarted":"Apr 29, 2016 3:53:27 PM","errorMessage":"","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461944672621_1666428700","id":"20160429-154432_1037042249","dateCreated":"Apr 29, 2016 3:44:32 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:389"}],"name":"HW13","id":"2BH5E8SFP","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}