{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Spring - Homework 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is MrJob? How is it different to Hadoop MapReduce? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob is a Python framework to make running complex Map Reduce tasks much simpler. It is capable of running sequences of MapReduce or even iterative MapReduce jobs. The really nice thing about MRJob is the almost pseudo-code like way of expressing how to execute and combine MapReduce jobs.\n",
    "\n",
    "MRJob is not Hadoop but it can execute in a stand-alone mode to run your MapReduce jobs, useful for small scale testing. MRJob also can submit your job to Hadoop via the Streaming API, whether on a local or remote Hadoop cluster. In addition, MRJob has a very nice integration with Amazon AWS Elastic Map Reduce, allowing the researcher to focus on the MapReduce and analysis instead of the infrastructure on which to execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob defines a base class that you as the developer must override to use MRJob. The base class executes the mapper, reducer, and combiner functions when you override them in the class. The MRJob base class also provides intializer and finalizer methods for each of the mapper, combiner and reducer functions. These methods are `mapper_init()`, `combiner_init()`, `reducer_init()`, `mapper_final()`, `combiner_final()`, and `reducer_final()` respectively. The init() methods are called before the corresponding `mapper()`, `combiner()`, `reducer()` methods, allowing setup of data or other things before the method is called. The final() methods are called immediately after the `mapper()`, `reducer()` or `combiner()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is serialization in the context of MrJob or Hadoop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization is the process of converting a machine representation of an object to a format used for storage or transmission. In the context of Hadoop Streaming all input and output is treated as a character stream with keys and values separated by tabs (or another specified delimiter). In the case of MRJob, serialization consists of three types: raw, json, or pickle. Raw is text streams, json is json formatted text streams, and pickle is the Python binary serialization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When it used in these frameworks?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob uses serialization for input and output as well as internal transmission of objects. Each place serialization is used can be defined by the type of protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the default serialization mode for input and outputs for MrJob? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default serialization mode for MRJob inputs is `RAWValueProtocol` which reads lines of text with no key - it's just a stream of text. The default output protocol is `JSONprotocol` which outputs JSON formatted strings separated by a tab character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2: \n",
    "\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html<br/>\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/<br/>\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    "Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "    C,\"10001\",10001   #Visitor id 10001\n",
    "    V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "    V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "    V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "    C,\"10002\",10002   #Visitor id 10001\n",
    "\n",
    "V\n",
    "Note: #denotes comments to the format:\n",
    "\n",
    "    V,1000,1,C, 10001\n",
    "    V,1001,1,C, 10001\n",
    "    V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing msdata_transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile msdata_transform.py\n",
    "# HW 4.2\n",
    "#\n",
    "# Read a CSV file that contains page visits by customers\n",
    "# A customer id record is followed by a number of page id records which are the pages\n",
    "# the customer visited on the web site\n",
    "# Transform the data such that the page visits contain the customer ids on the same record\n",
    "# The result is the elimination of the standalone customer id record and an extended page\n",
    "# visit record containing the page id and customer id\n",
    "# The file contains other records which are output unmodified\n",
    "#\n",
    "import sys\n",
    "with open('anonymous-msweb.data','rU') as datafile:\n",
    "    \n",
    "    # iterate over the lines in the data file\n",
    "    for line in datafile.readlines():\n",
    "        \n",
    "        # split the line into the CSV fields (tokens)\n",
    "        tokens = line.strip().split(',')\n",
    "        \n",
    "        # if a customer record then retain the customer id\n",
    "        if tokens[0] == 'C':\n",
    "            customer_id = tokens[2]\n",
    "            \n",
    "        # if a page visit record then transform to append the customer id\n",
    "        # V,pageid,count,C,cust_id\n",
    "        elif tokens[0] == 'V':\n",
    "            sys.stdout.write('{0},{1},{2},{3},{4}\\n'.format('V',tokens[1],tokens[2],'C',customer_id))\n",
    "            \n",
    "        # otherwise just output the line\n",
    "        else:\n",
    "            sys.stdout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python msdata_transform.py >anonymous-msweb-transformed.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.3: \n",
    "\n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostvisitedpage.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostvisitedpage.py\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "class MRMostVisitedPage(MRJob):\n",
    "    def mapper_get_visits(self, _, record):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "        # yield each visit in the line\n",
    "        tokens = record.split(',')\n",
    "        if tokens[0] == 'V':\n",
    "            yield (tokens[1], 1)\n",
    "\n",
    "    def combiner_count_visits(self, page, counts): \n",
    "        self.increment_counter('Execution Counts', 'combiner calls', 1)\n",
    "        # sum the page visits we've seen so far\n",
    "        yield (page, sum(counts))\n",
    "        \n",
    "    def reducer_count_visits(self, page, counts):\n",
    "        self.increment_counter('Execution Counts', 'reducer_count calls', 1)\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function. yield None, (sum(counts), page)\n",
    "        # discard the key; it is just None\n",
    "        yield None, (sum(counts), page)\n",
    "        \n",
    "    def reducer_find_top5_visits(self, _, page_count_pairs):\n",
    "        self.increment_counter('Execution Counts', 'reducer_find_max calls', 1)\n",
    "        # each item of page_count_pairs is (count, page),\n",
    "        # so yielding one results in key=counts, value=page yield max(page_count_pairs)\n",
    "        return heapq.nlargest(5, page_count_pairs)\n",
    "\n",
    "        \n",
    "    def steps(self): return [\n",
    "            MRStep(mapper=self.mapper_get_visits,\n",
    "                   combiner=self.combiner_count_visits,\n",
    "                   reducer=self.reducer_count_visits),\n",
    "            MRStep(reducer=self.reducer_find_top5_visits)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRMostVisitedPage.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  Execution Counts:\n",
      "    combiner calls: 285\n",
      "    mapper calls: 98955\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-0-mapper-sorted\n",
      "> sort /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-0-mapper_part-00000\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  Execution Counts:\n",
      "    combiner calls: 285\n",
      "    mapper calls: 98955\n",
      "    reducer_count calls: 285\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-mapper-sorted\n",
      "> sort /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-mapper_part-00000\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  Execution Counts:\n",
      "    reducer_find_max calls: 1\n",
      "Moving /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-reducer_part-00000 -> /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/output/part-00000\n",
      "Streaming final output from /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/output\n",
      "10836\t\"1008\"\n",
      "9383\t\"1034\"\n",
      "8463\t\"1004\"\n",
      "5330\t\"1018\"\n",
      "5108\t\"1017\"\n",
      "removing tmp directory /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794\n"
     ]
    }
   ],
   "source": [
    "!python mostvisitedpage.py anonymous-msweb-transformed.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.4: \n",
    "\n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostfreqvisitors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostfreqvisitors.py\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep \n",
    "\n",
    "class MRMostFrequentVisitors(MRJob):\n",
    "    def configure_options(self):\n",
    "        super(MRMostFrequentVisitors, self).configure_options()\n",
    "        self.SORT_VALUES = True\n",
    "        \n",
    "    # generate a dictionary of pages and URLs for them\n",
    "    def mapper_get_visits_init(self):\n",
    "        # create a dictionary to use for the page URLs and ids\n",
    "        self.pages = {}\n",
    "        \n",
    "    # generate keys of page,customer,url and values of 1\n",
    "    def mapper_get_visits(self, _, record):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "        tokens = record.split(',')\n",
    "        \n",
    "        # the page definitions come first in the file so create a dictionary from them.\n",
    "        if tokens[0] == 'A':\n",
    "            self.pages[tokens[1]] = tokens[4].strip('\"')\n",
    "            \n",
    "        # emit a key = (page_id, client_id, url) and value = 1\n",
    "        elif tokens[0] == 'V':\n",
    "            yield ((tokens[1], tokens[4], self.pages[tokens[1]]), 1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # combine page visits by key where the key is page,customer\n",
    "    def combiner_count_visits(self, key, counts): \n",
    "        self.increment_counter('Execution Counts', 'combiner count visits', 1)\n",
    "        # sum the keys we've seen so far.\n",
    "        # the key is (page_id, cust_id, page_url) so we're counting page views by client\n",
    "        yield (key, sum(counts))\n",
    "        \n",
    "    # set up instance variables to use to calculate the max visits to a page by a single customer\n",
    "    def reducer_count_visits_init(self):\n",
    "        self.current_page = None\n",
    "        self.max_count = 0\n",
    "        \n",
    "    # count the visits per page per customer and also compute the max visits per page by a single customer\n",
    "    def reducer_count_visits(self, key, counts):\n",
    "        self.increment_counter('Execution Counts', 'reducer_count visits', 1)\n",
    "        # make sure we have sums of all keys \n",
    "        s = sum(counts)\n",
    "        if self.current_page == key[0]:\n",
    "            if self.max_count < s:\n",
    "                self.max_count = s\n",
    "        else:\n",
    "            if self.current_page:\n",
    "                p = self.current_page\n",
    "                t = self.max_count\n",
    "                yield((self.current_page,'*',key[2]), t)\n",
    "                \n",
    "            self.current_page = key[0]\n",
    "            self.max_count = s\n",
    "\n",
    "        yield (key, s)\n",
    "\n",
    "    # set up a variable to contain the current page max count value\n",
    "    def reducer_find_max_visits_init(self):\n",
    "        self.page_max = 0\n",
    "     \n",
    "    # yield the max visits to a page and the customers that made them\n",
    "    def reducer_find_max_visits(self, key, counts):\n",
    "        self.increment_counter('Execution Counts', 'reducer_find_max visits', 1)\n",
    "        \n",
    "        # if this is the key with the max visits for the page then stash it\n",
    "        if key[1] == '*':\n",
    "            self.page_max = sum(counts)\n",
    "        else:\n",
    "            # otherwise sum the counts and store a local copy because it exhausts the generator\n",
    "            p = sum(counts)\n",
    "            # if this count is the same as the max visits for the page, yield it\n",
    "            if p == self.page_max:\n",
    "                yield key, p\n",
    "        \n",
    "          \n",
    "    def steps(self): return [\n",
    "            MRStep(mapper_init=self.mapper_get_visits_init,\n",
    "                    mapper=self.mapper_get_visits,\n",
    "                   combiner=self.combiner_count_visits,\n",
    "                   reducer_init=self.reducer_count_visits_init,\n",
    "                   reducer=self.reducer_count_visits),\n",
    "            MRStep(reducer_init=self.reducer_find_max_visits_init,\n",
    "                    reducer=self.reducer_find_max_visits)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRMostFrequentVisitors.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "creating tmp directory /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  Execution Counts:\n",
      "    combiner count visits: 98654\n",
      "    mapper calls: 98955\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-0-mapper-sorted\n",
      "> sort /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-0-mapper_part-00000\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  Execution Counts:\n",
      "    combiner count visits: 98654\n",
      "    mapper calls: 98955\n",
      "    reducer_count visits: 98654\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-mapper-sorted\n",
      "> sort /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-mapper_part-00000\n",
      "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  Execution Counts:\n",
      "    reducer_find_max visits: 98938\n",
      "Moving /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-reducer_part-00000 -> /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/output/part-00000\n",
      "Streaming final output from /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/output\n",
      "removing tmp directory /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269\n"
     ]
    }
   ],
   "source": [
    "!python mostfreqvisitors.py anonymous-msweb-transformed.data > max_page_visits_customer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1000\", \"10001\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10010\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10039\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10073\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10087\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10101\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10132\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10141\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10154\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10162\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10166\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10201\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10218\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10220\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10324\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10348\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10376\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10384\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10409\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10429\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10454\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10457\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10471\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10497\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10511\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10520\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10541\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10564\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10599\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10752\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10756\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10861\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10935\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10943\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"10969\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11027\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11050\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11410\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11429\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11440\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11490\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11501\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11528\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11539\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11544\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11685\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11695\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11723\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11766\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11774\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11779\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11898\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"11964\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12017\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12020\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12035\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12086\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12123\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12143\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12155\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12201\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12220\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12228\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12262\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12273\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12306\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12315\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12324\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12337\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12343\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12400\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12415\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12484\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12485\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12537\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12571\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12583\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12674\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12700\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12740\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12815\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12853\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12893\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12897\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12930\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12944\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12970\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"12982\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13015\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13049\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13079\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13080\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13085\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13128\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13176\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13197\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13223\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13248\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13275\", \"/regwiz\"]\t1\r\n",
      "[\"1000\", \"13294\", \"/regwiz\"]\t1\r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat max_page_visits_customer.output | head -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1295\", \"38244\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38296\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38313\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38454\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38571\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38573\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38661\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38678\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38755\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38831\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38869\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38953\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38981\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"38998\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39024\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39033\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39058\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39066\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39094\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39105\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39112\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39131\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39194\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39221\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39284\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39293\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39493\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39505\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39550\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39604\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39617\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39627\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39645\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39719\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39730\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39760\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39863\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39899\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39900\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39902\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39948\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"39977\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40025\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40046\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40207\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40233\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40274\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40310\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40390\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40419\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40482\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40597\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40616\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40679\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40758\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40787\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40827\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40923\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40930\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40942\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40946\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"40965\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41068\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41075\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41093\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41100\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41117\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41175\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41183\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41207\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41255\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41269\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41273\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41367\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41429\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41580\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41594\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41598\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41692\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41715\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41730\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41748\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41843\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"41953\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42065\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42146\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42161\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42198\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42234\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42241\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42262\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42313\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42353\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42385\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42497\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42516\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42568\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42576\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42600\", \"/train_cert\"]\t1\r\n",
      "[\"1295\", \"42616\", \"/train_cert\"]\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat max_page_visits_customer.output | tail -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.5 Clustering Tweet Dataset\n",
    "\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "- 0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "- 1: Cyborg, where language is primarily borrowed from other sources\n",
    "    (e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "- 2: Robot, where language is formulaically derived from unrelated sources\n",
    "    (e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "- 3: Spammer, where language is replicated to high multiplicity\n",
    "    (e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of  recent research, which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "    topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "    USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    "    .\n",
    "    .\n",
    "\n",
    "    where\n",
    "\n",
    "    USERID = unique user identifier\n",
    "    CODE = 0/1/2/3 class code\n",
    "    TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "- (A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)\n",
    "- (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "- (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "- (D) K=4 \"trained\" centroids, determined by the sums across the classes. Use use the (row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!).\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "    topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "    Row 1: Words\n",
    "    Row 2: Aggregated distribution across all classes\n",
    "    Row 3-6 class-aggregated distributions for clases 0-3\n",
    "    \n",
    "- For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]\n",
    "- For (B), (C), and (D)  you will have to use data from the auxiliary file: \n",
    "\n",
    "    `topUsers_Apr-Jul_2014_1000-words_summaries.txt`\n",
    "\n",
    "This file contains 5 special word-frequency distributions:\n",
    "\n",
    "- (1) The 1000-user-wide aggregate, which you will perturb for initializations in parts (B) and (C), and\n",
    "- (2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)\n",
    "\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate \n",
    "(after initially normalizing by its sum, which is also provided).\n",
    "So if in (B) you want to create 2 perturbations of the aggregate, start\n",
    "with (1), normalize, and generate 1000 random numbers uniformly \n",
    "from the unit interval (0,1) twice (for two centroids), using:\n",
    "\n",
    "from numpy import random\n",
    "numbers = random.sample(1000)\n",
    "\n",
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate,\n",
    "and then renormalize to obtain one of your aggregate-perturbed initial centroids.\n",
    "\n",
    "    ###################################################################################\n",
    "    # Generate random initial centroids around the global aggregate\n",
    "    # Part (B) and (C) of this question\n",
    "    ###################################################################################\n",
    "    def startCentroidsBC(k):\n",
    "        counter = 0\n",
    "        for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "            if counter == 2:        \n",
    "                data = re.split(\",\",line)\n",
    "                globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "            counter += 1\n",
    "        ## perturb the global aggregate for the four initializations    \n",
    "        centroids = []\n",
    "        for i in range(k):\n",
    "            rndpoints = random.sample(1000)\n",
    "            peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "            centroids.append(peturpoints)\n",
    "            total = 0\n",
    "            for j in range(len(centroids[i])):\n",
    "                total += centroids[i][j]\n",
    "            for j in range(len(centroids[i])):\n",
    "                centroids[i][j] = centroids[i][j]/total\n",
    "        return centroids\n",
    "\n",
    "------\n",
    "For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting assigncluster.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile assigncluster.py\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep \n",
    "from numpy import array, argmin\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "class AssignCluster(MRJob):\n",
    "    centroid_points=[]\n",
    "    centroids_file = '/Users/rcordell/Documents/MIDS/W261/week04/HW4/centroids.txt'\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.cluster_mapper_init, \n",
    "                      mapper=self.cluster_mapper)\n",
    "        ]\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def cluster_mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(self.centroids_file).readlines()]\n",
    "        \n",
    "    #load data and output the cluster id and customer id \n",
    "    def cluster_mapper(self, _, line):\n",
    "        self.increment_counter('Execution Counts', 'mapper', 1)\n",
    "        values = (map(float,line.split(',')))\n",
    "        cust_id = int(values[0])\n",
    "        vector = [x/values[2] for x in values[3:]]\n",
    "        yield int(MinDist(vector,self.centroid_points)), cust_id\n",
    "\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    AssignCluster.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrkmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrkmeans.py\n",
    "from numpy import argmin, array, random, zeros\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep \n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "class MRKMeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=4 \n",
    "    centroids_file = '/Users/rcordell/Documents/MIDS/W261/week04/HW4/centroids.txt'\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, \n",
    "                      mapper=self.mapper,\n",
    "                      combiner = self.combiner,\n",
    "                      reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(self.centroids_file).readlines()]\n",
    "        open(self.centroids_file, 'w').close()\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('Execution Counts', 'mapper', 1)\n",
    "        values = (map(float,line.split(',')))\n",
    "        vector = [x/values[2] for x in values[3:]]\n",
    "        yield int(MinDist(vector,self.centroid_points)), (vector,1)\n",
    "        \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, cluster, inputdata):\n",
    "        self.increment_counter('Execution Counts', 'combiner', 1)\n",
    "        vector_sum = [0]*1000\n",
    "        num = 0\n",
    "        for vector, n in inputdata:\n",
    "            num = num + n\n",
    "            vector_sum = [sum(x) for x in zip(vector_sum, vector)]\n",
    "        yield cluster,(vector_sum,num)\n",
    "        \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, cluster, inputdata): \n",
    "        self.increment_counter('Execution Counts', 'reducer', 1)\n",
    "        centroids = []\n",
    "        num = 0.0 \n",
    "        vector = [0.0]*1000\n",
    "        for vector_sum, n in inputdata:\n",
    "            num = num + n\n",
    "            vector = [sum(x) for x in zip(vector, vector_sum)]\n",
    "        \n",
    "        # the new centroids are the means of the sums of all the member vector elements\n",
    "        vector = [x/num for x in vector]\n",
    "\n",
    "        yield cluster,vector\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRKMeans.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "iteration1:\n",
      "iteration2:\n",
      "iteration3:\n",
      "{0: {'wrong': 734, 'right': 18}, 1: {'wrong': 90, 'right': 1}, 2: {'wrong': 33, 'right': 21}, 3: {'wrong': 53, 'right': 50}}\n"
     ]
    }
   ],
   "source": [
    "# initialize centroids by choosing k vectors at random from the data set\n",
    "from numpy import random, array, shape\n",
    "from mrkmeans import MRKMeans\n",
    "from assigncluster import AssignCluster\n",
    "from itertools import chain\n",
    "\n",
    "k = 4\n",
    "max_iterations = 10\n",
    "datafile = '/Users/rcordell/Documents/MIDS/W261/week04/HW4/topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "centroidsfile = '/Users/rcordell/Documents/MIDS/W261/week04/HW4/centroids.txt'\n",
    "scoresfile = '/Users/rcordell/Documents/MIDS/W261/week04/HW4/scores.txt'\n",
    "summaryfile = '/Users/rcordell/Documents/MIDS/W261/week04/HW4/topUsers_Apr-Jul_2014_1000-words_summaries.txt'\n",
    "\n",
    "scores = {}\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new, threshold):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for element in Diff:\n",
    "        if(element > threshold):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "def persist_centroids(centroids_filename, centroids):\n",
    "    # persist the centroids to disk \n",
    "    with open(centroids_filename,'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroids)\n",
    "\n",
    "def initialize_centroids(k, datafile):\n",
    "    centroids = []\n",
    "    # generate an array of k randomly chosen integers.\n",
    "    seed = sorted(random.random_integers(1, 1000, (1, k))[0])\n",
    "\n",
    "\n",
    "    # select the vectors from the data file from the randomly generated integers\n",
    "    with open(datafile, 'rU') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            # go to the ith vector in the file\n",
    "            if i+1 in seed:\n",
    "                # read the line which is userid,code,total,word1 count, word2 count,... , word1000 count\n",
    "                values = [float(x) for x in line.split(',')]\n",
    "\n",
    "                # normalize by dividing the word counts by the total word count\n",
    "                centroids.append(array([x/values[2] for x in values[3:]]))\n",
    "\n",
    "    return centroids    \n",
    "\n",
    "\n",
    "def initialize_centroids_perturb(k, summaryfile):\n",
    "    with open(summaryfile) as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            # second line is the aggregrated vector\n",
    "            if i == 1:        \n",
    "                data = line.split(\",\")\n",
    "                globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        \n",
    "        # why do we divide the randomized vector by 10 before adding to the aggregate vector?\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids  \n",
    "\n",
    "def initialize_centroids_trained(k, summaryfile):\n",
    "    centroids = []\n",
    "    with open(summaryfile) as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            if i > 1:        \n",
    "                data = line.split(\",\")\n",
    "                centroids.append([float(data[i+3])/float(data[2]) for i in range(1000)])\n",
    "\n",
    "    return centroids\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "#    centroids = initialize_centroids_trained(k, summaryfile)    \n",
    "    centroids = initialize_centroids(k, datafile)\n",
    "#    centroids = initialize_centroids_perturb(k, summaryfile)\n",
    "    \n",
    "    persist_centroids(centroidsfile, centroids)\n",
    "    mr_job = MRKMeans(args=[datafile,'--strict-protocols'])\n",
    "    assign_cluster = AssignCluster(args=[datafile,'--strict-protocols'])\n",
    "\n",
    "    # Update centroids iteratively\n",
    "    for i in range(max_iterations):\n",
    "        # save previous centoids to check convergency\n",
    "        centroids_old = centroids[:]\n",
    "\n",
    "        print \"iteration\"+str(i)+\":\"\n",
    "\n",
    "        # Assign the customers to a cluster\n",
    "        with assign_cluster.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            for line in runner.stream_output():\n",
    "                cluster,cust_id =  assign_cluster.parse_output_line(line)\n",
    "                scores[cust_id] = cluster\n",
    "\n",
    "        # write out the customer classifications/cluster assignments\n",
    "        with open(scoresfile, 'a') as s:\n",
    "            for cust_id in scores:\n",
    "                s.write('{0},{1}\\n'.format(cust_id, scores[cust_id]))\n",
    "\n",
    "        # update the centroids\n",
    "        with mr_job.make_runner() as runner: \n",
    "            runner.run()\n",
    "\n",
    "            # stream_output: get access of the output \n",
    "            for line in runner.stream_output():\n",
    "                cluster,centroid =  mr_job.parse_output_line(line)\n",
    "                centroids[cluster] = centroid     \n",
    "\n",
    "        with open(centroidsfile, 'a') as f:\n",
    "            f.writelines(','.join(str(element) for element in centroid) + '\\n' for centroid in centroids)\n",
    "\n",
    "        if(stop_criterion(centroids_old,centroids,0.01)):\n",
    "            break\n",
    "            \n",
    "    # compare our classification to the training set\n",
    "    train_Y = {}\n",
    "    tally = {}\n",
    "    with open(datafile, 'rU') as infile:\n",
    "        for line in infile.readlines():\n",
    "            data = line.split(\",\")\n",
    "            train_Y[int(data[0])] = int(data[1])\n",
    "    for cust_id in train_Y:\n",
    "        if train_Y[cust_id] not in tally:\n",
    "            tally[train_Y[cust_id]] = {'right' : 0, 'wrong' : 0}\n",
    "        if train_Y[cust_id] == scores[cust_id]:\n",
    "            tally[train_Y[cust_id]]['right'] += 1\n",
    "        else:\n",
    "            tally[train_Y[cust_id]]['wrong'] += 1\n",
    "\n",
    "    print tally\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.6  (OPTIONAL) Scaleable K-MEANS++ \n",
    "\n",
    "Over half a century old and showing no signs of aging,\n",
    "k-means remains one of the most popular data processing\n",
    "algorithms. As is well-known, a proper initialization\n",
    "of k-means is crucial for obtaining a good final solution.\n",
    "The recently proposed k-means++ initialization algorithm\n",
    "achieves this, obtaining an initial set of centers that is provably\n",
    "close to the optimum solution. A major downside of the\n",
    "k-means++ is its inherent sequential nature, which limits its\n",
    "applicability to massive data: one must make k passes over\n",
    "the data to find a good initial set of centers. The paper listed below \n",
    "shows how to drastically reduce the number of passes needed\n",
    "to obtain, in parallel, a good initialization. This is unlike\n",
    "prevailing efforts on parallelizing k-means that have mostly\n",
    "focused on the post-initialization phases of k-means. The \n",
    "proposed initialization algorithm k-means||\n",
    "obtains a nearly optimal solution after a logarithmic number\n",
    "of passes; the paper also shows that in practice a constant\n",
    "number of passes suffices. Experimental evaluation on realworld\n",
    "large-scale data demonstrates that k-means|| outperforms\n",
    "k-means++ in both sequential and parallel settings.\n",
    "\n",
    "Read the following paper entitled \"Scaleable K-MEANS++\" located at:\n",
    "\n",
    "http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf \n",
    "\n",
    "In MrJob, implement K-MEANS|| and compare with a random initializtion when used in \n",
    "conjunction with the kmeans algorithm as an initialization step for the 2D  dataset \n",
    "generated using code in the following notebook:\n",
    "\n",
    "https://www.dropbox.com/s/lbzwmyv0d8rocfq/MrJobKmeans.ipynb?dl=0\n",
    "\n",
    "Plot the initialation centroids and the centroid trajectory as the K-MEANS|| algorithms iterates. \n",
    "Repeat this for a random initalization (i.e., pick a training vector at random for each inital centroid)\n",
    "of the kmeans algorithm. Comment on the trajectories of both algorithms.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms.\n",
    "Also report the rand index score for both algorithms and comment on your findings.\n",
    "\n",
    "\n",
    "4.6.1 Apply your implementation of K-MEANS|| to the dataset  in HW 4.5 and compare to the a \n",
    "random initalization (i.e., pick a training vector at random for each inital centroid)of the kmeans algorithm.\n",
    "Report on the number passes over the training data, and time required to run all  clustering algorithms. \n",
    "Also report the rand index score for both algorithms and comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.7   (OPTIONAL) Canopy Clustering\n",
    "\n",
    "An alternative way to intialize the k-means algorithm is the  canopy clustering. The canopy clustering \n",
    "algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and \n",
    "Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the \n",
    "Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, \n",
    "where using another algorithm directly may be impractical due to the size of the data set.\n",
    "\n",
    "For more details on the Canopy Clustering algorithm see:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Canopy_clustering_algorithm\n",
    "\n",
    "Plot the initialation centroids and the centroid trajectory as the Canopy Clustering based K-MEANS algorithm iterates. \n",
    "Repeat this for a random initalization (i.e., pick a training vector at random for each inital centroid)\n",
    "of the kmeans algorithm. Comment on the trajectories of both algorithms.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms.\n",
    "Also report the rand index score for both algorithms and comment on your findings.\n",
    "\n",
    "4.7.1 Apply your implementation Canopy Clustering based K-MEANS algorithm to the dataset  in HW 4.5 and compare to the a \n",
    "random initalization (i.e., pick a training vector at random for each inital centroid)of the kmeans algorithm.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms. \n",
    "Also report the rand index score for both algorithms and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
