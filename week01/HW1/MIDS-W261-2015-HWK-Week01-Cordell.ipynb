{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Spring 2016 Homework Week 1\n",
    "\n",
    "Ron Cordell<br />\n",
    "W261-4<br />\n",
    "ron.cordell@ischool.berkeley.edu<br />\n",
    "January 19, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.0. \n",
    "#### Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Big Data\" is any analysis or problem that involves working with data at volumes, velocities and varieties that are beyond the boundaries of \"traditional\" vertically scaled solutions and systems. Big Data problems are such that they require tools and techniques using parallel processing, sampling of high speed data streams and/or combination of varieties of disparate data and formats (to name a few) to provide solutions.\n",
    "\n",
    "An example of a big data problem in the world of health care is in the area of population health management. Regional medical systems such as Sutter Health, John Muir, and even the VA consist of a huge number of disparate data systems all producing event data as patients, care providers, medications, procedures and more move through the dance of episodic medical care. A regional medical group or system can be a nucleus of hospitals, perhaps acquired or merged over time or as a public institution, with satellite medical offices and federated health care providers like pediatricians or Ob/Gyns, pharmacies, etc. A simple episodic example is the patient that comes into a doctor's office for an acute medical issue. The visit spawns a number of events in the medical records system of the doctor's office - patient demographic data, doctor's notes, pharmacy orders, doctor's orders for labs, etc. If the patient is sent to a lab to have a blood test, the lab medical systems generate more events: more patient demographic data, the lab order, eventually the intermediate lab results, the final lab results. The patient may then receive a prescription, again generating events for yet more demographic data, presctription data that may be coded in several different medication coding systems, etc. And this is a simple case. Consider what happens when the patient is referred to a hospital or surgery: multiple care providers generating orders, labs, results, medication, procedures, equipment, insurance data... We talk about digital exhaust often in the sense of social media or leaving footprints in our day to day lives but the amount of data generated from a single patient episode is much much larger.\n",
    "\n",
    "Now take this event data, this patient data exhaust, multiply it by the number of health care providers, the number of health care institutions, the number of patients on a daily basis, and the sheer variety of disparate systems involved. I haven't even really pointed out the insurance data, the medical supply chain data, the pharmaceutical supply chain data... When a medical community wants to understand patient populations they need data to provide insight, but the data is in the silos of systems that don't talk to each other and don't generally have readily common data formats. There are three major medication coding systems in the US alone, and none of them line up one for one - they all contain different sets of concepts and translations between them can be imperfect. There is some level of common health care data information interchange, but every system that implements it uses a unique dialect that is then customized upon installation. A single patient is replicated in all these disparate systems with different identifiers, mistyped or incorrect demographic data, or just empty data because people were in a hurry - all making it very challenging to match that patient across all those systems and across separate episodes. \n",
    "\n",
    "A medical community looking into patient population health may see obvious things such as regional and cultural influences on health, chronic disease by demographic, or the population density of common allergies but they have the potential to see so much more. For example, the propensity to diagnose a particular condition or procedure, the use of a particular drug, the observed outcomes of a procedure, how medications move through distribution channels to the patients and more. Looking at the data across regional health care systems can provide an almost unimaginable wealth of information about effective treatments, resource allocations of medications and supplies, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.1.  \n",
    "#### In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective in selecting a model, $f\\hat(X)$, is to select a model that describes the true underlying distibution, $f(X)$, by minimizing the error of the predicted values for the training set $T$ as well as for values not in the training set. In other words, we'd like to choose a model that fits the training set but also generalizes well outside the training set.\n",
    "\n",
    "Let's suppose that we sample with replacement bootrap samples $B$ from dataset $T$ such that we have a set of samples $B = {B_0, B_1, ..., B_n}$ . We learn a model for each polynomial degree on each bootstrap sample $B_i$ and apply each model to those values in $T$ not in $B_i$. From this we are able to estimate the bias, the variance and the noise for our different models.\n",
    "\n",
    "Using the mean squared error of the model we can derive the irreducable error (noise), the variance, and the bias for each based on the following relation which is nicely derived in this [wikipedia article](https://en.wikipedia.org/wiki/Bias–variance_tradeoff).\n",
    "\n",
    "$E[(y - f\\hat(x))^2] = Var[f\\hat(x)] + Bias[f\\hat(x)]^2 + \\sigma^2$\n",
    "\n",
    "Mean squared error  = variance   +   bias       +      noise\n",
    "\n",
    "As we increase the order of the polynomial we achieve a better fit of the test data, which reduces bias but increases variance; the noise is inherent and we can't do anything about that. The following graph illustrates how the bias and variance change with model complexity; this is the bias-variance tradeoff.\n",
    "\n",
    "<img src=\"https://github.com/rocket-ron/MIDS-W261/raw/master/week01/HW1/Bias-Variance-2.png\" width=600>\n",
    "\n",
    "As mentioned we want the model with the optimal combination of bias, variance and complexity. More complex models (higher order polynomial) fit the training set better (low bias) but will over-fit and don't generalize well to data not in the training set. Variance increases with model complexity as each model attempts to fit more and more points of its particular training set. \n",
    "\n",
    "When we compare our various models across each of the bootstrap training sets for differing degrees of polynomial model fit we end up with a graph like this one.\n",
    "\n",
    "<img src=\"https://github.com/rocket-ron/MIDS-W261/raw/master/week01/HW1/bias-variance-tradeoff.png\" width=400>\n",
    "\n",
    "Now we can answer the question of which model we would choose now that we have the information about bias, variance and noise as shown in the above graph. We would choose the model with the best combination of low bias, low variance and low complexity. In the graph above that corresponds to the value of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 1.1\n",
    "#### Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1.2\n",
    "#### Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW1.2\n",
    "## Given a file and list of words, read lines and count occurrences of words\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "filename = sys.argv[1]\n",
    "\n",
    "## Words in the word list are space delimited\n",
    "wordlist = sys.argv[2].lower().split(' ')\n",
    "counts = {}\n",
    "\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        fields = line.split('\\t')      \n",
    "        if len(fields) > 3:\n",
    "            # we are interested in subject and body but need to be resilient for missing fields\n",
    "            text = '{0} {1}'.format(fields[2],fields[3])\n",
    "        elif len(fields) > 2:\n",
    "            text = fields[2]\n",
    "        # extract only words from the combined subject and body text\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word.lower() in wordlist:\n",
    "                try:\n",
    "                    counts[word.lower()] += 1\n",
    "                except:\n",
    "                    counts[word.lower()] = 1\n",
    "\n",
    "for word in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.2\n",
    "## given a list of intermediate word count files, combine into a single word count list\n",
    "import sys\n",
    "counts = {}\n",
    "\n",
    "for intermediate_file in sys.argv:\n",
    "    with open(intermediate_file, 'rU') as infile:\n",
    "        # intermediate files are word <tab> count per line\n",
    "        for line in infile.readlines():\n",
    "            word_count = line.split('\\t')\n",
    "            if len(word_count) == 2:\n",
    "                try:\n",
    "                    counts[word_count[0]] += int(word_count[1])\n",
    "                except KeyError:\n",
    "                    counts[word_count[0]] = int(word_count[1])\n",
    "for word in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\n'.format(word, counts[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the pNaiveBayes.sh bash shell script\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "# dump the reducer output to check out results\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1.3 - 1.5\n",
    "#### Evolve a mapper and reducer that first takes a single word, then several words, then all words to classify spam or ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map\n",
    "The mapper.py code has evolved over the 3 iterations of single to all word execution. The final result handles all three cases.\n",
    "\n",
    "This mapper works by taking each record (a single email) and extracting the id, subject and body. The subject and body are concatenated and then tokens matched against a regular expression that screens for words, numbers and apostrophes. The result is a key,value pair output for each word in a record. The key consists of (email id, label, vocabulary word flag, word) and the value is the count. An example key for a non-spam email might be: \n",
    "\n",
    "    (0007.2001-02-09.kitchen, 0, 1, assistance) = 1\n",
    "    \n",
    "In the case that all tokens are used then all keys will have the vocabulary word flag = 1. \n",
    "\n",
    "The key value pairs are output to STDOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Ron Cordell\n",
    "## Description: mapper code for HW1.4\n",
    "## Given a file and list of words, read lines and count occurrences of words\n",
    "## Output a key, value => (id, class, token, term_flag) = count\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## Lines in the file have 4 fields:\n",
    "## ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "filename = sys.argv[1]\n",
    "\n",
    "## Words in the word list are space delimited\n",
    "wordlist = sys.argv[2].lower().split(' ')\n",
    "\n",
    "all_words = False\n",
    "if wordlist[0] == '*':\n",
    "    all_words = True\n",
    "    \n",
    "counts = {}\n",
    "\n",
    "with open (filename, \"rU\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        fields = line.split('\\t')      \n",
    "        if len(fields) > 3:\n",
    "            # we are interested in subject and body but need to be resilient for missing fields\n",
    "            text = '{0} {1}'.format(fields[2],fields[3])\n",
    "        elif len(fields) > 2:\n",
    "            text = fields[2]\n",
    "        # extract only words from the combined subject and body text\n",
    "        for word in WORD_RE.findall(text):\n",
    "            term = '0'\n",
    "            if all_words:\n",
    "                term = '1'\n",
    "            elif word.lower() in wordlist:\n",
    "                term = '1'\n",
    "            try:\n",
    "                counts[(fields[0],fields[1],word.lower(), term)] += 1\n",
    "            except:\n",
    "                counts[(fields[0],fields[1],word.lower(), term)] = 1\n",
    "        # ensure that all words in the list are represented even if they don't occur\n",
    "        if not all_words:\n",
    "            for word in wordlist:\n",
    "                try:\n",
    "                    if counts[(fields[0],fields[1], word, '1')] > 0:\n",
    "                        pass\n",
    "                except KeyError:\n",
    "                    counts[(fields[0], fields[1], word, '1')] = 0\n",
    "\n",
    "for key in counts:\n",
    "    sys.stdout.write('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\n'.format(key[0], key[1], key[2], key[3], counts[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce\n",
    "The reducer.py code implements the consolidation of the key, value pairs emitted by the mapper into a single dictionary and subsequent helper data structures.\n",
    "\n",
    "`counts` is a dictionary of key, value pairs where the key is a `tuple (id, label, vocab_flag, word)`\n",
    "\n",
    "`spam_doc_ids` and `ham_doc_ids` are lists of the email ids for each label `spam` or `ham`\n",
    "\n",
    "`term_counts` is a dictionary of vocabulary terms generated based on the `counts` key tuple `vocab_flag` value. If the value is `1` it indicates that the word is a vocabulary word and will be added to the `term_counts` dictionary. The count value is accumulated depending on whether the term occurs in a `spam` document or a `ham` document. Probabilities of `P(term|ham)` and `P(term|spam)` are also calculated in this dictionary.\n",
    "        \n",
    "Laplace smoothing is used in the calculation of the `P(term|label)` for all terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Ron Cordell\n",
    "## Description: reducer code for HW1.4\n",
    "## given a list of intermediate word count files, compute NB classification\n",
    "import sys\n",
    "counts = {}\n",
    "term_counts = {}\n",
    "spam_doc_ids = []\n",
    "ham_doc_ids = []\n",
    "spam_doc_word_count = 0.0\n",
    "ham_doc_word_count = 0.0\n",
    "spam_term_count = 0.0\n",
    "ham_term_count = 0.0\n",
    "terms = 0.0\n",
    "\n",
    "for intermediate_file in sys.argv:\n",
    "    with open(intermediate_file, 'rU') as infile:\n",
    "        # intermediate files are id <tab> class <tab> word <tab> term_flag <tab> count per line\n",
    "        for line in infile.readlines():\n",
    "            word_count = line.split('\\t')\n",
    "            if len(word_count) == 5:\n",
    "                # some things to make this more readable\n",
    "                mail_id = word_count[0]\n",
    "                label = word_count[1]\n",
    "                word = word_count[2]\n",
    "                if word_count[3] == '0':\n",
    "                    vocab_word = False\n",
    "                else:\n",
    "                    vocab_word = True\n",
    "                count = float(word_count[4].strip())\n",
    "\n",
    "                if (mail_id, label, vocab_word, word) in counts:\n",
    "                    counts[(mail_id, label, vocab_word, word)] += count\n",
    "                else:\n",
    "                    counts[(mail_id, label, vocab_word, word)] = count\n",
    "\n",
    "spam = { k:v for k,v in counts.items() if k[1] == '1' }\n",
    "for k in spam:\n",
    "    spam_doc_word_count += spam[k]\n",
    "    if k[0] not in spam_doc_ids:\n",
    "        spam_doc_ids.append(k[0])\n",
    "    if k[2]:\n",
    "        if k[3] in term_counts:\n",
    "            term_counts[k[3]]['spam_count'] += spam[k]\n",
    "        else:\n",
    "            term_counts[k[3]] = {'spam_count' : spam[k],\n",
    "                                 'ham_count' : 0.0,\n",
    "                                  'prob_ham'  : 0.0,\n",
    "                                  'prob_spam' : 0.0}\n",
    "    \n",
    "ham = { k:v for k,v in counts.items() if k[1] == '0' }\n",
    "for k in ham:\n",
    "    ham_doc_word_count += ham[k]\n",
    "    if k[0] not in ham_doc_ids:\n",
    "        ham_doc_ids.append(k[0])\n",
    "    if k[2]:\n",
    "        if k[3] in term_counts:\n",
    "            term_counts[k[3]]['ham_count'] += ham[k]\n",
    "        else:\n",
    "            term_counts[k[3]] = {'ham_count' : ham[k],\n",
    "                                 'spam_count' : 0.0,\n",
    "                                  'prob_ham'  : 0.0,\n",
    "                                  'prob_spam' : 0.0}\n",
    "                       \n",
    "# now we should have consolidated the intermediate counts and we can compute the rest\n",
    "\n",
    "# count the number of terms\n",
    "term_count = len(term_counts.keys()) * 1.0\n",
    "\n",
    "prior = (len(spam_doc_ids)*1.0)/(1.0*(len(spam_doc_ids) + len(ham_doc_ids)))\n",
    "\n",
    "# calculate the P(term|class) for each term\n",
    "for term in term_counts:\n",
    "    term_counts[term]['prob_ham'] = (term_counts[term]['ham_count'] + 1.0)/(ham_doc_word_count + term_count)\n",
    "    term_counts[term]['prob_spam'] = (term_counts[term]['spam_count'] + 1.0)/(spam_doc_word_count + term_count)\n",
    "\n",
    "# for debugging\n",
    "if False:    \n",
    "    for k in counts:\n",
    "        print '{0} {1} {2} {3} {4}'.format(k[0],k[1],k[2],k[3],counts[k])\n",
    "    for t in term_counts:\n",
    "        print '{0} {1} {2} {3} {4}'.format(t, term_counts[t]['spam_count'], \n",
    "                                       term_counts[t]['ham_count'],\n",
    "                                       term_counts[t]['prob_spam'],\n",
    "                                       term_counts[t]['prob_ham'])\n",
    "    print len(spam_doc_ids)\n",
    "    print len(ham_doc_ids)\n",
    "    print spam_doc_word_count\n",
    "    print ham_doc_word_count\n",
    "    print term_count\n",
    "    print prior\n",
    "\n",
    "right = 0.0\n",
    "for email in spam_doc_ids + ham_doc_ids:\n",
    "    if email in ham_doc_ids:\n",
    "        true_class = '0'\n",
    "    else:\n",
    "        true_class = '1'\n",
    "    docs = {k:v for k,v in counts.items() if k[0]==email}\n",
    "    label = '0'\n",
    "    prob_spam = 0.0\n",
    "    prob_ham  = 0.0\n",
    "    for term in term_counts:\n",
    "        # does this email contain any of the vocabulary terms\n",
    "        ham = { k:v for k,v in docs.items() if k[3] == term }\n",
    "        for h in ham:           \n",
    "            if ham[h] > 0.0:\n",
    "                if prob_spam > 0.0:\n",
    "                    prob_spam = prob_spam * term_counts[term]['prob_spam']**ham[h]\n",
    "                    prob_ham = prob_ham * term_counts[term]['prob_ham']**ham[h]\n",
    "                else:\n",
    "                    prob_spam = prior * term_counts[term]['prob_spam']**ham[h]\n",
    "                    prob_ham = (1.0 - prior) * term_counts[term]['prob_ham']**ham[h]\n",
    "    if prob_spam > 0.0:\n",
    "        if prob_spam > prob_ham:\n",
    "            label = '1'\n",
    "    sys.stdout.write('{0}\\t{1}\\t{2}\\n'.format(email, true_class, label))\n",
    "    if true_class == label:\n",
    "        right += 1.0\n",
    "sys.stdout.write('Score: {0}/{1}'.format(right, len(spam_doc_ids) + len(ham_doc_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the Python script\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the execution permissions of the pNaiveBayes.sh bash shell script\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 1.3: Classify using the single word \"assistance\" and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the classifier using the single word \"assistance\" and using Laplace smoothing yields a classification success rate of 60/100 = 60%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 1.4: Classify using the single word \"assistance\", \"viagra\" and \"enlargementWithAType\" and report results\n",
    "These instructions weren't entirely clear as to what was meant by \"enlargementWithATypo\". I couldn't find the occurance of any form or portions of the word \"enlargement\" in the corpus. I decided to go with \"enlargement\" for this excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance viagra enlargement\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the classifier improved slightly to 62/100 = 62%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 1.5: Classify using all terms and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the classifier improved to 99/100 = 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1.6\n",
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error \n",
    "- Please prepare a table to present your results\n",
    "- Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)\n",
    "- Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn\n",
    "\n",
    "Let DF represent the training set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "# replicate our mapper code here where we take the subject and body together\n",
    "# except now we grab the label field as well to use for the Y values\n",
    "with open('enronemail_1h.txt', 'rU') as infile:\n",
    "    for line in infile.readlines():\n",
    "        fields = line.split('\\t')      \n",
    "        if len(fields) > 3:\n",
    "            # we are interested in subject and body but need to be resilient for missing fields\n",
    "            text = '{0} {1}'.format(fields[2],fields[3])\n",
    "        elif len(fields) > 2:\n",
    "            text = fields[2]\n",
    "        # extract only words from the combined subject and body text\n",
    "        X_train.append(text)\n",
    "        Y_train.append(fields[1])\n",
    "\n",
    "# Use the TfidVectorizer to create the feature vectors\n",
    "# We should override the tokenizer regular expression to make it the same as what we used\n",
    "# in our poor man's mapper code\n",
    "vectorizer = TfidfVectorizer(token_pattern = \"[\\w']+\")\n",
    "vf = vectorizer.fit(X_train,Y_train)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(vf.fit_transform(X_train), Y_train)\n",
    "print 1.0 - clf.score(vf.fit_transform(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18\n"
     ]
    }
   ],
   "source": [
    "# The change in the tfidf vectorizer to use the same token regex as our mapper results in \n",
    "# a 2% increase of training error, from 16% to 18%\n",
    "clf = BernoulliNB()\n",
    "clf.fit(vf.fit_transform(X_train), Y_train)\n",
    "print 1.0 - clf.score(vf.fit_transform(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The following table summarizes our results of the various classification method training error:\n",
    "\n",
    "| Classification Methodology  | Training error |\n",
    "|-----------------------------|----------------|\n",
    "| map/reduce single word `assignment`    |      40%       |\n",
    "| map/reduce multiple word `assignment`, `vallium`, `enlargement` |      38%       |\n",
    "| map/reduce all words                   |      1%        |\n",
    "| scikit-learn MultinomialNB  |      0%        |\n",
    "| scikit-learn BernoulliNB    |     18%        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
